{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84e0f92d-885a-42f9-806a-3b38b4e70876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime,timezone\n",
    "\n",
    "from scipy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64a1a652-0f97-4508-bcb3-90a1c0e325c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import functions_framework\n",
    "# @functions_framework.http\n",
    "# def load_v2_new_incident_ml_to_bq(request):\n",
    "\n",
    "isTrainData=False\n",
    "isExploreMode=False\n",
    "\n",
    "schema_for_new_data=None\n",
    "threshold_x_sd = 3  # 2x-4x\n",
    "split_test=0.2\n",
    "isShuffle=True  # if false ,it is not stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bee7814-0bed-4bf5-8986-75e4a816a73f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported DateTime: 2024-01-06 11:28:11\n"
     ]
    }
   ],
   "source": [
    "dt_imported=datetime.now(timezone.utc)\n",
    "\n",
    "str_imported=dt_imported.strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(f\"Imported DateTime: {str_imported}\" )\n",
    "\n",
    "# explore min and max import data  Incident update every 00:05 AM(Asia bangkok)\n",
    "imported_to='2024-01-01'    # get last item of 2023-12-01  00\n",
    "# SELECT max(imported_at) as max_date,min(imported_at) as min_date  FROM `pongthorn.SMartDW.incident` ;\n",
    "# SELECT max(open_datetime) as max_date,min(open_datetime) as min_date  FROM `pongthorn.SMartDW.incident` ;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb6404b-66f9-4649-9fc4-c5be0e276a20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d80accdd-3135-4709-ae6b-c07206592bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# projectId='smart-data-ml'\n",
    "# credentials = service_account.Credentials.from_service_account_file(r'C:\\Windows\\smart-data-ml-91b6f6204773.json')\n",
    "# client = bigquery.Client(credentials=credentials, project=projectId)\n",
    "\n",
    "\n",
    "projectId='pongthorn'\n",
    "client = bigquery.Client(project=projectId)\n",
    "\n",
    "dw_dataset_id=\"SMartDW\"\n",
    "dataset_id=\"SMartML\"\n",
    "\n",
    "dw_table_id = f\"{projectId}.{dw_dataset_id}.incident\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef765238-7818-4d10-b19a-0bf9ab232a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_bq(sql:str):\n",
    "\n",
    " query_result=client.query(sql)\n",
    " df_all=query_result.to_dataframe()\n",
    " return df_all\n",
    "\n",
    "def loadDataFrameToBQ(table_id,dfx):\n",
    "    try:\n",
    "        if isTrainData:\n",
    "            job_config = bigquery.LoadJobConfig(\n",
    "                write_disposition=\"WRITE_TRUNCATE\",\n",
    "            )\n",
    "        else:\n",
    "                   job_config = bigquery.LoadJobConfig(\n",
    "                write_disposition=\"WRITE_APPEND\",schema=schema_for_new_data\n",
    "            )\n",
    "\n",
    "        job = client.load_table_from_dataframe(\n",
    "            dfx, table_id, job_config=job_config\n",
    "        )\n",
    "        job.result()  # Wait for the job to complete.\n",
    "        print(f\"TrainData={isTrainData} : {len(dfx)} rows imported to {table_id} successfully\")\n",
    "\n",
    "    except BadRequest as e:\n",
    "        print(\"Bigquery Error\\n\")\n",
    "        for e in job.errors:\n",
    "            print('ERROR: {}'.format(e['message']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a034146f-e44c-4bd5-811d-6d2a62477250",
   "metadata": {},
   "outputs": [],
   "source": [
    "label='severity_id'\n",
    "labelName='severity_name'\n",
    "\n",
    "removeCols=['updated_at','imported_at']\n",
    "\n",
    "# dateCols=['open_datetime','close_datetime','response_datetime','resolved_datetime']\n",
    "# numbericCols=['open_to_close_hour','response_to_resolved_hour']\n",
    "# start_end_list=[ ['open_datetime','close_datetime'],['response_datetime','resolved_datetime']]\n",
    "\n",
    "dateCols=['open_datetime','close_datetime']\n",
    "numbericCols=['open_to_close_hour']\n",
    "start_end_list=[ ['open_datetime','close_datetime']]\n",
    "\n",
    "\n",
    "rangeCols=[]\n",
    "cateCols=['sla','product_type','brand','service_type','incident_type']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fce784-7d27-45fc-bd03-1a84d3e12639",
   "metadata": {},
   "source": [
    "# Create New Data Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c148917a-0b07-4538-8b27-fdb6411ed50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_schema(table_ml_id):\n",
    "    schema=[]\n",
    "    try:\n",
    "        table=client.get_table(table_ml_id)  # Make an API request.\n",
    "        schema=table.schema\n",
    "        print(\"Table {} already exists.\".format(table_ml_id))\n",
    "    except Exception as ex:\n",
    "        schema = [\n",
    "        bigquery.SchemaField(\"id\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"severity_id\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"severity_name\", \"STRING\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"sla\", \"STRING\", mode=\"REQUIRED\"),    \n",
    "        bigquery.SchemaField(\"product_type\", \"STRING\", mode=\"REQUIRED\"),  \n",
    "        bigquery.SchemaField(\"brand\", \"STRING\", mode=\"REQUIRED\"),  \n",
    "        bigquery.SchemaField(\"service_type\", \"STRING\", mode=\"REQUIRED\"),  \n",
    "        bigquery.SchemaField(\"incident_type\", \"STRING\", mode=\"REQUIRED\"),  \n",
    "        bigquery.SchemaField(\"open_to_close_hour\", \"FLOAT\", mode=\"REQUIRED\"),\n",
    "        bigquery.SchemaField(\"range_open_to_close_hour\", \"STRING\", mode=\"REQUIRED\"),    \n",
    "        bigquery.SchemaField(\"imported_at\", \"DATETIME\", mode=\"REQUIRED\")    \n",
    "        ]\n",
    "\n",
    "        table = bigquery.Table(table_ml_id,schema=schema)\n",
    "        table.time_partitioning = bigquery.TimePartitioning(\n",
    "        type_=bigquery.TimePartitioningType.DAY,field=\"imported_at\")\n",
    "\n",
    "\n",
    "        table = client.create_table(table)  # Make an API request.\n",
    "        print(\n",
    "            \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "        )\n",
    "    finally:\n",
    "       print(f\"Get Schema of {table_ml_id} as below.\")    \n",
    "       return schema\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdb2ce1-36f7-4d2e-aae2-ca5c230dd4ac",
   "metadata": {},
   "source": [
    "# Get Max Import Data From New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0f4d80c6-ca02-40bf-9319-70c1023f3979",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_imported_at(table_ml_id):\n",
    "\n",
    "    sql_lastImport=f\"SELECT max(imported_at) as last_imported from `{table_ml_id}` \"\n",
    "\n",
    "    print(sql_lastImport)\n",
    "\n",
    "    job_lastImported=client.query(sql_lastImport)\n",
    "    str_lastImported=None\n",
    "    for row in job_lastImported:    \n",
    "        if row.last_imported is not None: \n",
    "            str_lastImported=row.last_imported.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    print(f\"Last Imported DateTime: {str_lastImported}\" )\n",
    "\n",
    "    if str_lastImported is not None:\n",
    "      print(\"Start date from last loading\")  \n",
    "      start_date_query=str_lastImported\n",
    "    else:\n",
    "      start_date_query=imported_to  \n",
    "      print(\"Init First loading\") \n",
    "    return start_date_query    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845a1184-9fab-4326-8a0f-e0e0f9a22f72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18e5a81d-bb30-40ac-9ead-dfe21e96e9d2",
   "metadata": {},
   "source": [
    "# Explore and Transform Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d0375bba-02df-43c2-bc2e-66dfab84589a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pongthorn.SMartML.new2_incident\n",
      "Created table pongthorn.SMartML.new2_incident\n",
      "Get Schema of pongthorn.SMartML.new2_incident as below.\n",
      "SELECT max(imported_at) as last_imported from `pongthorn.SMartML.new2_incident` \n",
      "Last Imported DateTime: None\n",
      "Init First loading\n",
      "Load the most up-to-date new data from imported date >= 2024-01-01\n",
      "2024-01-01\n",
      " \n",
      "SELECT  id,severity_id,severity_name\n",
      ",sla,product_type,brand,service_type,incident_type\n",
      ",open_datetime,  close_datetime\n",
      ",updated_at,imported_at\n",
      "FROM `pongthorn.SMartDW.incident` \n",
      "    WHERE imported_at>= '2024-01-01'\n",
      "     order by id\n",
      "    \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "order_by=\" order by id\"\n",
    "#order_by=\"order by imported_at\n",
    "\n",
    "sql_all=f\"\"\"\n",
    "SELECT  id,severity_id,severity_name\n",
    ",sla,product_type,brand,service_type,incident_type\n",
    ",open_datetime,  close_datetime\n",
    ",updated_at,imported_at\n",
    "FROM `{dw_table_id}` \"\"\"\n",
    "\n",
    "if isTrainData: # get from imported_to specified by you\n",
    "    print(f\"Build train/test data during imported date < {datetime.strptime(imported_to,'%Y-%m-%d')}\")\n",
    "\n",
    "    train_name='train2_incident'\n",
    "    test_name='test2_incident'\n",
    "    train_table_id=f\"{projectId}.{dataset_id}.{train_name}\"\n",
    "    test_tabel_id=f\"{projectId}.{dataset_id}.{test_name}\"\n",
    "    print(f\"TrainTable2={train_table_id} and TestTable2={test_tabel_id}\")\n",
    "    \n",
    "    sql_all=f\"\"\" {sql_all}\n",
    "    WHERE imported_at< '{imported_to}'\n",
    "    {order_by}\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "else: # get from max update\n",
    "    new_incident_name='new2_incident'    \n",
    "    new_incident_table_id=f\"{projectId}.{dataset_id}.{new_incident_name}\"\n",
    "    print(f\"{new_incident_table_id}\")\n",
    "    \n",
    "    # 1.get table and its schema\n",
    "    schema_for_new_data=get_table_schema(new_incident_table_id)\n",
    "    # print(schema_for_new_data)\n",
    "    imported_to=get_last_imported_at(new_incident_table_id)  # get  start_date_query  from this method\n",
    "    print(f\"Load the most up-to-date new data from imported date >= {imported_to}\")\n",
    "    print(imported_to)\n",
    "    \n",
    "    \n",
    "    sql_all=f\"\"\" {sql_all}\n",
    "    WHERE imported_at>= '{imported_to}'\n",
    "    {order_by}\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "print(sql_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc68930b-7635-4417-8e9d-36ebc4790468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from Bigquery\n",
      "Total origianal rows prior to perform data prep: 12\n",
      "Import at :2024-01-02 17:05:06.109259 - 2024-01-05 17:05:09.414918\n",
      "Open Date : 2023-11-30 15:17:00 - 2024-01-05 09:10:00\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12 entries, 0 to 11\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   id              12 non-null     Int64         \n",
      " 1   severity_id     12 non-null     Int64         \n",
      " 2   severity_name   12 non-null     object        \n",
      " 3   sla             12 non-null     object        \n",
      " 4   product_type    12 non-null     object        \n",
      " 5   brand           12 non-null     object        \n",
      " 6   service_type    12 non-null     object        \n",
      " 7   incident_type   12 non-null     object        \n",
      " 8   open_datetime   12 non-null     datetime64[ns]\n",
      " 9   close_datetime  12 non-null     datetime64[ns]\n",
      " 10  updated_at      12 non-null     datetime64[ns]\n",
      " 11  imported_at     12 non-null     datetime64[ns]\n",
      "dtypes: Int64(2), datetime64[ns](4), object(6)\n",
      "memory usage: 1.2+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>severity_id</th>\n",
       "      <th>severity_name</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>imported_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4305</td>\n",
       "      <td>3</td>\n",
       "      <td>Minor</td>\n",
       "      <td>2024-01-05 13:49:57</td>\n",
       "      <td>2024-01-05 17:05:09.414918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4307</td>\n",
       "      <td>4</td>\n",
       "      <td>Cosmetic</td>\n",
       "      <td>2024-01-05 16:36:22</td>\n",
       "      <td>2024-01-05 17:05:09.414918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4308</td>\n",
       "      <td>4</td>\n",
       "      <td>Cosmetic</td>\n",
       "      <td>2024-01-05 16:38:51</td>\n",
       "      <td>2024-01-05 17:05:09.414918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4309</td>\n",
       "      <td>4</td>\n",
       "      <td>Cosmetic</td>\n",
       "      <td>2024-01-05 16:46:27</td>\n",
       "      <td>2024-01-05 17:05:09.414918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4310</td>\n",
       "      <td>2</td>\n",
       "      <td>Major</td>\n",
       "      <td>2024-01-05 20:07:35</td>\n",
       "      <td>2024-01-05 17:05:09.414918</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  severity_id severity_name          updated_at  \\\n",
       "7   4305            3         Minor 2024-01-05 13:49:57   \n",
       "8   4307            4      Cosmetic 2024-01-05 16:36:22   \n",
       "9   4308            4      Cosmetic 2024-01-05 16:38:51   \n",
       "10  4309            4      Cosmetic 2024-01-05 16:46:27   \n",
       "11  4310            2         Major 2024-01-05 20:07:35   \n",
       "\n",
       "                  imported_at  \n",
       "7  2024-01-05 17:05:09.414918  \n",
       "8  2024-01-05 17:05:09.414918  \n",
       "9  2024-01-05 17:05:09.414918  \n",
       "10 2024-01-05 17:05:09.414918  \n",
       "11 2024-01-05 17:05:09.414918  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Load data from Bigquery\")\n",
    "df_all=load_data_bq(sql_all)\n",
    "print(f\"Total origianal rows prior to perform data prep: {len(df_all)}\")\n",
    "no_original_rows=len(df_all)\n",
    "\n",
    "if no_original_rows==0:\n",
    " print(\"No records from bigquery\")  \n",
    " quit()\n",
    " # return \"No new data imported to bigquery\"   \n",
    "else:\n",
    "    \n",
    " df_all=df_all.drop_duplicates(subset=['id'],keep='last')   \n",
    " print(f\"Import at :{df_all['imported_at'].min()} - {df_all['imported_at'].max()}\")\n",
    " print(f\"Open Date : {df_all['open_datetime'].min()} - {df_all['open_datetime'].max()}\")   \n",
    "\n",
    "print(df_all.info())\n",
    "df_all[['id',\"severity_id\",\"severity_name\",\"updated_at\",\"imported_at\"]].tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b433b4a7-a3ff-4fde-911d-6820b5664c75",
   "metadata": {},
   "source": [
    "# Transform and enrich data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e511da8d-67bb-4352-856c-9c12a4ddc3ca",
   "metadata": {},
   "source": [
    "# Manage Numberic Cols\n",
    "### 1-Find Time Inverval\n",
    "### 2-Remove outlier on Time Interval to service\n",
    "### 3-Find any rows  contain zero time period\n",
    "### 4-Create range from time-interval (best,good,moderate,bad,worst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b93a24d-ed5f-4a63-80be-923250b5949a",
   "metadata": {},
   "source": [
    "# Bining Range\n",
    "\n",
    "function is used to separate the array elements into many different ranges . \n",
    "The cut function is mainly used to perform statistical analysis on scalar data. \n",
    "\n",
    "we can convert hour to range   \n",
    "* (0, 24] =by 1 day =best\n",
    "* (24, 168] =  1day -1 week  =good\n",
    "* (168, 360]=  1week- 15 days(half a month) =fair\n",
    "* (360, 720]= 15 dasy-1 month =bad\n",
    "* (720, 2349]=1 month-1 Q =worst\n",
    "\n",
    "open_to_close_hour ,response_to_resolved_hour , Mostly  we can complate by 1 day (0, 24]\n",
    "there are few cases that take long to close incident (360, 720]   15 day to  1month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bdeb3f72-4569-4e29-bc8c-a70363c0ab79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manage Numberic Cols\n"
     ]
    }
   ],
   "source": [
    "print(\"Manage Numberic Cols\")\n",
    "listDiffDateDeltaCols=[]\n",
    "listDiffHourCols=[]\n",
    "for item  in  start_end_list:\n",
    "   diff_str=f\"{item[0]}_to_{item[1]}\" \n",
    "   diff_str=diff_str.replace('_datetime','')  \n",
    "   listDiffDateDeltaCols.append(diff_str)\n",
    "   df_all[diff_str]=df_all[item[1]]-df_all[item[0]]\n",
    "    \n",
    "   diff_hour=f'{diff_str}_hour'\n",
    "   listDiffHourCols.append(diff_hour)\n",
    "   df_all[diff_hour] = df_all[diff_str].apply(lambda x:  x.total_seconds() / (60*60) if x is not np.nan else np.nan  )\n",
    "\n",
    "\n",
    "# https://www.geeksforgeeks.org/z-score-for-outlier-detection-python/\n",
    "xScoreDiffHourCols=[]\n",
    "for col in listDiffHourCols:\n",
    "  z_col=f\"zscore_{col}\"   \n",
    "  df_all[z_col] = np.abs(stats.zscore(df_all[col]))   \n",
    "  xScoreDiffHourCols.append(z_col)  \n",
    "\n",
    "# df_all.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1cebbd-0afb-4404-a4c6-952080f347d3",
   "metadata": {},
   "source": [
    "# Remove oulier  and delete zero hour training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "092fba6b-dcd5-461e-80bd-20fb64946ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if isTrainData :\n",
    "    print(\"Remove oulier  and delete zero hour training data\")\n",
    "    for col in  xScoreDiffHourCols:\n",
    "      df_all=df_all.query(f\"{col}<@threshold_x_sd\")\n",
    "\n",
    "\n",
    "    allRows=df_all.shape[0]\n",
    "    for col in numbericCols:\n",
    "        zeroRows=len(df_all.query(f\"{col}==0\"))\n",
    "        pctZeroHour=round(zeroRows/allRows*100,1)\n",
    "        print(f\"No. 0-hour row on {col} = {zeroRows}({pctZeroHour} %)\")\n",
    "        df_all=df_all.query(f'{col}!=0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ed1426-d435-4d40-be30-c311314b6b59",
   "metadata": {},
   "source": [
    "# Apply Service Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "050d1b27-1c91-4595-b944-9dba4bb761eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def service_hour_range(x_hour):\n",
    "    if x_hour>0 and x_hour<=24:\n",
    "      return \"soonest\"\n",
    "    elif x_hour>25 and x_hour<=168:\n",
    "      return \"soon\"\n",
    "    elif x_hour>168 and x_hour<=360:\n",
    "      return \"fair\"\n",
    "    elif x_hour>360 and x_hour<=720:\n",
    "      return \"late\"\n",
    "    else:\n",
    "      return \"latest\"\n",
    "\n",
    "for col in numbericCols:\n",
    "    range_col=f\"range_{col}\"\n",
    "    rangeCols.append(range_col)\n",
    "    df_all[range_col]=df_all[col].apply(service_hour_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e224a718-eea8-4afc-ae56-a07090bf140f",
   "metadata": {},
   "source": [
    "# Final TRansform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7459c270-e68f-46f7-b27a-08bf2c44d3bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove ['updated_at', 'imported_at', 'open_datetime', 'close_datetime', 'zscore_open_to_close_hour', 'open_to_close']\n",
      "% remove data 0.0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 12 entries, 0 to 11\n",
      "Data columns (total 11 columns):\n",
      " #   Column                    Non-Null Count  Dtype              \n",
      "---  ------                    --------------  -----              \n",
      " 0   id                        12 non-null     int64              \n",
      " 1   severity_id               12 non-null     int64              \n",
      " 2   severity_name             12 non-null     object             \n",
      " 3   sla                       12 non-null     object             \n",
      " 4   product_type              12 non-null     object             \n",
      " 5   brand                     12 non-null     object             \n",
      " 6   service_type              12 non-null     object             \n",
      " 7   incident_type             12 non-null     object             \n",
      " 8   open_to_close_hour        12 non-null     float64            \n",
      " 9   range_open_to_close_hour  12 non-null     object             \n",
      " 10  imported_at               12 non-null     datetime64[ns, UTC]\n",
      "dtypes: datetime64[ns, UTC](1), float64(1), int64(2), object(7)\n",
      "memory usage: 1.1+ KB\n",
      "None\n",
      "pct removed data =0.0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>severity_id</th>\n",
       "      <th>severity_name</th>\n",
       "      <th>sla</th>\n",
       "      <th>product_type</th>\n",
       "      <th>brand</th>\n",
       "      <th>service_type</th>\n",
       "      <th>incident_type</th>\n",
       "      <th>open_to_close_hour</th>\n",
       "      <th>range_open_to_close_hour</th>\n",
       "      <th>imported_at</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4197</td>\n",
       "      <td>2</td>\n",
       "      <td>Major</td>\n",
       "      <td>24x7 4Hrs Response Time</td>\n",
       "      <td>Software</td>\n",
       "      <td>Red Hat</td>\n",
       "      <td>Incident</td>\n",
       "      <td>General Incident</td>\n",
       "      <td>484.900000</td>\n",
       "      <td>late</td>\n",
       "      <td>2024-01-06 11:28:11.200874+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4204</td>\n",
       "      <td>4</td>\n",
       "      <td>Cosmetic</td>\n",
       "      <td>24x7 4Hrs Response Time</td>\n",
       "      <td>Software</td>\n",
       "      <td>Red Hat</td>\n",
       "      <td>Request</td>\n",
       "      <td>General Incident</td>\n",
       "      <td>848.300000</td>\n",
       "      <td>latest</td>\n",
       "      <td>2024-01-06 11:28:11.200874+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4298</td>\n",
       "      <td>2</td>\n",
       "      <td>Major</td>\n",
       "      <td>24x7 4Hrs Resolution Time</td>\n",
       "      <td>Firewall</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>Incident</td>\n",
       "      <td>General Incident</td>\n",
       "      <td>0.866667</td>\n",
       "      <td>soonest</td>\n",
       "      <td>2024-01-06 11:28:11.200874+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4299</td>\n",
       "      <td>2</td>\n",
       "      <td>Major</td>\n",
       "      <td>24x7 4Hrs Resolution Time</td>\n",
       "      <td>Firewall</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>Incident</td>\n",
       "      <td>General Incident</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>soonest</td>\n",
       "      <td>2024-01-06 11:28:11.200874+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4302</td>\n",
       "      <td>3</td>\n",
       "      <td>Minor</td>\n",
       "      <td>24x7 4Hrs Response Time</td>\n",
       "      <td>Storage</td>\n",
       "      <td>NetApp</td>\n",
       "      <td>Incident</td>\n",
       "      <td>Network Adapter Failure</td>\n",
       "      <td>160.083333</td>\n",
       "      <td>soon</td>\n",
       "      <td>2024-01-06 11:28:11.200874+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4303</td>\n",
       "      <td>4</td>\n",
       "      <td>Cosmetic</td>\n",
       "      <td>24x7 4Hrs Response Time</td>\n",
       "      <td>Software</td>\n",
       "      <td>Red Hat</td>\n",
       "      <td>Request</td>\n",
       "      <td>General Incident</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>soonest</td>\n",
       "      <td>2024-01-06 11:28:11.200874+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4304</td>\n",
       "      <td>2</td>\n",
       "      <td>Major</td>\n",
       "      <td>24x7 4Hrs Resolution Time</td>\n",
       "      <td>Firewall</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>Incident</td>\n",
       "      <td>General Incident</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>soonest</td>\n",
       "      <td>2024-01-06 11:28:11.200874+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4305</td>\n",
       "      <td>3</td>\n",
       "      <td>Minor</td>\n",
       "      <td>24x7 4Hrs Response Time</td>\n",
       "      <td>Server</td>\n",
       "      <td>HPE</td>\n",
       "      <td>Incident</td>\n",
       "      <td>Memory Failure</td>\n",
       "      <td>11.433333</td>\n",
       "      <td>soonest</td>\n",
       "      <td>2024-01-06 11:28:11.200874+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4307</td>\n",
       "      <td>4</td>\n",
       "      <td>Cosmetic</td>\n",
       "      <td>24x7 4Hrs Response Time</td>\n",
       "      <td>Software</td>\n",
       "      <td>Red Hat</td>\n",
       "      <td>Request</td>\n",
       "      <td>General Incident</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>soonest</td>\n",
       "      <td>2024-01-06 11:28:11.200874+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4308</td>\n",
       "      <td>4</td>\n",
       "      <td>Cosmetic</td>\n",
       "      <td>24x7 4Hrs Response Time</td>\n",
       "      <td>Software</td>\n",
       "      <td>Red Hat</td>\n",
       "      <td>Request</td>\n",
       "      <td>General Incident</td>\n",
       "      <td>2.500000</td>\n",
       "      <td>soonest</td>\n",
       "      <td>2024-01-06 11:28:11.200874+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4309</td>\n",
       "      <td>4</td>\n",
       "      <td>Cosmetic</td>\n",
       "      <td>24x7 4Hrs Response Time</td>\n",
       "      <td>Software</td>\n",
       "      <td>Red Hat</td>\n",
       "      <td>Request</td>\n",
       "      <td>General Incident</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>soonest</td>\n",
       "      <td>2024-01-06 11:28:11.200874+00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4310</td>\n",
       "      <td>2</td>\n",
       "      <td>Major</td>\n",
       "      <td>24x7 4Hrs Resolution Time</td>\n",
       "      <td>Firewall</td>\n",
       "      <td>Palo Alto</td>\n",
       "      <td>Incident</td>\n",
       "      <td>General Incident</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>soonest</td>\n",
       "      <td>2024-01-06 11:28:11.200874+00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  severity_id severity_name                        sla product_type  \\\n",
       "0   4197            2         Major    24x7 4Hrs Response Time     Software   \n",
       "1   4204            4      Cosmetic    24x7 4Hrs Response Time     Software   \n",
       "2   4298            2         Major  24x7 4Hrs Resolution Time     Firewall   \n",
       "3   4299            2         Major  24x7 4Hrs Resolution Time     Firewall   \n",
       "4   4302            3         Minor    24x7 4Hrs Response Time      Storage   \n",
       "5   4303            4      Cosmetic    24x7 4Hrs Response Time     Software   \n",
       "6   4304            2         Major  24x7 4Hrs Resolution Time     Firewall   \n",
       "7   4305            3         Minor    24x7 4Hrs Response Time       Server   \n",
       "8   4307            4      Cosmetic    24x7 4Hrs Response Time     Software   \n",
       "9   4308            4      Cosmetic    24x7 4Hrs Response Time     Software   \n",
       "10  4309            4      Cosmetic    24x7 4Hrs Response Time     Software   \n",
       "11  4310            2         Major  24x7 4Hrs Resolution Time     Firewall   \n",
       "\n",
       "        brand service_type            incident_type  open_to_close_hour  \\\n",
       "0     Red Hat     Incident         General Incident          484.900000   \n",
       "1     Red Hat      Request         General Incident          848.300000   \n",
       "2   Palo Alto     Incident         General Incident            0.866667   \n",
       "3   Palo Alto     Incident         General Incident            0.666667   \n",
       "4      NetApp     Incident  Network Adapter Failure          160.083333   \n",
       "5     Red Hat      Request         General Incident            3.000000   \n",
       "6   Palo Alto     Incident         General Incident            0.600000   \n",
       "7         HPE     Incident           Memory Failure           11.433333   \n",
       "8     Red Hat      Request         General Incident            2.500000   \n",
       "9     Red Hat      Request         General Incident            2.500000   \n",
       "10    Red Hat      Request         General Incident            3.500000   \n",
       "11  Palo Alto     Incident         General Incident            0.500000   \n",
       "\n",
       "   range_open_to_close_hour                      imported_at  \n",
       "0                      late 2024-01-06 11:28:11.200874+00:00  \n",
       "1                    latest 2024-01-06 11:28:11.200874+00:00  \n",
       "2                   soonest 2024-01-06 11:28:11.200874+00:00  \n",
       "3                   soonest 2024-01-06 11:28:11.200874+00:00  \n",
       "4                      soon 2024-01-06 11:28:11.200874+00:00  \n",
       "5                   soonest 2024-01-06 11:28:11.200874+00:00  \n",
       "6                   soonest 2024-01-06 11:28:11.200874+00:00  \n",
       "7                   soonest 2024-01-06 11:28:11.200874+00:00  \n",
       "8                   soonest 2024-01-06 11:28:11.200874+00:00  \n",
       "9                   soonest 2024-01-06 11:28:11.200874+00:00  \n",
       "10                  soonest 2024-01-06 11:28:11.200874+00:00  \n",
       "11                  soonest 2024-01-06 11:28:11.200874+00:00  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "removeCols=removeCols+dateCols+xScoreDiffHourCols+listDiffDateDeltaCols\n",
    "print(f\"Remove {removeCols}\")\n",
    "df_all=df_all.drop(columns=removeCols)\n",
    "no_rows_after_removing_outlier=len(df_all)\n",
    "pct_row_decrease=round( (no_original_rows-no_rows_after_removing_outlier)/no_original_rows*100 ,0)\n",
    "\n",
    "print(f\"% remove data {pct_row_decrease}\")\n",
    "\n",
    "df_all[['id','severity_id']] =df_all[['id','severity_id']].astype('int64')\n",
    "\n",
    "if isTrainData==False:\n",
    "    df_all['imported_at']=dt_imported\n",
    "\n",
    "print(df_all.info())\n",
    "\n",
    "\n",
    "print(f\"pct removed data ={(no_original_rows-len(df_all))/no_original_rows*100}\")\n",
    "#it is pretty close to 5%   , at most 10% is removable\n",
    "# df_all[listDiffHourCols].describe(percentiles=[.95,.75,.50,.25,.05]).drop(index=['count','std'])\n",
    "\n",
    "df_all.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "82892e02-14e5-4789-82ab-e6855e7dbe47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://www.geeksforgeeks.org/pandas-cut-method-in-python/\n",
    "# def explore_ranges_numberic_val(col,rangeList):\n",
    "#     print(col)\n",
    "#     rangeList.sort()\n",
    "#     return pd.cut(df_all[col],rangeList, right=True).value_counts()\n",
    "# range1= [0,24, 168, 360, 720,math.floor(df_all['open_to_close_hour'].max())]\n",
    "# print(explore_ranges_numberic_val('open_to_close_hour',range1))\n",
    "# print(\"=======================================================\")\n",
    "\n",
    "# range2= [0,24, 168, 360, 720,math.floor(df_all['response_to_resolved_hour'].max())]\n",
    "# print(explore_ranges_numberic_val('response_to_resolved_hour',range2))\n",
    "# print(\"=======================================================\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de4dc52-7540-493d-9ef1-0554b445e15d",
   "metadata": {},
   "source": [
    "# Plot Numeric  and Category columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "baae5996-3ac2-43a3-9fe3-c19b61937e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comment on google cloud\n",
    "if isExploreMode:\n",
    "    print(\"Plot Numeric  and Category columns\")\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot  as plt\n",
    "\n",
    "if isExploreMode:\n",
    "    df_all[numbericCols].plot(kind='box',subplots=True,layout=(1,len(numbericCols)),sharex=False, sharey=False,figsize=(10,5))\n",
    "    plt.show()\n",
    "    \n",
    "if isExploreMode:\n",
    "    barCols=rangeCols+[\"severity_name\"]\n",
    "    for col in barCols:\n",
    "        fig , ax = plt.subplots(figsize=(15,5))\n",
    "        ax =sns.countplot(x=col, data=df_all,)\n",
    "        for p in ax.patches:\n",
    "           ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n",
    "        plt.title(col)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c95b90d-adb7-4e88-8081-04fcc949a620",
   "metadata": {},
   "source": [
    "# Build Label and Split into Train/Test DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a785398d-9ccf-473e-9255-7eb9dde7f222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Build Unseen DataSet\n",
      "TrainData=False : 12 rows imported to pongthorn.SMartML.new2_incident successfully\n"
     ]
    }
   ],
   "source": [
    "if isTrainData:\n",
    "    \n",
    "    print(\" Build Label and Split into Train/Test DataSet\")\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # create label\n",
    "    map_severity_to_class={4:0,3: 1, 2: 2, 1: 3}\n",
    "    print(f\"Map severity id to LabelCode: {str(map_severity_to_class)}\")\n",
    "\n",
    "    df_all['label_multi_severity'] =df_all['severity_id'].map(map_severity_to_class) \n",
    "    def map_4to2_serverity(severity_id):\n",
    "        if severity_id==1 or severity_id==2:\n",
    "           return 1\n",
    "        else:\n",
    "           return 0 \n",
    "    df_all['label_binary_severity'] =df_all['severity_id'].apply(map_4to2_serverity)\n",
    "\n",
    "    print( list(df_all['label_multi_severity'].unique()))\n",
    "    print(df_all['label_binary_severity'].unique())\n",
    "\n",
    "    #Split data into 2 dataset to BQ\n",
    "    len_all=len(df_all)\n",
    "    print(f\"All Data = {len_all}\")\n",
    "    \n",
    "# shufflebool, default=True\n",
    "# Whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None.\n",
    "# stratifyarray-like, default=None\n",
    "# If not None, data is split in a stratified fashion, using this as the class labels. Read more in the User Guide.\n",
    "\n",
    "# Split the 80% of total as train\n",
    "# The remaining  will be splited equally 50% for valuation and the rest of later part is test\n",
    "    train, test = train_test_split(df_all,test_size=split_test,random_state=1000,shuffle=isShuffle) \n",
    "\n",
    "    len_all=len(df_all)\n",
    "    len_train=len(train)\n",
    "\n",
    "    len_test=len(test)\n",
    "    print(f'{len_train} =train examples ({round(len_train/len_all*100,1)}%)')\n",
    "    print(f'{len_test} =test examples ({round(len_test/len_all*100,1)}%)')\n",
    "\n",
    "    print(train.tail())\n",
    "\n",
    "    loadDataFrameToBQ(train_table_id,train)\n",
    "    loadDataFrameToBQ(test_tabel_id,test)  \n",
    "    \n",
    "    # if isExploreMode:\n",
    "    #     train.to_csv(f\"data/{train_name}.csv\",index=False)\n",
    "    #     test.to_csv(f\"data/{test_name}.csv\",index=False)\n",
    "        \n",
    "else:\n",
    "    print(\" Build Unseen DataSet\")\n",
    "    \n",
    "    loadDataFrameToBQ(new_incident_table_id,df_all)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "9dffecb0-2790-4d35-bae6-2da2c2871aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return \"Imported data to bigquery successfully\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f926af5-edc7-4fc9-84d9-33ffc24148d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-12.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-12:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
