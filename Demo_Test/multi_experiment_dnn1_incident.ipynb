{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd54814-e227-4302-9a81-3d6ddcf0e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "#https://codelabs.developers.google.com/codelabs/fraud-detection-ai-explanations?hl=en#0\n",
    "#he Explainable AI SDK and Copy Model to Deploy\n",
    "\n",
    "#https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive2/introduction_to_tensorflow/solutions/preprocessing_layers.ipynb\n",
    "#https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/custom/custom-tabular-bq-managed-dataset.ipynb\n",
    "#https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/experiments/comparing_local_trained_models.ipynb\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,DenseFeatures\n",
    "\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "from tensorflow.python.keras.utils import data_utils\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28a9538-ca23-45f0-817b-e4b067cf9ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cateCols=['sla','product_type','brand','service_type','incident_type']\n",
    "numbericCols=['open_to_close_hour','response_to_resolved_hour']\n",
    "\n",
    "unusedCols=['severity_id','severity_name','label_binary_severity']\n",
    "labelCol='label_multi_severity'\n",
    "\n",
    "model_dir='model'\n",
    "\n",
    "\n",
    "main_metric='accuracy'\n",
    "# df['label_multi_severity'] =df['severity_name'].map({'Cosmatic':0,'Minor': 1, \"Major\": 2, \"Critical\": 3}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddea3e1-b732-48c4-95fc-637108802a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ml_data(data_path):\n",
    " df=pd.read_csv(data_path)\n",
    " df =df.drop(columns=unusedCols)\n",
    " return df\n",
    "    \n",
    "root_path='../../data'    \n",
    "train = load_ml_data(f\"{root_path}/train_incident.csv\")\n",
    "# val=train.copy()\n",
    "val=load_ml_data(f\"{root_path}/validation_incident.csv\")\n",
    "# test =val.copy()\n",
    "test =load_ml_data(f\"{root_path}/test_incident.csv\")\n",
    "\n",
    "labelList=list(train[labelCol].unique())\n",
    "print(labelList)\n",
    "nLabel=len(labelList)\n",
    "print(f\"No target label : {nLabel}\")\n",
    "\n",
    "# sr_predict=df.iloc[-1,:]\n",
    "# df=df.iloc[0:len(df)-1,:]\n",
    "                 \n",
    "print(train.info())\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64adbe5e-d737-45a4-a04e-340a2926ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalPctEachTargetClass(dfx,colSev,colPctSev):\n",
    "    dfClassSummary=dfx.groupby([labelCol]).size().to_frame(colSev)\n",
    "    dfClassSummary[colPctSev]= dfClassSummary[colSev]/dfClassSummary[colSev].sum() *100\n",
    "    dfClassSummary=dfClassSummary.round(0)\n",
    "    return dfClassSummary\n",
    "\n",
    "pctDF1=CalPctEachTargetClass(train,'Train-No-Severity','Train-%-Severity')\n",
    "pctDF2=CalPctEachTargetClass(val,'Val-No-Severity','Val-%-Severity')\n",
    "pdcDF3=CalPctEachTargetClass(test,'Test-No-Severity','Test-%-Severity')\n",
    "pctDF=pd.concat([pctDF1,pctDF2,pdcDF3],axis=1)\n",
    "\n",
    "pctDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e9916-6668-4c4e-8974-af63c3f840b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_class(df):\n",
    "#     fig , ax = plt.subplots(figsize=(15,5))\n",
    "#     ax =sns.countplot(x=labelCol, data=df,)\n",
    "#     for p in ax.patches:\n",
    "#        ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n",
    "#     plt.title(labelCol.title())\n",
    "#     plt.show()\n",
    "    \n",
    "# plot_class(train)\n",
    "# plot_class(val)\n",
    "# plot_class(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d3a943-09d5-4a43-b8d8-633bc4a90353",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_label_df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  features = dataframe.copy()\n",
    "  labels = features.pop(labelCol)\n",
    "  labels  = tf.keras.utils.to_categorical(labels, num_classes=nLabel)\n",
    "    \n",
    "  ds = tf.data.Dataset.from_tensor_slices(( dict(features), labels ))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(features))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274122c2-9481-438b-b637-6e6ad393d809",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Explore Data from Tensor before keras data processing\")\n",
    "batch_size = 1\n",
    "train_ds =multiple_label_df_to_dataset(train, batch_size=batch_size)\n",
    "\n",
    "[(train_features, label_batch)] = train_ds.take(1)\n",
    "print(\"========Features==========\")\n",
    "print('Every feature:', list(train_features.keys()))\n",
    "print('product_type:', train_features['product_type'])\n",
    "print('open_to_close_hour:', train_features['open_to_close_hour'])\n",
    "print(\"========Labels==========\")\n",
    "print(f'{label_batch }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ad8837-968e-4d68-bdb6-1c7eba97e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalization_layer(name, dataset):\n",
    "  # Create a Normalization layer for our feature.\n",
    "  normalizer = preprocessing.Normalization(axis=None)\n",
    "\n",
    "  # Prepare a Dataset that only yields our feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the statistics of the data.\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237bce3c-9c77-4318-8682-fc36af0a6804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "  # Create a StringLookup layer which will turn strings into integer indices\n",
    "  if dtype == 'string':\n",
    "    index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
    "  else:\n",
    "    index = preprocessing.IntegerLookup(max_tokens=max_tokens)\n",
    "\n",
    "  # Prepare a Dataset that only yields our feature\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the set of possible values and assign them a fixed integer index.\n",
    "  index.adapt(feature_ds)\n",
    "\n",
    "  # Create a Discretization for our integer indices.\n",
    "  encoder = preprocessing.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "\n",
    "  # Apply one-hot encoding to our indices. The lambda function captures the\n",
    "  # layer so we can use them, or include them in the functional model later.\n",
    "  return lambda feature: encoder(index(feature))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab6c0ac-ee9b-42f3-b140-a102e8f2f121",
   "metadata": {},
   "outputs": [],
   "source": [
    "hour_col = train_features['open_to_close_hour']\n",
    "layer = get_normalization_layer('open_to_close_hour', train_ds)\n",
    "layer(hour_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c0de50-a1fb-44b9-b91f-555ac779ec40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# total no type+unkonw\n",
    "type_col = train_features['product_type']\n",
    "layer = get_category_encoding_layer('product_type', train_ds, 'string')\n",
    "layer(type_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa30d3fd-14e9-4c3d-a6aa-45d8114f5362",
   "metadata": {},
   "source": [
    "# Process Data  Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c78f01-000a-4d3e-9fbf-6f4660cfb75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =32\n",
    "train_ds = multiple_label_df_to_dataset (train, batch_size=batch_size)\n",
    "val_ds = multiple_label_df_to_dataset(val, batch_size=batch_size)\n",
    "test_ds = multiple_label_df_to_dataset(test, batch_size=batch_size)\n",
    "# for element in train_ds.as_numpy_iterator():\n",
    "#     print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9958d24-4d8f-4527-bc19-c657e1d9b22e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a69b6-d9c1-49d9-bc81-ed421d8700cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "encoded_features = []\n",
    "\n",
    "# Numeric features.\n",
    "for header in numbericCols:\n",
    "  print(header)  \n",
    "  numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "  normalization_layer = get_normalization_layer(header, train_ds)\n",
    "  encoded_numeric_col = normalization_layer(numeric_col)\n",
    "  all_inputs.append(numeric_col)\n",
    "  encoded_features.append(encoded_numeric_col)\n",
    "    \n",
    "    # Categorical features encoded as string.\n",
    "categorical_cols = cateCols\n",
    "for header in categorical_cols:\n",
    "  print(header)  \n",
    "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "  encoding_layer = get_category_encoding_layer(header, train_ds, dtype='string')\n",
    "                                        \n",
    "  encoded_categorical_col = encoding_layer(categorical_col)\n",
    "  all_inputs.append(categorical_col)\n",
    "  encoded_features.append(encoded_categorical_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa733595-628e-4a07-a6b8-719995fa535b",
   "metadata": {},
   "source": [
    "# Build and Train and Eveluate and Plot Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6af5c9-fc24-486a-bdd7-f6c4730dd236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model(num_units_1,num_units_layer2, dropout_rate):\n",
    "def build_model(num_units_layer1, dropout_rate):\n",
    "    all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "    x = tf.keras.layers.Dense(num_units_layer1, activation=\"relu\")(all_features)\n",
    "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    # if num_units_2>0:\n",
    "    #     x = tf.keras.layers.Dense(num_units_layer2, activation=\"relu\")(x)\n",
    "    #     x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    output = tf.keras.layers.Dense(nLabel,activation=tf.nn.softmax)(x)\n",
    "\n",
    "    model = tf.keras.Model(all_inputs, output)\n",
    "\n",
    "\n",
    "    model.compile(optimizer='adam',loss=tf.keras.losses.CategoricalCrossentropy(),metrics=[main_metric])\n",
    "    return model\n",
    "\n",
    "# METRICS = [\n",
    "#       keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "#       keras.metrics.Precision(name='precision'),\n",
    "#       keras.metrics.Recall(name='recall'),\n",
    "#       keras.metrics.AUC(name='auc'),\n",
    "# ]\n",
    "\n",
    "N_EARLY=11\n",
    "def train_model(model,x_epochs,x_batch_size):  \n",
    "    # model.summary()\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=N_EARLY, verbose=1)\n",
    "    history =model.fit(train_ds, validation_data=val_ds,epochs=x_epochs,batch_size=x_batch_size,callbacks = [early_stopping])\n",
    "    return history\n",
    "  \n",
    "    \n",
    "\n",
    "\n",
    "def plot_metrics(history):\n",
    "  plt.figure(figsize=(15,8))\n",
    "  colors = plt.rcParams['axes.prop_cycle'].by_key()['color']  \n",
    "  metrics =  ['loss', 'accuracy']\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    if metric == 'loss':\n",
    "      plt.ylim([0, plt.ylim()[1]])\n",
    "    elif metric == 'auc':\n",
    "      plt.ylim([0.8,1])\n",
    "    else:\n",
    "      plt.ylim([0,1])\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3816a15-6688-4c87-9d20-e5fc45d48d11",
   "metadata": {},
   "source": [
    "# Test Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25688a22-05f5-4cac-b662-d261b5dcfb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS =100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "print(\"1#Build model\")\n",
    "model=build_model(64,0.2)\n",
    "\n",
    "plot_metrics(history)\n",
    "\n",
    "print(\"2#Train model\")\n",
    "history=train_model(model,EPOCHS,BATCH_SIZE)\n",
    "print(\"=============================================================================\")\n",
    "print(\"3#Evaluate model\")\n",
    "loss, accuracy = model.evaluate(val_ds)\n",
    "print(\"Average Accuracy on Eveluation\", accuracy)\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"=============================================================================\")\n",
    "print(\"Average Accuracy  on Test\", accuracy)\n",
    "print(\"4#Explore Result model\")\n",
    "plot_metrics(history)\n",
    "print(\"=============================================================================\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810a3b32-0c99-46f1-947a-a398981bdd60",
   "metadata": {},
   "source": [
    "# Experidment on Vertext AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60ad2b8-ac6c-4203-bfed-167be852b73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://console.cloud.google.com/vertex-ai/experiments/experiments?project=pongthorn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22af6971-d598-47de-9e25-d4270a80e9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(project=PROJECT, location=REGION)\n",
    "vertex_ai.init(experiment=EXPERIMENT_NAME)\n",
    "# vertex_ai_tb = vertex_ai.Tensorboard.create()\n",
    "# vertex_ai.init(experiment=EXPERIMENT_NAME, experiment_tensorboard=vertex_ai_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c37a16-1cd5-4887-ba5c-b52a48fc159f",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    {\"num_units\": 8, \"dropout_rate\": 0.2, \"epochs\":50 ,\"batch_size\":32},\n",
    "    {\"num_units\": 16, \"dropout_rate\": 0.2, \"epochs\": 50,\"batch_size\":32},\n",
    "]\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeb8035-bf31-42cb-a80b-1a29e75922c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, params in enumerate(parameters):\n",
    "    print(params)\n",
    "    \n",
    "    # Initialize Vertex AI Experiment run   \n",
    "    vertex_ai.start_run(run=f\"incident expreriment by particular parameter-{i}\")  \n",
    "    vertex_ai.log_params(params)\n",
    "\n",
    "#     # Build model\n",
    "#     model = build_model(num_units_layer1=params[\"num_units\"], dropout_rate=params[\"dropout_rate\"])\n",
    "\n",
    "#     # Train model\n",
    "#     history = train_model(model,x_epochs=params[\"epochs\"],x_batch_size=params[\"batch_size\"])\n",
    "\n",
    "#     vertex_ai.log_params(history.params)\n",
    "\n",
    "#     # Log metrics per epochs\n",
    "#     for idx in range(0, history.params[\"epochs\"]):\n",
    "#         vertex_ai.log_time_series_metrics({f\"train_{main_metric}\": history.history[main_metric][idx]}\n",
    "#     )\n",
    "\n",
    "#     # Log final metrics\n",
    "#     loss_val, accuracy_val = model.evaluate(val_ds)\n",
    "#     vertex_ai.log_metrics({\"eval_loss\": loss_val, f\"eval_{main_metric}\": accuracy_val})\n",
    "\n",
    "#     loss_test, accuracy_test = model.evaluate(test_ds_ds)\n",
    "#     vertex_ai.log_metrics({\"test_loss\": loss_test, f\"test_{main_metric}\":accuracy_test})\n",
    "\n",
    "#     vertex_ai.end_run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6597f43-7bed-44dc-9391-e1aec84aa3f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a607edef-0ad0-4bc0-9f6e-3f746b9be5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217b56cb-75ff-4661-9145-fb59b8516b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499f77f8-03ed-4485-b0e6-7f6f5da16223",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a67b13a-63e9-4e8e-afae-b18a6cb0426b",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb8a1e7-3e2d-4ff6-9fd1-65a1a8ed8b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "press_y=input(f\"Press y=True and n=False to Save Mode if you are sastify : \") \n",
    "if press_y.lower()=='y':\n",
    " model.save(model_dir)\n",
    "else:\n",
    " quite()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f677c-dfa2-48b5-b980-6cc10f2e0033",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db2353b-b178-4e97-9ca8-ca5550569793",
   "metadata": {},
   "outputs": [],
   "source": [
    "press_y2=input(f\"Press y=True to reload model to predict test data : \") \n",
    "if press_y2.lower()=='y':\n",
    "  reloaded_model = tf.keras.models.load_model(model_dir)\n",
    "else:\n",
    " quite()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd916787-bacc-4537-b2b6-1c6d62932de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#label_multi_severity\n",
    "sample={\"sla\":\"24x7 4Hrs Response Time\",\n",
    "        \"product_type\":\"Server\",\n",
    "        \"brand\":\"VMWare\",                \n",
    "        \"service_type\":\"Incident\",\n",
    "        \"incident_type\":\"General Incident\",\n",
    "        \"open_to_close_hour\":10,\n",
    "        \"response_to_resolved_hour\":6.000000 \\\n",
    "       }\n",
    "\n",
    "print(sample)\n",
    "              \n",
    "print(\"===============================================================================================================\")    \n",
    "print(\"convert pain data to serdor as input to predict\")    \n",
    "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
    "print(input_dict)\n",
    "\n",
    "predictionList = reloaded_model.predict(input_dict)\n",
    "print(predictionList)\n",
    "prob = tf.nn.sigmoid(predictionList[0])\n",
    "print(f\"{(100 * prob)} %  as Severity\")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5587682-2265-4eff-985f-ca1f7852ddf4",
   "metadata": {},
   "source": [
    "# Copy Model From Local To GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaabd3ac-47ad-43ae-b6e5-fc16fecec238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://codelabs.developers.google.com/codelabs/fraud-detection-ai-explanations?hl=en#6\n",
    "press_y3=input(f\"Press y=True to save model to Google Cloud storage : \") \n",
    "if press_y3.lower()=='y':\n",
    "    MODEL_BUCKET = 'gs://tf1-incident-pongthorn'\n",
    "    MODEL_NAME = 'tf1_incident_multi_model'\n",
    "\n",
    "# !gsutil mb -l $REGION $MODEL_BUCKET\n",
    "# !gsutil -m cp -r ./$model_dir/* $MODEL_BUCKET/model\n",
    "else:\n",
    " quite()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc32a07-c2fc-4ff6-a771-fc3532630f01",
   "metadata": {},
   "source": [
    "# Import Model to Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9774f1-2293-43db-9464-47ea7f44b540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cloud.google.com/vertex-ai/docs/model-registry/import-model#get-operation\n",
    "\n",
    "#https://codelabs.developers.google.com/vertex-p2p-predictions#3\n",
    "# Import model manually\n",
    "#https://console.cloud.google.com/vertex-ai/models?project=pongthorn\n",
    "# Upload wand wait for vertex to complete process (Email notification)\n",
    "# if run package ( you need to specify precuild-pacage\n",
    "\n",
    "\n",
    "#Import model programactically\n",
    "# https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers\n",
    "\n",
    "# from google.cloud import aiplatform\n",
    "# DEPLOY_IMAGE = \"asia-docker.pkg.dev/vertex-ai/prediction/xgboost-cpu.1-6:latest\"\n",
    "\n",
    "# model = aip.Model.upload(\n",
    "#     display_name=\"ponghthorn_xgb_\" + TIMESTAMP,\n",
    "#     artifact_uri=MODEL_DIR,\n",
    "#     serving_container_image_uri=DEPLOY_IMAGE,\n",
    "#     sync=False,\n",
    "# )\n",
    "\n",
    "# model.wait()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2792371f-3e81-4717-95f5-4acff020cfb3",
   "metadata": {},
   "source": [
    "# Deploy Model to EndPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbcd51c-0dce-4b75-bc53-70539c352ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://codelabs.developers.google.com/vertex-p2p-predictions#4\n",
    "# Manual\n",
    "\n",
    "# Program\n",
    "# my_model = aiplatform.Model(\"projects/{PROJECT_NUMBER}/locations/us-central1/models/{MODEL_ID}\") \n",
    "\n",
    "# endpoint = my_model.deploy(\n",
    "#      deployed_model_display_name='my-endpoint',\n",
    "#      traffic_split={\"0\": 100},\n",
    "#      machine_type=\"n1-standard-4\",\n",
    "#      accelerator_count=0,\n",
    "#      min_replica_count=1,\n",
    "#      max_replica_count=1,\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a601d87-b648-4eea-b2de-b85b7723c5ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20082661-dc7f-414b-a759-3478163adcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy to AIPlatform (Deprecated)\n",
    "# MODEL = MODEL_NAME\n",
    "# VERSION = 'v1'\n",
    "# model_path = MODEL_BUCKET + '/model'\n",
    "\n",
    "# # !gcloud ai-platform models create $MODEL --region=$REGION\n",
    "\n",
    "# MACHINE_TYPE='n1-standard-2'\n",
    "\n",
    "# !gcloud  ai-platform versions create $VERSION --model $MODEL --origin $model_path --runtime-version 2.11 --framework TENSORFLOW --machine-type $MACHINE_TYPE --python-version 3.7 --region=$REGION\n",
    "# !gcloud ai-platform versions describe $VERSION --model $MODEL --region=$REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6465851a-f078-41a2-991c-80a71d3e695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0f6c5d-3ea4-4fae-ba78-b8decf134a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff433f46-0e98-48ec-8b32-25b840541253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
