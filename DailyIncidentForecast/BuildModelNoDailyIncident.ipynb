{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "import math \n",
    "from datetime import date, timedelta, datetime \n",
    "import time\n",
    "import os\n",
    "\n",
    "from pandas.plotting import register_matplotlib_converters \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates \n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import  MinMaxScaler \n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error \n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential \n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout \n",
    "from tensorflow.keras.callbacks import EarlyStopping \n",
    "from tensorflow.keras import backend\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "import joblib\n",
    "\n",
    "import keras_tuner as kt\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "  \n",
    "print('Tensorflow Version: ' + tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date='2022-01-01' \n",
    "end_date='2023-07-08'\n",
    "asset_name='Incident'   \n",
    "\n",
    "# univaiate  (single feature)\n",
    "prediction_col='count_incident'\n",
    "feature_cols=['count_incident']\n",
    "\n",
    "strStartMY=datetime.strptime(start_date,\"%Y-%m-%d\").strftime(\"%m%y\")\n",
    "strEndMY=datetime.strptime(end_date,\"%Y-%m-%d\").strftime(\"%m%y\")\n",
    "period_str=f\"M{strStartMY}-{strEndMY}\"\n",
    "data_path=f\"DailyIncident.csv\"\n",
    "print(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init ML Constant Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_col='date'\n",
    "\n",
    "main_obj_metric='mean_absolute_error'\n",
    "main_loss='mean_absolute_error'\n",
    "\n",
    "input_sequence_length =90  #30 60# 90 \n",
    "output_sequence_length =7  # 5 7\n",
    "train_ratio=0.8\n",
    "\n",
    "n_epochs =150  #50 #100/150/200\n",
    "n_batch_size = 32  # 16/32/64\n",
    "\n",
    "n_early=15\n",
    "\n",
    "seed=99\n",
    "\n",
    "#Tuning\n",
    "mx_step=3# 2,3,5,10\n",
    "mxMultipleStep=1  # double trail\n",
    "nExecutions_per_trial=3 #3  #averger re-run on 1 trail \n",
    "\n",
    "min_drop=0.0\n",
    "max_drop=  0.2 #0.1 0.2  0.5\n",
    "step_drop=0.1\n",
    "\n",
    "defaultMaxTrail=10\n",
    "\n",
    "ratio_traial_max= 3 #0.5 0.7/0.8/1\n",
    "\n",
    "model_path='train_model'\n",
    "\n",
    "tune_folder=f'{asset_name}_{input_sequence_length}To{output_sequence_length}_E{n_epochs}S{n_early}B{n_batch_size}'\n",
    "\n",
    "\n",
    "\n",
    "modelName=f\"{tune_folder}-{period_str}\"\n",
    "\n",
    "print(modelName)\n",
    "print(tune_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(data_path)\n",
    "# print(df.tail(input_sequence_length+output_sequence_length+2).reset_index(drop=True)[feature_cols])\n",
    "\n",
    "df.set_index(date_col,inplace=True)\n",
    "df=df[feature_cols]\n",
    "print(df.info())\n",
    "\n",
    "allCols=list(df.columns)\n",
    "print(allCols)\n",
    "\n",
    "print(df.head())\n",
    "print(df.tail())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ncols = 1\n",
    "nrows = len(feature_cols)\n",
    "fig, ax = plt.subplots(nrows=nrows, ncols=ncols, sharex=True, figsize=(15, 15))\n",
    "for i, ax in enumerate(fig.axes):\n",
    "        sns.lineplot(data = df.iloc[:, i], ax=ax)\n",
    "        ax.tick_params(axis=\"x\", rotation=30, labelsize=10, length=0)\n",
    "        ax.xaxis.set_major_locator(mdates.AutoDateLocator())\n",
    "        ax.title.set_text(df.columns[i])\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Split dataframe to show time-dependent data sequencial sample data point)\")\n",
    "# _dfTrain=df.loc['2015-01-01':'2021-12-31',:]\n",
    "# print(_dfTrain.shape)  \n",
    "# print(_dfTrain.head())\n",
    "# print(_dfTrain.tail())\n",
    "\n",
    "# print(\"=========================================================\")\n",
    "\n",
    "# _dfTest=df.loc['2022':,:]\n",
    "# print(_dfTest.shape)  \n",
    "# print(_dfTest.head())\n",
    "# print(_dfTest.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Data Proper To Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df):\n",
    "\n",
    "    print(\"Convert the data to numpy arrays value\")\n",
    "    np_feature_unscaled = np.array(df)\n",
    "    np_feature_unscaled = np.reshape(np_feature_unscaled, (df.shape[0], -1))\n",
    "    print(np_feature_unscaled.shape)\n",
    "    \n",
    "    np_pred_unscaled = np.array(df[prediction_col]).reshape(-1, 1)\n",
    "    print(np_pred_unscaled.shape)\n",
    "    \n",
    "    return np_feature_unscaled, np_pred_unscaled\n",
    "\n",
    "np_feature_unscaled, np_pred_unscaled = prepare_data(df)   \n",
    "\n",
    "\n",
    "print(\"Feature Data\",np_feature_unscaled.shape)\n",
    "print(np_feature_unscaled[:3])\n",
    "print(\"Prediction Data\",np_pred_unscaled.shape)\n",
    "print(np_pred_unscaled[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{input_sequence_length} = How far back the model looks back during training to forecast).\")\n",
    "print(f\"{output_sequence_length} = How far into the future the model forecasts the target value\")\n",
    "\n",
    "#How much ratio to split\n",
    "index_Prediction = df.columns.get_loc(prediction_col)\n",
    "print(f\"Prediction Index = {index_Prediction}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As a first step, we get the number of rows to train the model on 80% of the data \n",
    "train_data_length = math.ceil(np_feature_unscaled.shape[0] * train_ratio)\n",
    "print(f\"{train_ratio} = {train_data_length} Split the training data into train and train data sets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Target value column data disttribution\")\n",
    "df[prediction_col].plot.hist(figsize=(10, 8))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scale feature and prediction to a range Max-Min between 0 and 1\")\n",
    "scaler_train = MinMaxScaler()\n",
    "scaler_pred = MinMaxScaler()\n",
    "\n",
    "\n",
    "print(\"Split data and apply only train to scale\")\n",
    "\n",
    "train_data = np_feature_unscaled[0:train_data_length, :]\n",
    "test_data = np_feature_unscaled[train_data_length - input_sequence_length:, :]\n",
    "\n",
    "np_PredictVal_unScaled=np_pred_unscaled[0:train_data_length]\n",
    "\n",
    "print(\"Train Data to be scaled(feature and prediction)\") \n",
    "print(train_data.shape,np_PredictVal_unScaled.shape)\n",
    "print(train_data[:5])\n",
    "print(np_PredictVal_unScaled[:5])\n",
    "\n",
    "train_data=scaler_train.fit_transform(train_data)\n",
    "test_data=scaler_train.transform(test_data)\n",
    "\n",
    "print(f\"For predict test that take the past {input_sequence_length} train value to predict the first test value\")\n",
    "np_scaled=scaler_train.transform(np_feature_unscaled)\n",
    "np_scaled_prediction = scaler_pred.fit_transform(np_PredictVal_unScaled)\n",
    "\n",
    "print(\"==============Scaler Object For Features=================\")  \n",
    "print(f\"{scaler_train.n_features_in_} features are scaled in range 0-1 such as {feature_cols}\")\n",
    "print(f\"Max:{scaler_train.data_max_} -  Min:{scaler_train.data_min_}\")\n",
    "\n",
    "print(\"==============Scaler Object For Prediction Target Value=================\")  \n",
    "print(f\"{scaler_pred.n_features_in_} pred col is scaled in range 0-1 such as {prediction_col}\")\n",
    "print(f\"Max:{scaler_pred.data_max_} -  Min:{scaler_pred.data_min_}\")\n",
    "\n",
    "\n",
    "print(\"=========================================\")       \n",
    "print(\"Scaled data completely\")\n",
    "print(\"Scaled train and test data\")\n",
    "print(train_data.shape,test_data.shape)\n",
    "print(train_data[:5])\n",
    "print(test_data[:5])\n",
    "\n",
    "print(\"=========================================\")  \n",
    "print(\"Scale Prediction data (Scaled from Train data)\")\n",
    "print(np_scaled_prediction.shape)\n",
    "print(np_scaled_prediction[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Input data  as 3 dimension array (samples, time steps, features]\u000b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The LSTM RNN needs data with the format of [sample rows, time steps, features]\")\n",
    "def partition_dataset(input_sequence_length, output_sequence_length, data):\n",
    "    x, y = [], []\n",
    "    data_len = data.shape[0]\n",
    "    for i in range(input_sequence_length, data_len - output_sequence_length):\n",
    "        x.append(data[i-input_sequence_length:i,:])\n",
    "        y.append(data[i:i + output_sequence_length, index_Prediction]) \n",
    "    \n",
    "    # Convert the x and y to numpy arrays\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Here, we create {len(train_data)} samples, input_sequence_length time steps per sample, and features\")\n",
    "\n",
    "print(\"Generate training data and test data\")\n",
    "x_train, y_train = partition_dataset(input_sequence_length, output_sequence_length, train_data)\n",
    "x_test, y_test = partition_dataset(input_sequence_length, output_sequence_length, test_data)\n",
    "\n",
    "print(\"The shapes: the result is: (rows, training_sequence, features) (prediction value, )\")\n",
    "print(x_train.shape, y_train.shape)\n",
    "print(x_test.shape, y_test.shape)\n",
    "\n",
    "print(f\"Use {prediction_col} samples over the past {input_sequence_length} days to predict the future over the next {output_sequence_length} days\")\n",
    "print(x_train[-1:],y_train[-1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paramterter Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the neural network model\n",
    "# tf.random.set_seed(7)\n",
    "print(\"Keras Tuning Parameter Setting\")\n",
    "\n",
    "model = Sequential()\n",
    "n_output_neurons = output_sequence_length\n",
    "  \n",
    "inputshape_Timestamps= x_train.shape[1]\n",
    "no_feature=x_train.shape[2]\n",
    "neuron_number =inputshape_Timestamps * no_feature\n",
    "\n",
    "\n",
    "step_neuron=int(neuron_number)\n",
    "max_neuron=int(step_neuron*mx_step)\n",
    "print(f\"Train Row : {x_train.shape[0]}\")\n",
    "print(f\"{inputshape_Timestamps}(backward step) x {no_feature}(features) = {neuron_number}, it is total no.NN  to predict future {n_output_neurons} with Hidden Node Unit from {step_neuron} to {max_neuron}\")\n",
    "    \n",
    "\n",
    "\n",
    "neuron_list = [*range(neuron_number, max_neuron+step_neuron, step_neuron)]\n",
    "\n",
    "drop_list=np.arange (min_drop, max_drop ,step_drop)\n",
    "drop_list=np.append(drop_list,[max_drop])\n",
    "\n",
    "allPosComb=len(neuron_list)*len(drop_list)\n",
    "print(f\"{neuron_list} and {drop_list}\")\n",
    "print(f\"all combination : {len(neuron_list)} x {len(drop_list)} = {allPosComb}\")\n",
    "\n",
    "nMax_trials=math.ceil(allPosComb*ratio_traial_max)\n",
    "nMax_trials=nMax_trials*mxMultipleStep\n",
    "\n",
    "# if nMax_trials<defaultMaxTrail: \n",
    "#     nMax_trials=defaultMaxTrail\n",
    "\n",
    "nMax_trials=nMax_trials*1\n",
    "print(\"MAx run :\" ,nMax_trials , \"each trail run to essemble \",nExecutions_per_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetWork Architeture Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_way(hp):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(LSTM( hp.Int('input_unit',min_value=neuron_number ,max_value=max_neuron,step=step_neuron) \\\n",
    "                   , return_sequences=False, input_shape=(inputshape_Timestamps,no_feature)))\n",
    "    \n",
    "    model.add(Dropout(hp.Float('Dropout_rate',min_value=min_drop,max_value=max_drop,step=step_drop))) \n",
    "    \n",
    "    model.add(Dense(n_output_neurons))\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=main_loss, metrics=[main_obj_metric])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buildtime = datetime.now().strftime('%d%m%y_%H%M')\n",
    "project_model=f\"{buildtime}\"\n",
    "print(project_model)\n",
    "\n",
    "\n",
    "t_Start=time.time()\n",
    "\n",
    "print(f\"Start tund at {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor='val_loss', patience=n_early, verbose=1)\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "# tuner = kt.Hyperband(\n",
    "# tuner= kt.RandomSearch(\n",
    "        build_model_way,\n",
    "        objective=main_obj_metric,\n",
    "        max_trials=nMax_trials,\n",
    "        seed=seed,\n",
    "        executions_per_trial=nExecutions_per_trial,\n",
    "        directory=f\"tuning/{tune_folder}/\",\n",
    "        project_name= project_model\n",
    "   \n",
    "        )\n",
    "tuner.search(x_train, y_train, batch_size=n_batch_size, epochs=n_epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        shuffle=False,verbose=1,callbacks=[early_stop])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Space to search\")\n",
    "tuner.search_space_summary()\n",
    "print(f\"Suumary of {nMax_trials}\")\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_End=time.time()\n",
    "t_elapsed=(t_End-t_Start)/60/60\n",
    "print('Total execution : ',round(t_elapsed,2)) \n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Optimal HyperParamters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(best_hps.values)\n",
    "print(\"build model from  the best tuning\")\n",
    "\n",
    "# model=tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the TUNED Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training the Best Model with early stop at {n_early}\")\n",
    "t_Start=time.time()\n",
    "print(datetime.now())\n",
    "\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=n_early, verbose=1)\n",
    "history = model.fit(x_train, y_train, \n",
    "                batch_size=n_batch_size, \n",
    "                epochs=n_epochs,\n",
    "                shuffle=False,\n",
    "                validation_data=(x_test, y_test),\n",
    "                verbose=2, callbacks=[early_stop]\n",
    "               )\n",
    "        \n",
    "t_End=time.time()\n",
    "t_elapsed=(t_End-t_Start)/60/60\n",
    "print('Total execute making report : ',round(t_elapsed,2)) \n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_per_epoch = history.history[main_obj_metric]\n",
    "best_epoch = len(val_per_epoch) + 1\n",
    "print('Best epoch: %d' % (best_epoch,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"List Metrics\")\n",
    "for key in history.history.keys():\n",
    "    print(key)\n",
    "print(\"=====================================================\")    \n",
    "# Evaluate the model on the test data using `evaluate`\n",
    "#https://www.tensorflow.org/guide/keras/train_and_evaluate\n",
    "print(\"Evaluate on test data\")\n",
    "results = model.evaluate(x_test, y_test, batch_size=n_batch_size)\n",
    "print(\"test loss, test acc:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(main_obj_metric)\n",
    "plt.plot(history.history[main_obj_metric], label='train')\n",
    "plt.plot(history.history[f'val_{main_obj_metric}'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(f\"Train/Test Loss - {main_loss}\")\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step #5 Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted values\n",
    "y_pred_scaled = model.predict(x_test)\n",
    "# print(y_pred_scaled.shape)\n",
    "\n",
    "# Unscale the predicted values\n",
    "y_pred = scaler_pred.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler_pred.inverse_transform(y_test).reshape(-1, output_sequence_length)\n",
    "print(y_pred.shape)\n",
    "print(y_test_unscaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTestActual=pd.DataFrame(data=y_test_unscaled, \\\n",
    "                          columns=[ f\"Step{x}-{prediction_col}\"  for x in range(1,output_sequence_length+1,1)])\n",
    "dfTestPred=pd.DataFrame(data=y_pred, \\\n",
    "                          columns=[ f\"Step{x}-Pred-{prediction_col}\"  for x in range(1,output_sequence_length+1,1)]) \n",
    "dfActualPred=pd.concat([dfTestActual,dfTestPred],axis=1)\n",
    "ActualPredcols=sorted(dfActualPred.columns.tolist())\n",
    "dfActualPred=dfActualPred[ActualPredcols]\n",
    "dfActualPred.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Absolute Error (MAE)\n",
    "MAE = mean_absolute_error(y_test_unscaled, y_pred)\n",
    "print(f'Mean Absolute Error (MAE): {np.round(MAE, 2)}')\n",
    "print(\"===========================================================================\")\n",
    "\n",
    "MAPE = np.mean((np.abs(np.subtract(y_test_unscaled, y_pred)/ y_test_unscaled))) * 100\n",
    "print(f'Mean Absolute Percentage Error (MAPE): {np.round(MAPE, 2)} %')\n",
    "\n",
    "\n",
    "# Root Mean  Squre Error\n",
    "# RMSE = math.sqrt( mean_squared_error(y_test_unscaled, y_pred))\n",
    "# print(f'Root MEAN Square Error(RMSE): {np.round(RMSE, 2)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Multiple Forcast (Most Complex Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_df(i, x, y, y_pred_unscaled):\n",
    "    # Undo the scaling on x, reshape the testset into a one-dimensional array, so that it fits to the pred scaler\n",
    "    x_test_unscaled_df = pd.DataFrame(scaler_pred.inverse_transform((x[i]))[:,index_Prediction])\\\n",
    "    .rename(columns={0:'x_test'})\n",
    "    \n",
    "    y_test_unscaled_df = []\n",
    "    # Undo the scaling on y\n",
    "    if type(y) == np.ndarray:\n",
    "        y_test_unscaled_df = pd.DataFrame(scaler_pred.inverse_transform(y)[i]).rename(columns={0:'y_test'})\n",
    "\n",
    "    # Create a dataframe for the y_pred at position i, y_pred is already unscaled\n",
    "    y_pred_df = pd.DataFrame(y_pred_unscaled[i]).rename(columns={0:'y_pred'})\n",
    "    return x_test_unscaled_df, y_pred_df, y_test_unscaled_df\n",
    "\n",
    "\n",
    "def plot_multi_test_forecast(x_test_unscaled_df, y_test_unscaled_df, y_pred_df, title): \n",
    "    # Package y_pred_unscaled and y_test_unscaled into a dataframe with columns pred and true   \n",
    "    if type(y_test_unscaled_df) == pd.core.frame.DataFrame:\n",
    "        df_merge = y_pred_df.join(y_test_unscaled_df, how='left')\n",
    "    else:\n",
    "        df_merge = y_pred_df.copy()\n",
    "    \n",
    "    # Merge the dataframes \n",
    "    df_merge_ = pd.concat([x_test_unscaled_df, df_merge]).reset_index(drop=True)\n",
    "    \n",
    "    # Plot the linecharts\n",
    "    fig, ax = plt.subplots(figsize=(20, 8))\n",
    "    plt.title(title, fontsize=12)\n",
    "    ax.set(ylabel = f\"{asset_name}-{prediction_col}\")\n",
    "    sns.lineplot(data = df_merge_, linewidth=2.0, ax=ax)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step #6 Create a new Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_latest_batch = np_scaled[-(input_sequence_length+1):-1,:].reshape(1,input_sequence_length,len(feature_cols))\n",
    "print(f\"{x_test_latest_batch.shape} is the latest input batch from the test dataset, which is contains the price values for the last {input_sequence_length} trading days\")\n",
    "\n",
    "x_test_latest_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_scaled = model.predict(x_test_latest_batch)\n",
    "y_pred_unscaled = scaler_pred.inverse_transform(y_pred_scaled)\n",
    "print(f\"Predict on the inverse transformed batch {y_pred_unscaled.shape}\")\n",
    "print(y_pred_unscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Prepare the data and plot the input data and the predictions\")\n",
    "x_test_unscaled_df, y_test_unscaled_df, _ = prepare_df(0, x_test_latest_batch, '', y_pred_unscaled)\n",
    "print(f\"X-Test= {x_test_unscaled_df.shape}\")\n",
    "print(f\"Y-Test= {y_test_unscaled_df.shape}\")\n",
    "\n",
    "# print(x_test_unscaled_df.tail(len(y_test_unscaled_df)))\n",
    "# print(y_test_unscaled_df.tail(len(y_test_unscaled_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_multi_test_forecast(x_test_unscaled_df, '',y_test_unscaled_df, \"x_new Vs. y_new_pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAll=df.sort_values(by=[date_col]).copy()\n",
    "npAll_unscaled, npPredictCol_unscaled = prepare_data(dfAll) \n",
    "print(npAll_unscaled.shape,npPredictCol_unscaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalerFinalTrain = MinMaxScaler()\n",
    "scalerFinalPred = MinMaxScaler()\n",
    "npAll_scaled = scalerFinalTrain.fit_transform(npAll_unscaled)\n",
    "npAllPredictCol_scaled = scalerFinalPred.fit_transform(npPredictCol_unscaled)\n",
    "print(npAll_scaled.shape,npAllPredictCol_scaled.shape)\n",
    "print(npAll_scaled[:2])\n",
    "print(npAllPredictCol_scaled[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_all, y_all = partition_dataset(input_sequence_length, output_sequence_length,npAll_scaled)\n",
    "print(f\"We will tranin {x_all.shape},{y_all.shape} with batch={n_batch_size} and best epoch={best_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training All\")\n",
    "t_Start=time.time()\n",
    "print(datetime.now())\n",
    "\n",
    "history_final=model.fit(x=x_all, y=y_all, batch_size=n_batch_size, epochs=best_epoch,shuffle=False)\n",
    "        \n",
    "t_End=time.time()\n",
    "t_elapsed=(t_End-t_Start)/60/60\n",
    "print('Total execute making report : ',round(t_elapsed,2)) \n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "fig, ax = plt.subplots(figsize=(10, 5), sharex=True)\n",
    "plt.plot(history_final.history[\"loss\"])\n",
    "plt.title(\"Model loss\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "ax.xaxis.set_major_locator(plt.MaxNLocator(n_epochs))\n",
    "plt.legend([\"All Data\"], loc=\"upper left\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Save model and scaler\")\n",
    "\n",
    "#modelName='test2022-20for10'\n",
    "file_model=f'{model_path}/{modelName}.h5'\n",
    "file_scaler=f'{model_path}/scaler_{modelName}.gz'\n",
    "file_scaler_pred=f'{model_path}/scaler_pred_{modelName}.gz'\n",
    "\n",
    "model.save(file_model)\n",
    "\n",
    "joblib.dump(scalerFinalTrain,file_scaler)\n",
    "joblib.dump(scalerFinalPred ,file_scaler_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "cdb995469269d53d5ebc14eb4bb889bd554e0493b704e0e84f3a885021cdd43e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
