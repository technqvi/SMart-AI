{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ae199c-8589-46ed-9d1b-43955d41ca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guide for Tuturial\n",
    "#https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/prediction/custom_batch_prediction_feature_filter.ipynb\n",
    "#https://cloud.google.com/vertex-ai/docs/tutorials/train-tensorflow-bigquery\n",
    "#https://cloud.google.com/vertex-ai/docs/tutorials/tabular-bq-prediction\n",
    "\n",
    "#https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/custom/custom-tabular-bq-managed-dataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2671a710-86f5-4cac-a321-023064dc86f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from typing import Tuple, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ccaf78-6aa4-46fe-a65d-6aa5c9008bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "TRAIN_STRATEGY = \"single\"\n",
    "\n",
    "\n",
    "mean_and_std_json_file=\"incident_mean_and_std.json\"\n",
    "\n",
    "LABEL_COLUMN = \"severity_name\"\n",
    "UNUSED_COLUMNS = ['severity_id','label_binary_severity','label_multi_severity']\n",
    "\n",
    "model_dir='model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec911c00-ea96-4977-a7f7-b521eb2a2433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_data_bq(sql:str):\n",
    "#  client_bq = bigquery.Client()\n",
    "#  query_result=client_bq.query(sql)\n",
    "#  df=query_result.to_dataframe()\n",
    "#  return df\n",
    "\n",
    "# def download_table(bq_table_uri: str):\n",
    "#     # Remove bq:// prefix if present\n",
    "#     bqclient= bigquery.Client()\n",
    "#     prefix = \"bq://\"\n",
    "#     if bq_table_uri.startswith(prefix):\n",
    "#         bq_table_uri = bq_table_uri[len(prefix) :]\n",
    "\n",
    "#     table = bigquery.TableReference.from_string(bq_table_uri)\n",
    "#     rows = bqclient.list_rows(\n",
    "#         table,\n",
    "#     )\n",
    "#     return rows.to_dataframe()\n",
    "\n",
    "# df = download_table(\"pongthorn.SMartML.TrainEval_Incident_20230316\")\n",
    "\n",
    "def load_ml_data(data_path):\n",
    " df=pd.read_csv(data_path)\n",
    " df =df.drop(columns=UNUSED_COLUMNS)\n",
    " return df\n",
    "\n",
    "root_path='../../data'    \n",
    "\n",
    "dfAll=pd.read_csv(f\"{root_path}/ML_Incident_20230316.csv\",\n",
    "                  usecols=['severity_name','sla','product_type','brand','service_type','incident_type'])\n",
    "\n",
    "df_train = load_ml_data(f\"{root_path}/train_incident.csv\")\n",
    "# val=train.copy()\n",
    "df_validation=load_ml_data(f\"{root_path}/validation_incident.csv\")\n",
    "# test =val.copy()\n",
    "df_test =load_ml_data(f\"{root_path}/test_incident.csv\")\n",
    "\n",
    "# sr_predict=df.iloc[-1,:]\n",
    "# df=df.iloc[0:len(df)-1,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a8902a-ffc1-4cf6-8e42-2fc15bf64a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfAll.info())\n",
    "dfAll.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a19a8-7d21-4a50-92ac-5edfe110d8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_label=dfAll[LABEL_COLUMN].unique()\n",
    "print(list_label)\n",
    "\n",
    "cate_sla=dfAll['sla'].unique()\n",
    "print(cate_sla)\n",
    "\n",
    "cate_productType=dfAll['product_type'].unique()\n",
    "print(cate_productType)\n",
    "\n",
    "cate_brand=dfAll['brand'].unique()\n",
    "print(cate_brand)\n",
    "\n",
    "cate_serviceType=dfAll['service_type'].unique()\n",
    "print(cate_serviceType)\n",
    "\n",
    "cate_incidentType=dfAll['incident_type'].unique()\n",
    "print(cate_incidentType)\n",
    "\n",
    "\n",
    "_CATEGORICAL_TYPES = {  \n",
    "    LABEL_COLUMN:pd.api.types.CategoricalDtype(categories=list_label),\n",
    "    \"sla\": pd.api.types.CategoricalDtype(categories=cate_sla),\n",
    "    \"product_type\": pd.api.types.CategoricalDtype(categories=cate_productType),\n",
    "    \"brand\": pd.api.types.CategoricalDtype(categories=cate_brand),\n",
    "    \"service_type\": pd.api.types.CategoricalDtype(categories=cate_serviceType),\n",
    "    \"incident_type\": pd.api.types.CategoricalDtype(categories=cate_incidentType),\n",
    "}\n",
    "#print(_CATEGORICAL_TYPES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f9da3-2e23-495c-9f3f-23b95cfe645d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_mean_and_std(mean_and_std_json_file):\n",
    "    \"\"\"Download mean and std for each column\"\"\"\n",
    "    import json\n",
    "    file_path=mean_and_std_json_file\n",
    "    # bucket, file_path = extract_bucket_and_prefix_from_gcs_path(mean_and_std_json_file)\n",
    "    # download_blob(bucket_name=bucket, source_blob_name=file_path, destination_file_name=file_path)\n",
    "    \n",
    "    with open(file_path, 'r') as file:\n",
    "        return json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670db99c-42d4-4456-9150-052fe70b6f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    \"\"\"Converts categorical features to numeric. Removes unused columns.\n",
    "\n",
    "    Args:\n",
    "      df: Pandas df with raw data\n",
    "\n",
    "    Returns:\n",
    "      df with preprocessed data\n",
    "    \"\"\"\n",
    "\n",
    "    # Drop rows with NaN's\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Convert integer valued (numeric) columns to floating point\n",
    "    numeric_columns = df.select_dtypes([\"int32\", \"float32\", \"float64\"]).columns\n",
    "    df[numeric_columns] = df[numeric_columns].astype(\"float32\")\n",
    "\n",
    "    # Convert categorical columns to numeric\n",
    "    cat_columns = df.select_dtypes([\"object\"]).columns\n",
    "\n",
    "    df[cat_columns] = df[cat_columns].apply(\n",
    "        lambda x: x.astype(_CATEGORICAL_TYPES[x.name])\n",
    "    )\n",
    "    df[cat_columns] = df[cat_columns].apply(lambda x: x.cat.codes)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6462d09-a694-4b37-b789-027e457ef155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(df, mean_and_std):\n",
    "    \"\"\"Scales numerical columns using their means and standard deviation to get\n",
    "    z-scores: the mean of each numerical column becomes 0, and the standard\n",
    "    deviation becomes 1. This can help the model converge during training.\n",
    "\n",
    "    Args:\n",
    "      df: Pandas df\n",
    "\n",
    "    Returns:\n",
    "      Input df with the numerical columns scaled to z-scores\n",
    "    \"\"\"\n",
    "    dtypes = list(zip(df.dtypes.index, map(str, df.dtypes)))\n",
    "    # Normalize numeric columns.\n",
    "    for column, dtype in dtypes:\n",
    "        if dtype == \"float32\":\n",
    "            df[column] -= mean_and_std[column][\"mean\"]\n",
    "            df[column] /= mean_and_std[column][\"std\"]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc947ca4-0df0-46a8-8d8d-bf5bab072fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataframe_to_dataset(\n",
    "    df_train,\n",
    "    df_validation,\n",
    "    mean_and_std\n",
    "):\n",
    "\n",
    "    df_train_x, df_train_y = df_train, df_train.pop(LABEL_COLUMN)\n",
    "    df_validation_x, df_validation_y = df_validation, df_validation.pop(LABEL_COLUMN)\n",
    "\n",
    "    # Join train_x and eval_x to normalize on overall means and standard\n",
    "    # deviations. Then separate them again.\n",
    "    all_x = pd.concat([df_train_x, df_validation_x], keys=[\"train\", \"eval\"])\n",
    "    all_x = standardize(all_x, mean_and_std)\n",
    "    df_train_x, df_validation_x = all_x.xs(\"train\"), all_x.xs(\"eval\")\n",
    "\n",
    "    y_train = np.asarray(df_train_y).astype(\"float32\")\n",
    "    y_validation = np.asarray(df_validation_y).astype(\"float32\")\n",
    "\n",
    "    # Convert to numpy representation\n",
    "    x_train = np.asarray(df_train_x)\n",
    "    x_test = np.asarray(df_validation_x)\n",
    "\n",
    "    # Convert to one-hot representation\n",
    "    y_train = tf.keras.utils.to_categorical(y_train, num_classes=len(list_label))\n",
    "    y_validation = tf.keras.utils.to_categorical(y_validation, num_classes=len(list_label))\n",
    "    \n",
    "    print(x_train.shape,y_train.shape, x_test.shape,y_validation.shape)\n",
    "    \n",
    "    # return   x_train,y_train, x_test,y_validation\n",
    "\n",
    "    dataset_train = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    dataset_validation = tf.data.Dataset.from_tensor_slices((x_test, y_validation))\n",
    "    \n",
    "    return (dataset_train, dataset_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05e780a-f88a-4770-ae6f-abde7b242706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_features,num_classes):\n",
    "    # Create model\n",
    "    Dense = tf.keras.layers.Dense\n",
    "    model = tf.keras.Sequential(\n",
    "        [\n",
    "            Dense(\n",
    "                32,\n",
    "                activation=tf.nn.relu,\n",
    "                input_dim=num_features,\n",
    "            ),\n",
    "            Dense(32, activation=tf.nn.relu),\n",
    "            Dense(num_classes, activation=tf.nn.softmax),\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Compile Keras model\n",
    "    # optimizer = tf.keras.optimizers.RMSprop(lr=0.001)\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\", metrics=[\"accuracy\"], optimizer='adam'\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e68102-686a-471b-98c3-31cae4cb3bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_and_std = download_mean_and_std(mean_and_std_json_file)\n",
    "print(mean_and_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c4ae00-8223-4503-9abf-f6906fe9470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = preprocess(df_train)\n",
    "df_validation = preprocess(df_validation)\n",
    "\n",
    "print(df_train.info())\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfc2224-809c-4f55-be45-932359bc9888",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train, dataset_validation = convert_dataframe_to_dataset(\n",
    "  df_train, \n",
    "  df_validation, \n",
    "  mean_and_std\n",
    ")\n",
    "dataset_train = dataset_train.shuffle(len(df_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d141b4-6afc-4542-bb7f-ce7035490dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model( num_features=dataset_train._flat_shapes[0].dims[0].value,num_classes=len(list_label))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effae586-27b5-49b9-b838-5ce2d60d483f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset_train.batch(BATCH_SIZE)\n",
    "dataset_validation = dataset_validation.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5c716d-ae5a-4d6b-8a1d-910a7832938e",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "history=model.fit(dataset_train, epochs=EPOCHS, validation_data=dataset_validation,batch_size=BATCH_SIZE,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9d75aa-9891-4037-bbb8-71aae66fb433",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(dataset_validation)\n",
    "print(\"Average Accuracy on Eveluation\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d3ef03-bb34-40e4-83c6-fb3d3f54587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.saved_model.save(model, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5efddd5-ff55-422a-8df2-85d1d85e279d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01abd76d-acb0-4231-b4bc-6df3b8a2ccd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f85005-bed9-48ea-8d9e-66b5b038cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_test = preprocess(df_test)\n",
    "print(df2_test.info())\n",
    "df2_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148d6ce2-4ead-422e-817f-b95ecf70a3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dataframe_to_list(df, mean_and_st):\n",
    "    df = preprocess(df)\n",
    "\n",
    "    df_x, df_y = df, df.pop(LABEL_COLUMN)\n",
    "\n",
    "    # Normalize on overall means and standard deviations.\n",
    "    df = standardize(df, mean_and_std)\n",
    "\n",
    "    y = np.asarray(df_y).astype(\"float32\")\n",
    "\n",
    "    # Convert to numpy representation\n",
    "    x = np.asarray(df_x)\n",
    "\n",
    "    # Convert to one-hot representation\n",
    "    return x.tolist(), y.tolist(), df_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19a15ad-be5e-4fa3-b82f-98d3d71293a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test, df_x = convert_dataframe_to_list(df2_test, mean_and_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8d8501-f62a-4faa-926b-91633a13c414",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_COLUMN_NAME = \"id\"\n",
    "df_x_with_id = df_x.copy()\n",
    "df_x_with_id[ID_COLUMN_NAME] = [i for i in range(0, df_x_with_id.shape[0])]\n",
    "\n",
    "# Print columns of the datafram\n",
    "print(f\"Test dataset columns: {df_x_with_id.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb20bfc9-6ce5-42b7-8394-45312061f2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://codelabs.developers.google.com/vertex-xgb-wit#7\n",
    "#https://codelabs.developers.google.com/vertex-p2p-predictions#3\n",
    "\n",
    "#https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_registry/get_started_with_model_registry.ipynb\n",
    "#https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive2/introduction_to_tensorflow/solutions/1_training_at_scale_vertex.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7fddf6-85f0-4c70-90b8-9c289660b606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
