{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300e7850-0b43-4731-9253-684ba1a6a8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://codelabs.developers.google.com/codelabs/fraud-detection-ai-explanations?hl=en#0\n",
    "#he Explainable AI SDK and Copy Model to Deploy\n",
    "\n",
    "#https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive2/introduction_to_tensorflow/solutions/preprocessing_layers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd54814-e227-4302-9a81-3d6ddcf0e9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,DenseFeatures\n",
    "\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28a9538-ca23-45f0-817b-e4b067cf9ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cateCols=['sla','product_type','brand','service_type','incident_type']\n",
    "numbericCols=['open_to_close_hour','response_to_resolved_hour']\n",
    "\n",
    "unusedCols=['severity_id','severity_name','label_multi_severity']\n",
    "labelCol='label_binary_severity'\n",
    "\n",
    "\n",
    "model_dir='model_binary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a2593-c0b1-4400-ba85-4f78a2db5b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from BigQuery\n",
    "projectId='pongthorn'\n",
    "dataset_id='DemoSMartDW'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00050eef-6eb7-4869-a90c-10382f285775",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_name='train_incident'\n",
    "validation_name='validation_incident'\n",
    "test_name='test_incident'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee731e7-2d6a-452c-9868-f73d09e78c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_ml_data(data_path):\n",
    " df=pd.read_csv(data_path)\n",
    " df =df.drop(columns=unusedCols)\n",
    " df[labelCol]=df[labelCol].astype('int64')   \n",
    " return df\n",
    "\n",
    "def load_data_bq(sql:str):\n",
    " \n",
    " query_result=client.query(sql)\n",
    " df=query_result.to_dataframe()\n",
    " df =df.drop(columns=unusedCols)\n",
    " df[labelCol]=df[labelCol].astype('int64')   \n",
    " return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57ff6a1-7e79-46f7-9bba-3ff6b0d5a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_table_id=f\"{projectId}.{dataset_id}.{train_name}\"\n",
    "val_tabel_id=f\"{projectId}.{dataset_id}.{validation_name}\"\n",
    "test_tabel_id=f\"{projectId}.{dataset_id}.{test_name}\"\n",
    "\n",
    "client = bigquery.Client(project=projectId)\n",
    "\n",
    "train=load_data_bq(f\"SELECT * FROM {train_table_id}\")\n",
    "val=load_data_bq(f\"SELECT * FROM {val_tabel_id}\")\n",
    "test=load_data_bq(f\"SELECT * FROM {test_tabel_id}\")\n",
    "\n",
    "\n",
    "# root_path='../../data'    \n",
    "# train = load_ml_data(f\"{root_path}/{train_name}.csv\")\n",
    "# val=load_ml_data(f\"{root_path}/{validation_name}.csv\")\n",
    "# test =load_ml_data(f\"{root_path}/{test_name}.csv\")\n",
    "\n",
    "print(train.shape)\n",
    "print(train.info())\n",
    "print(val.shape)\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddea3e1-b732-48c4-95fc-637108802a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "labelList=list(train[labelCol].unique())\n",
    "print(labelList)\n",
    "nLabel=len(labelList)\n",
    "print(f\"No target label : {nLabel}\")\n",
    "\n",
    "# sr_predict=df.iloc[-1,:]\n",
    "# df=df.iloc[0:len(df)-1,:]\n",
    "                 \n",
    "print(train.info())\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b6f44a-0672-46d9-812d-6bac766765d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CalPctEachTargetClass(dfx,colSev,colPctSev):\n",
    "    dfClassSummary=dfx.groupby([labelCol]).size().to_frame(colSev)\n",
    "    dfClassSummary[colPctSev]= dfClassSummary[colSev]/dfClassSummary[colSev].sum() *100\n",
    "    dfClassSummary=dfClassSummary.round(0)\n",
    "    return dfClassSummary\n",
    "\n",
    "pctDF1=CalPctEachTargetClass(train,'Train-No-Severity','Train-%-Severity')\n",
    "pctDF2=CalPctEachTargetClass(val,'Val-No-Severity','Val-%-Severity')\n",
    "pdcDF3=CalPctEachTargetClass(test,'Test-No-Severity','Test-%-Severity')\n",
    "pctDF=pd.concat([pctDF1,pctDF2,pdcDF3],axis=1)\n",
    "\n",
    "pctDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e9916-6668-4c4e-8974-af63c3f840b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_class(df):\n",
    "    fig , ax = plt.subplots(figsize=(15,5))\n",
    "    ax =sns.countplot(x=labelCol, data=df,)\n",
    "    for p in ax.patches:\n",
    "       ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n",
    "    plt.title(labelCol.title())\n",
    "    plt.show()\n",
    "    \n",
    "plot_class(train)\n",
    "plot_class(val)\n",
    "plot_class(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b44b56b-79a8-46e4-ace9-f44040e72b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def binary_label_df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop(labelCol)\n",
    "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ad8837-968e-4d68-bdb6-1c7eba97e69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_normalization_layer(name, dataset):\n",
    "  # Create a Normalization layer for our feature.\n",
    "  normalizer = preprocessing.Normalization(axis=None)\n",
    "\n",
    "  # Prepare a Dataset that only yields our feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the statistics of the data.\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237bce3c-9c77-4318-8682-fc36af0a6804",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "  # Create a StringLookup layer which will turn strings into integer indices\n",
    "  if dtype == 'string':\n",
    "    index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
    "  else:\n",
    "    index = preprocessing.IntegerLookup(max_tokens=max_tokens)\n",
    "\n",
    "  # Prepare a Dataset that only yields our feature\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the set of possible values and assign them a fixed integer index.\n",
    "  index.adapt(feature_ds)\n",
    "\n",
    "  # Create a Discretization for our integer indices.\n",
    "  encoder = preprocessing.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "\n",
    "  # Apply one-hot encoding to our indices. The lambda function captures the\n",
    "  # layer so we can use them, or include them in the functional model later.\n",
    "  return lambda feature: encoder(index(feature))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79150379-8d52-43eb-92fa-3a13d3c57633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa30d3fd-14e9-4c3d-a6aa-45d8114f5362",
   "metadata": {},
   "source": [
    "# Process Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3122dc-b371-46ec-bd8c-82e81eb44ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =32\n",
    "train_ds = binary_label_df_to_dataset (train, batch_size=batch_size)\n",
    "val_ds = binary_label_df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = binary_label_df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a208edf3-43b2-421d-909b-add075802245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9958d24-4d8f-4527-bc19-c657e1d9b22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "encoded_features = []\n",
    "\n",
    "numInputFeatToInitNodeUnit=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a69b6-d9c1-49d9-bc81-ed421d8700cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric features.\n",
    "for header in numbericCols:\n",
    "  stat_data=train[header].describe()\n",
    "  print(f\"header: Mean={stat_data['mean']} and Std={stat_data['std']}\") \n",
    "  numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "  normalization_layer = get_normalization_layer(header, train_ds)\n",
    "  encoded_numeric_col = normalization_layer(numeric_col)\n",
    "  all_inputs.append(numeric_col)\n",
    "  encoded_features.append(encoded_numeric_col)\n",
    "\n",
    "numInputFeatToInitNodeUnit=numInputFeatToInitNodeUnit+len(numbericCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b59cb7-0c30-42f7-b87f-f408c72dcc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical features encoded as string.\n",
    "categorical_cols = cateCols\n",
    "for header in categorical_cols:\n",
    "  listCateItem=train[header].unique()\n",
    "  noCateItem=len(listCateItem)\n",
    "  numInputFeatToInitNodeUnit=numInputFeatToInitNodeUnit+noCateItem +1  # last 1 is unknow\n",
    "    \n",
    "  print(f\"{header} = {noCateItem} : {listCateItem}\")\n",
    "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "  encoding_layer = get_category_encoding_layer(header, train_ds, dtype='string')\n",
    "                                        \n",
    "  encoded_categorical_col = encoding_layer(categorical_col)\n",
    "  all_inputs.append(categorical_col)\n",
    "  encoded_features.append(encoded_categorical_col)\n",
    "\n",
    "\n",
    "print(f\"Total column to initialize first node input: {numInputFeatToInitNodeUnit}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89ba051-c41f-432a-b306-7ab9dad5ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "\n",
    "x = tf.keras.layers.Dense(numInputFeatToInitNodeUnit, activation=\"relu\")(all_features)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "# x = tf.keras.layers.Dense(32, activation=\"relu\")(x)\n",
    "# x = tf.keras.layers.Dropout(0.1)(x)\n",
    "\n",
    "output = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "model = tf.keras.Model(all_inputs, output)\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "#model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25688a22-05f5-4cac-b662-d261b5dcfb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "history =model.fit(train_ds, validation_data=val_ds,epochs=EPOCHS,batch_size=BATCH_SIZE,callbacks = [early_stopping])\n",
    "# history =model.fit(train_ds,verbose=1,validation_data=val_ds,epochs=EPOCHS,batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e3efc-10f6-498b-97f6-5ed44b9534d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(val_ds)\n",
    "print(\"Average Accuracy on Eveluation\", accuracy)\n",
    "loss, accuracy = model.evaluate(test_ds)\n",
    "print(\"Average Accuracy  on Test\", accuracy)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f86605-c8aa-47d9-97da-9771844c236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f677c-dfa2-48b5-b980-6cc10f2e0033",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db2353b-b178-4e97-9ca8-ca5550569793",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.keras.models.load_model(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd916787-bacc-4537-b2b6-1c6d62932de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_binary_severity\n",
    "sample={\"sla\":\"24x7 4Hrs Response Time\",\n",
    "        \"product_type\":\"Server\",\n",
    "        \"brand\":\"VMWare\",                \n",
    "        \"service_type\":\"Incident\",\n",
    "        \"incident_type\":\"General Incident\",\n",
    "        \"open_to_close_hour\":8.30,\n",
    "        \"response_to_resolved_hour\":6.000000 \\\n",
    "       }\n",
    "\n",
    "print(sample)\n",
    "              \n",
    "print(\"===============================================================================================================\")    \n",
    "print(\"input t0 predict\")    \n",
    "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
    "print(input_dict)\n",
    "\n",
    "predictionList = reloaded_model.predict(input_dict)\n",
    "print(predictionList)\n",
    "prob = tf.nn.sigmoid(predictionList[0])\n",
    "print(f\"{(100 * prob)} %  as Critical/Major\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739ebc03-3f34-4f1a-92c4-b152887a538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "def plot_metrics(history):\n",
    "  metrics =  ['loss', 'accuracy']\n",
    "  for n, metric in enumerate(metrics):\n",
    "    name = metric.replace(\"_\",\" \").capitalize()\n",
    "    plt.subplot(2,2,n+1)\n",
    "    plt.plot(history.epoch,  history.history[metric], color=colors[0], label='Train')\n",
    "    plt.plot(history.epoch, history.history['val_'+metric],\n",
    "             color=colors[0], linestyle=\"--\", label='Val')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel(name)\n",
    "    if metric == 'loss':\n",
    "      plt.ylim([0, plt.ylim()[1]])\n",
    "    elif metric == 'auc':\n",
    "      plt.ylim([0.8,1])\n",
    "    else:\n",
    "      plt.ylim([0,1])\n",
    "\n",
    "    plt.legend()\n",
    "\n",
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eca661-46ba-4492-9b48-41ef85487ee3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8161192-5269-40e8-935e-82d828a7ebb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
