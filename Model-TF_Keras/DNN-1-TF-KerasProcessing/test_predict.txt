import tensorflow as tf
import json
import numpy as np
import pandas as pd

from google.cloud import bigquery
from google.cloud import storage

--------------
unUsedColtoPredict=['severity','id','severity_id','severity_name','imported_at']
isLocalModel=False

localFile=True



--------------
if  localFile:
    newData_path='data/New_Incident.csv'
    dfNewData=pd.read_csv(newData_path)

dfNewData=dfNewData.tail(3)

# Download means and std
def download_map_severity_class_as_dict():
    mapping_file="incident_sevirity_to_class.json"
    storage_client = storage.Client()
    bucket = storage_client.bucket("smart-ml-pongthorn")
    blob = bucket.blob(mapping_file)
    blob.download_to_filename(mapping_file)
    with open(mapping_file, 'r') as file:
            return json.loads(file.read())

    
map_sevirity_to_class= download_map_severity_class_as_dict()
print(map_sevirity_to_class)

dfNewData.insert(2, 'severity', dfNewData['severity_name'].map(map_sevirity_to_class),True)
print(dfNewData.info())
dfNewData


----------------------

if  isLocalModel: # Window Enviroment
    PATH_FOLDER_ARTIFACTS="model"    
else: # For running on Vertex AI Notebook t
    PATH_FOLDER_ARTIFACTS="gs://tf1-incident-pongthorn/model"
    
model = tf.keras.models.load_model(PATH_FOLDER_ARTIFACTS)    
print(f"Load from {PATH_FOLDER_ARTIFACTS}")
# model.summary()
===========================

pdPrediction=pd.DataFrame(columns=['_id','_predict_severity'])
for  row_dict in dfNewData.to_dict(orient="records"):
      incident_id=row_dict['id']
      print(f"{incident_id} - {row_dict['severity']}({row_dict['severity_name']})") 
      for key_removed in unUsedColtoPredict:
       row_dict.pop(key_removed)
      # print(row_dict)  

      input_dict = {name: tf.convert_to_tensor([value]) for name, value in row_dict.items()}
      # print(input_dict)
    
      predictionResult = model.predict(input_dict)
      print(f"{predictionResult}")   
      # maxResult=np.max(arryResult)
      # maxResult=round( maxResult,4)
      # print(f"{maxResult} of {arryResult}")
        
      prob = tf.nn.softmax(predictionResult)
      prob_pct=(100 * prob)  
      _class = tf.argmax(predictionResult,-1).numpy()[0]
      print(f"{prob_pct} %   as {_class}")   
        
      pdPrediction = pdPrediction.append({'_id':incident_id, '_predict_severity':_class}, ignore_index=True)
      print("======================================================================================")

dfNewData=pd.merge(dfNewData,pdPrediction,how='inner',left_on='id',right_on='_id')
dfNewData=dfNewData.drop(columns=['_id'])
dfNewData=dfNewData[['id','_predict_severity','severity','severity_name']]
dfNewData

from google.cloud import aiplatform
PROJECT_NUMBER="pongthorn"
ENDPOINT_ID="7022571970512289792"
REGION="asia-southeast1"
endpoint = aiplatform.Endpoint(
    endpoint_name=f"projects/{PROJECT_NUMBER}/locations/{REGION}/endpoints/{ENDPOINT_ID}")
print(endpoint)


df2=dfNewData.copy().drop(columns=unUsedColtoPredict)
for row in df2.to_dict(orient="records"):
    for key,value in row.items():
        row[key]=[value]
    print(row)
    response = endpoint.predict([row])
    print("=================================================================")
    print('API response: ')
    print(response)
    # print('Predicted Sevirity: ', response.predictions[0][0])




