{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8a4684-bb85-4e37-bcd9-26177ef1eaf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# from google.cloud import aiplatform\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaa70dc-6acc-4119-b8a6-4a37c4ef3458",
   "metadata": {},
   "source": [
    "# Reference\n",
    "https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/custom/custom-tabular-bq-managed-dataset.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3015b10-ce8e-4004-afd2-e5c60d3b5c08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a91ce-d7ff-423a-9aee-4be795283c77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#table_id='test_incident'\n",
    "#table_id='validation_incident'\n",
    "table_id='new_incident'\n",
    "#table_id='train_incident'\n",
    "\n",
    "ENDPOINT_ID=\"2469916532356939776\"\n",
    "MODEL_ID=\"2354195132556771328\"\n",
    "\n",
    "isLocalModel=True\n",
    "isBQToCSV=True\n",
    "localMapFile=True\n",
    "gs_store_class=\"demo2-tf-incident-pongthorn\"\n",
    "gs_model_path=\"gs://demo2-tf-incident-pongthorn/demo_model_tf\"\n",
    "\n",
    "local_model_path='demo_model_tf'\n",
    "# ! gsutil -m cp -r \"gs://demo2-tf-incident-pongthorn/demo_model_tf\" .\n",
    "\n",
    "\n",
    "modelFile_endPoint=2\n",
    "#1=local(dev)/gcs(clound function) and 2=endpoint on vertext\n",
    "\n",
    "# projectId='smart-data-ml'\n",
    "projectId='pongthorn'\n",
    "dataset_id='SMartML'\n",
    "REGION=\"asia-southeast1\"\n",
    "\n",
    "n_sample_run=0\n",
    "\n",
    "# mycredentials = service_account.Credentials.from_service_account_file(r'C:\\Windows\\smart-data-ml-91b6f6204773.json')\n",
    "# bqclient= bigquery.Client(project=projectId,credentials=mycredentials)\n",
    "bqclient= bigquery.Client(project=projectId)\n",
    "\n",
    "if  table_id=='new_incident':\n",
    " unUsedColtoPredict=['severity','id','severity_id','severity_name','imported_at']\n",
    " filePath='csv_dataset/Prediction_New_Incident.csv'\n",
    "elif table_id=='test_incident' or table_id=='validation_incident':\n",
    " unUsedColtoPredict=['severity','id','severity_id','severity_name']  \n",
    " filePath=f'csv_dataset/Prediction_{table_id}.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d52b41-422e-42f8-8cae-79ad17f1e475",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if isBQToCSV:\n",
    "    \n",
    "    def download_table(bq_table_uri: str):\n",
    "\n",
    "        prefix = \"bq://\"\n",
    "        if bq_table_uri.startswith(prefix):\n",
    "            bq_table_uri = bq_table_uri[len(prefix) :]\n",
    "\n",
    "        table = bigquery.TableReference.from_string(bq_table_uri)\n",
    "        rows = bqclient.list_rows(table)\n",
    "        return rows.to_dataframe()\n",
    "\n",
    "    dfNewData=download_table(f\"{projectId}.{dataset_id}.{table_id}\")\n",
    "    dfNewData.to_csv(filePath,index=False)    \n",
    "    \n",
    "  \n",
    "if n_sample_run>0:\n",
    " dfNewData=dfNewData.tail(n_sample_run)\n",
    "print(dfNewData.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c26dc93-8386-4960-bf33-c53cbe9618c4",
   "metadata": {},
   "source": [
    "\n",
    "# Load and Map Data Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cad9950-cb7a-4421-8e4d-89d1d0367f43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "mapping_file=\"incident_severity_to_class.json\"\n",
    "\n",
    "def download_map_severity_class_as_dict():\n",
    " \n",
    "    storage_client = storage.Client()\n",
    "    buckdfNewData = storage_client.bucket(gs_store_class)\n",
    "    blob = bucket.blob(mapping_file)\n",
    "    blob.download_to_filename(mapping_file)\n",
    "    with open(mapping_file, 'r') as file:\n",
    "            return json.loads(file.read())\n",
    "\n",
    "if  localMapFile==False:    \n",
    "    map_sevirity_to_class= download_map_severity_class_as_dict()\n",
    "else:\n",
    "   with open(mapping_file, 'r') as json_file:\n",
    "     map_sevirity_to_class= json.load(json_file)\n",
    "                                  \n",
    "                                  \n",
    "print(map_sevirity_to_class)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8a438e-a9cb-40cf-81ea-01f326e35c6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if  table_id=='new_incident':\n",
    "   dfNewData['severity']= dfNewData['severity_name'].map(map_sevirity_to_class)\n",
    "else:\n",
    "    dfNewData=dfNewData.rename(columns={'label_multi_severity':'severity'})\n",
    "    dfNewData=dfNewData.drop(columns=['label_binary_severity'])\n",
    "\n",
    "if 'id' not in dfNewData.columns.to_list():\n",
    "    dfNewData=dfNewData.reset_index(drop=True)\n",
    "    dfNewData = dfNewData.reset_index(level=0)\n",
    "    dfNewData.rename(columns={\"index\": \"id\"},inplace=True)\n",
    "    dfNewData['id']=dfNewData['id']+1\n",
    "print(f\"Table: {table_id}\")\n",
    "print(filePath) \n",
    "print(dfNewData.info())\n",
    "\n",
    "dfNewData.to_csv(f\"{table_id}.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d8d093-e85d-4b90-9533-90292b721769",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_class(df):\n",
    "    fig , ax = plt.subplots(figsize=(8,5))\n",
    "    ax =sns.countplot(x='severity_name', data=df,)\n",
    "    for p in ax.patches:\n",
    "       ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n",
    "    plt.title('Severity')\n",
    "    plt.show()\n",
    "    \n",
    "plot_class(dfNewData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7742d2bd-7e76-440b-bbe8-5f27211e6f9b",
   "metadata": {},
   "source": [
    "# Classification Rerport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4e6110-be32-4f05-9c4f-192268098b39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_classification_report(dfResult):\n",
    "    print(dfResult.tail())\n",
    "    className=list(set().union(list(dfResult['severity'].unique()),list(dfResult['_predict_severity'].unique())))\n",
    "    actualClass=[  f'actual-{x}' for x in  className]\n",
    "    predictedlClass=[  f'predcited-{x}' for x in className]\n",
    "\n",
    "    y_true=list(dfResult['severity'])\n",
    "    y_pred=list(dfResult['_predict_severity'])\n",
    "    cnf_matrix = confusion_matrix(y_true,y_pred)\n",
    "\n",
    "    # #index=actual , column=prediction\n",
    "    cm_df = pd.DataFrame(cnf_matrix,\n",
    "                         index = actualClass, \n",
    "                         columns = predictedlClass)\n",
    "    print(cm_df)\n",
    "\n",
    "    print(classification_report(y_true, y_pred, labels=className))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93793bce-3345-490d-b051-596cd6577210",
   "metadata": {},
   "source": [
    "# Load Model from Directory to Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d804ccc7-44b6-4471-8a45-c785fda30bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if  isLocalModel: # Window Enviroment\n",
    "    PATH_FOLDER_ARTIFACTS=local_model_path   \n",
    "else: # For running on Vertex AI Notebook t\n",
    "    PATH_FOLDER_ARTIFACTS=gs_model_path\n",
    "print(f\"Load from {PATH_FOLDER_ARTIFACTS}\")\n",
    "model = tf.keras.models.load_model(PATH_FOLDER_ARTIFACTS)   \n",
    "print(model.tensorflow_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae97ee2-5930-4511-bc31-5d6108f348c4",
   "metadata": {},
   "source": [
    "# Prediction by model on entire rows at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9336b4-1e34-4e2d-91b5-50dc9b0567cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if modelFile_endPoint==1 :\n",
    "    pdPrediction=pd.DataFrame(columns=['_id','_predict_severity'])\n",
    "    pdPrediction\n",
    "\n",
    "    # print(model.summary())\n",
    "\n",
    "    for  row_dict in dfNewData.to_dict(orient=\"records\"):\n",
    "          incident_id=row_dict['id']\n",
    "          # print(f\"{incident_id} - {row_dict['severity']}({row_dict['severity_name']})\") \n",
    "        \n",
    "          for key_removed in unUsedColtoPredict:\n",
    "           row_dict.pop(key_removed)\n",
    "          print(row_dict)  \n",
    "\n",
    "          input_tensor = {name: tf.convert_to_tensor([value]) for name, value in row_dict.items()}\n",
    "          predictionResult = model.predict(input_tensor)\n",
    "\n",
    "          prob = tf.nn.softmax(predictionResult)\n",
    "          prob_pct=(100 * prob)  \n",
    "          _class = tf.argmax(predictionResult,-1).numpy()[0]\n",
    "          \n",
    "          pdPrediction =pd.concat([pdPrediction,pd.DataFrame.from_dict([{'_id':incident_id, '_predict_severity':_class}])] )\n",
    "          \n",
    "          # print(input_tensor)    \n",
    "          print(f\"{prob_pct} %   as {_class}\")     \n",
    "          print(\"======================================================================================\")\n",
    "\n",
    "    dfPredictData=pd.merge(dfNewData,pdPrediction,how='inner',left_on='id',right_on='_id')\n",
    "    dfPredictData=dfPredictData.drop(columns=['_id'])\n",
    "    dfPredictData=dfPredictData[['id','_predict_severity','severity','severity_name']]\n",
    "    print(\"Predicted all data completely\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81ffff1-c715-4b97-aa85-46a6bbca6514",
   "metadata": {},
   "source": [
    "# Registry and Deploy Model to VertextAI\n",
    "- https://cloud.google.com/vertex-ai/docs/predictions/get-predictions#deploy_a_model_to_an_endpoint\n",
    "\n",
    "\n",
    "## Load Model from Online EndPpint to Predict\n",
    "- https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-online-predictions\n",
    "- https://cloud.google.com/vertex-ai/docs/tutorials/tabular-bq-prediction/train-and-deploy-model\n",
    "- https://cloud.google.com/vertex-ai/docs/tutorials/tabular-bq-prediction/make-prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd4f304-0054-4464-af76-7948133d8557",
   "metadata": {},
   "source": [
    "# Load model from Vertext-Ai Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f940b5-2b27-4c12-b7d1-667c0b59ec02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://console.cloud.google.com/vertex-ai/endpoints?project=pongthorn\n",
    "#https://console.cloud.google.com/vertex-ai/batch-predictions?project=pongthorn\n",
    "\n",
    "MODEL_URI =  f'projects/{projectId}/locations/{REGION}/models/{MODEL_ID}'\n",
    "print(MODEL_URI)\n",
    "\n",
    "model_reg = aiplatform.Model(MODEL_URI)\n",
    "\n",
    "model_name=model_reg.display_name\n",
    "print(model_reg.uri,\" - \",model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968155c4-f3b4-4adb-b599-9caffe6bb70c",
   "metadata": {},
   "source": [
    "# Make a call to Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f410f4f-fd78-4bb1-bf3f-9fb2ee398b46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "isEndpointAvaiable=False\n",
    "\n",
    "listRequestData=[]\n",
    "try:\n",
    "    \n",
    "    endpoint = aiplatform.Endpoint(endpoint_name=f\"projects/{projectId}/locations/{REGION}/endpoints/{ENDPOINT_ID}\")\n",
    "                                   \n",
    "    # endpoint = aiplatform.Endpoint(endpoint_name=f\"projects/{projectId}/locations/{REGION}/endpoints/{ENDPOINT_ID}\"\n",
    "    #                                ,credentials=mycredentials)\n",
    "    print(endpoint)\n",
    "    isEndpointAvaiable=True\n",
    "except Exception as error:\n",
    "    print(str(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d7d2f-2f68-47d1-9f63-56ca5f2deba2",
   "metadata": {},
   "source": [
    "# Create Instances and feed it to endpoint to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044d7e96-0526-4a33-aa98-79fc1b4126a8",
   "metadata": {},
   "source": [
    "# Convert to tenfor format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5983af29-cf9f-46a6-8967-41937d9fa45a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfID=dfNewData[['id']]\n",
    "dfID.reset_index(inplace=True,drop=True)\n",
    "dfY=dfNewData[['severity','severity_name']]\n",
    "dfY.reset_index(inplace=True,drop=True)\n",
    "dfX=dfNewData.drop(columns=unUsedColtoPredict)\n",
    "dfX.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f9f0a6-7c88-4c61-9c71-2fdf879102b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "instances = []\n",
    "for index,row in dfX.iterrows():\n",
    "    dict_item=row.to_dict()\n",
    "    instance = {}\n",
    "    for key, value in dict_item.items():\n",
    "        if key in unUsedColtoPredict:\n",
    "            continue\n",
    "        if value is None:\n",
    "            value = \"\"\n",
    "        instance[key] = [value]\n",
    "        # instance[key] = value\n",
    "    instances.append(instance)\n",
    "\n",
    "print(len(instances))\n",
    "print(instances[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d6c2e2-83d3-455a-821e-485da6a018a8",
   "metadata": {},
   "source": [
    "# Prediction by endpoint on entire rows at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70641190-a2f0-4fd5-9cf8-09df7f4c7bdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = endpoint.predict(instances=instances)\n",
    "predictedY = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "dfPredictedY=pd.DataFrame(data={'_predict_severity':predictedY})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4513b75c-77fa-472c-ba60-03058374c045",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfOnlinePred=pd.concat([dfID,dfPredictedY,dfY],axis=1)\n",
    "dfOnlinePred.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da1689c-acdb-49d7-8ed7-d6e1198af166",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_classification_report(dfOnlinePred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d3a417-e235-4ede-a784-53c878d1c820",
   "metadata": {},
   "source": [
    "# Get explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dceeb6d1-0924-4ec3-b7c0-71a0ac2d2f11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_name = model.input_names\n",
    "print(\"Model input name:\", input_name)\n",
    "output_name = model.output_names\n",
    "print(\"Model output name:\", output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76692cb4-64f4-497f-bd60-47568936d711",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-online-predictions#interpret_explanation_results\n",
    "# https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-online-predictions#example_output_for_predictions_and_explanations\n",
    "# https://cloud.google.com/vertex-ai/docs/tabular-data/classification-explanations\n",
    "\n",
    "\n",
    "listExplainModel=[]\n",
    "def explain_model( instanceList):\n",
    "\n",
    "    response = endpoint.explain(instances=instanceList, parameters={})\n",
    "\n",
    "    for explanation in response.explanations:\n",
    "        # print(\"Explanation\")\n",
    "        # Feature attributions.\n",
    "        attributions = explanation.attributions\n",
    "        item={}\n",
    "        for attribution in attributions:\n",
    "            item[\"baseline_output\"]=attribution.baseline_output_value\n",
    "            item[\"instance_output\"]=attribution.instance_output_value\n",
    "            item[\"approx_error\"]=attribution.approximation_error\n",
    "            item[\"baseline_output\"]=attribution.baseline_output_value\n",
    "            sum_feat=0;\n",
    "            for name in input_name:\n",
    "              feat_value= attribution.feature_attributions[name][0]\n",
    "              sum_feat=sum_feat+feat_value\n",
    "              # print(f\"  {name} :{feat_value}\")\n",
    "              item[name]=feat_value\n",
    "            # print(f\"  The sum of all of the feature importance values(instance-baseline) = {sum_feat}\")\n",
    "            item[\"sum_feat_imp\"]=sum_feat\n",
    "        listExplainModel.append(item)   \n",
    "        \n",
    "\n",
    "explain_model(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f4c51b-58be-4f69-bcca-7f2fc1422ab1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfExplainModelResult=pd.DataFrame(data=listExplainModel)\n",
    "dfExplainModelResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e204f-ec1b-4348-b9bb-fac8b4efcd57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfABC=dfExplainModelResult[input_name].sum().to_frame().T\n",
    "dfABC['dataset']=table_id\n",
    "dfABC=dfABC.set_index('dataset')\n",
    "dfABC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3658da-04de-469d-b869-d5bb66e1b601",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# writer = pd.ExcelWriter(f'{table_id}_{model_name}_exp.xlsx')\n",
    "# for i, dataframe in enumerate([dfExplainModelResult,dfABC]):\n",
    "#     dataframe.to_excel(writer, sheet_name=f'Sheet{i + 1}')\n",
    "# writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0481fe7-a0fc-4555-b504-68de88815fd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99e4fec5-4263-4e08-9c69-0bad47aaf932",
   "metadata": {},
   "source": [
    "# Prediction individual row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f314c67-2d1c-46a9-aee8-50ba071a9ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if modelFile_endPoint==2 and isEndpointAvaiable==True :\n",
    "\n",
    "#     for data in dfNewData.to_dict(orient=\"records\"):\n",
    "#         incident_id=data['id']\n",
    "#         severity=data['severity']\n",
    "#         severity_name=data['severity_name']\n",
    "#         # convert dict to tf format\n",
    "#         request_data = {key : [value] for key,value in data.items() }\n",
    "#         for key_removed in unUsedColtoPredict:\n",
    "#            request_data.pop(key_removed)\n",
    "#         listRequestData.append(request_data)\n",
    "#         print(request_data)\n",
    "\n",
    "#         response = endpoint.predict([request_data])\n",
    "#         y_predicted = np.argmax(response.predictions, axis=1)\n",
    "#         predictionResult=response[0][0]\n",
    "#         print(predictionResult)\n",
    "#         print(y_predicted)\n",
    "\n",
    "#         pdPrediction =pd.concat([pdPrediction,pd.DataFrame.from_dict([{'_id':incident_id, '_predict_severity':y_predicted[0]}])] )\n",
    "#         print(\"======================================================================================\")\n",
    "\n",
    "#     dfPredictData=pd.merge(dfNewData,pdPrediction,how='inner',left_on='id',right_on='_id')\n",
    "#     dfPredictData=dfPredictData.drop(columns=['_id'])\n",
    "#     dfPredictData=dfPredictData[['id','_predict_severity','severity','severity_name']]       \n",
    "# else:\n",
    "#      print(\"No endpoint to predict  as online predction\")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da4a4a-32ae-4d62-ad8b-cf686e01f8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84e61c1-9d9a-4a41-862c-e3c454eff9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-12.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-12:m109"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
