{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd54814-e227-4302-9a81-3d6ddcf0e9a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "#https://codelabs.developers.google.com/codelabs/fraud-detection-ai-explanations?hl=en#0\n",
    "#he Explainable AI SDK and Copy Model to Deploy\n",
    "\n",
    "#https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive2/introduction_to_tensorflow/solutions/preprocessing_layers.ipynb\n",
    "#https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/custom/custom-tabular-bq-managed-dataset.ipynb\n",
    "#https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/experiments/comparing_local_trained_models.ipynb\n",
    "\n",
    "#https://pair-code.github.io/lit/\n",
    "## https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "#https://www.tensorflow.org/tensorboard/get_started\n",
    "# https://www.tensorflow.org/tensorboard/dataframe_api\n",
    "# https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks\n",
    "#https://keras.io/guides/keras_tuner/visualize_tuning/\n",
    "\n",
    "#https://github.com/technqvi/TimeSeriesML-FinMarket/blob/main/MultiVarToManyOutputLSTM.ipynb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,DenseFeatures\n",
    "\n",
    "from datetime import date, timedelta, datetime # Date Functions\n",
    "import time\n",
    "\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "# from google.cloud import aiplatform as vertex_ai\n",
    "from tensorflow.python.keras.utils import data_utils\n",
    "\n",
    "import tensorboard as tb\n",
    "\n",
    "# from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf74622-6c60-4bda-9b9b-9a799bfaed45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# major_ver, minor_ver, _ = version.parse(tb.__version__).release\n",
    "# assert major_ver >= 2 and minor_ver >= 3, \\\n",
    "#     \"This notebook requires TensorBoard 2.3 or later.\"\n",
    "# print(\"TensorBoard version: \", tb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab04a472-7abe-4315-b555-0583fdb2a5d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "# !rm -rf ./logs/ \n",
    "\n",
    "%load_ext tensorboard\n",
    "#%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28a9538-ca23-45f0-817b-e4b067cf9ab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cateCols=['sla','product_type','brand','service_type','incident_type']\n",
    "numbericCols=['open_to_close_hour','response_to_resolved_hour']\n",
    "unusedCols=['severity_id','severity_name','label_binary_severity']\n",
    "\n",
    "# cateCols=['service_type','product_type','incident_type','sla']\n",
    "# numbericCols=[]\n",
    "# unusedCols=['severity_id','severity_name','label_binary_severity', 'open_to_close_hour','response_to_resolved_hour' ]\n",
    "\n",
    "# cateCols=['sla','product_type','service_type','incident_type']\n",
    "# numbericCols=['open_to_close_hour']\n",
    "# unusedCols=['severity_id','severity_name','label_binary_severity','brand','response_to_resolved_hour']\n",
    "\n",
    "labelCol='label_multi_severity'\n",
    "\n",
    "main_metric='accuracy'\n",
    "main_objective=f'val_{main_metric}'\n",
    "\n",
    "\n",
    "objective_to_tued_and_monitor='val_loss' \n",
    "# objective_to_tued_and_monitor=main_objective\n",
    "\n",
    "seed=1932\n",
    "\n",
    "# EPOCHS =100\n",
    "# BATCH_SIZE = 32\n",
    "# unitList=[32,64,128]\n",
    "# dropOutList= [0.1, 0.2]\n",
    "# lrList=[0.01,0.001,0.0001]\n",
    "# nAtleastMaxTrials=10\n",
    "# nExecutions_per_trial=3  # 3,5\n",
    "# nEarlyPatience=10\n",
    "\n",
    "\n",
    "EPOCHS =25\n",
    "BATCH_SIZE = 32\n",
    "unitList=[64]\n",
    "dropOutList= [0.1]\n",
    "lrList=[0.01,0.001]\n",
    "nAtleastMaxTrials=5\n",
    "nExecutions_per_trial=1  # 3,5\n",
    "nEarlyPatience=5\n",
    "\n",
    "model_tuned_dir='tuned_model_V2'\n",
    "\n",
    "tsb_path=\"tsb_logs/tune/\"\n",
    "\n",
    "# df['label_multi_severity'] =df['severity_name'].map({'Cosmatic':0,'Minor': 1, \"Major\": 2, \"Critical\": 3}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0aa901-563f-4d2c-9203-67175431c32f",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1db6b46-bd0e-4053-8b4e-42e43f822da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "projectId='pongthorn'\n",
    "client = bigquery.Client(project=projectId)\n",
    "dataset_id='SMartML'\n",
    "\n",
    "train_name='train_incident'\n",
    "validation_name='validation_incident'\n",
    "test_name='test_incident'\n",
    "\n",
    "train_table_id=f\"{projectId}.{dataset_id}.{train_name}\"\n",
    "val_tabel_id=f\"{projectId}.{dataset_id}.{validation_name}\"\n",
    "test_tabel_id=f\"{projectId}.{dataset_id}.{test_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddea3e1-b732-48c4-95fc-637108802a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def load_ml_data(data_path):\n",
    "#  df=pd.read_csv(data_path)\n",
    "#  df =df.drop(columns=unusedCols)\n",
    "#  return df\n",
    "\n",
    "# root_path='../../data'    \n",
    "# train = load_ml_data(f\"{root_path}/train_incident.csv\")\n",
    "# # val=train.copy()\n",
    "# val=load_ml_data(f\"{root_path}/validation_incident.csv\")\n",
    "# # test =val.copy()\n",
    "# test =load_ml_data(f\"{root_path}/test_incident.csv\")\n",
    "\n",
    "def load_data_bq(sql:str):\n",
    " \n",
    " query_result=client.query(sql)\n",
    " df=query_result.to_dataframe()\n",
    " df =df.drop(columns=unusedCols)\n",
    "  \n",
    " return df\n",
    "    \n",
    "\n",
    "train=load_data_bq(f\"SELECT * FROM {train_table_id}\")\n",
    "val=load_data_bq(f\"SELECT * FROM {val_tabel_id}\")\n",
    "test=load_data_bq(f\"SELECT * FROM {test_tabel_id}\")\n",
    "\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)\n",
    "\n",
    "\n",
    "labelList=list(train[labelCol].unique())\n",
    "print(labelList)\n",
    "nLabel=len(labelList)\n",
    "print(f\"No target label : {nLabel}\")\n",
    "\n",
    "# sr_predict=df.iloc[-1,:]\n",
    "# df=df.iloc[0:len(df)-1,:]\n",
    "                 \n",
    "print(train.info())\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64adbe5e-d737-45a4-a04e-340a2926ad2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CalPctEachTargetClass(dfx,colSev,colPctSev):\n",
    "    dfClassSummary=dfx.groupby([labelCol]).size().to_frame(colSev)\n",
    "    dfClassSummary[colPctSev]= dfClassSummary[colSev]/dfClassSummary[colSev].sum() *100\n",
    "    dfClassSummary=dfClassSummary.round(0)\n",
    "    return dfClassSummary\n",
    "\n",
    "pctDF1=CalPctEachTargetClass(train,'Train-No-Severity','Train-%-Severity')\n",
    "pctDF2=CalPctEachTargetClass(val,'Val-No-Severity','Val-%-Severity')\n",
    "pdcDF3=CalPctEachTargetClass(test,'Test-No-Severity','Test-%-Severity')\n",
    "pctDF=pd.concat([pctDF1,pctDF2,pdcDF3],axis=1)\n",
    "\n",
    "pctDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa30d3fd-14e9-4c3d-a6aa-45d8114f5362",
   "metadata": {},
   "source": [
    "# Process Data  Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d3a943-09d5-4a43-b8d8-633bc4a90353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multiple_label_df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  features = dataframe.copy()\n",
    "  labels = features.pop(labelCol)\n",
    "  labels  = tf.keras.utils.to_categorical(labels, num_classes=nLabel)\n",
    "    \n",
    "  ds = tf.data.Dataset.from_tensor_slices(( dict(features), labels ))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(features))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ad8837-968e-4d68-bdb6-1c7eba97e69c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_normalization_layer(name, dataset):\n",
    "  # Create a Normalization layer for our feature.\n",
    "  normalizer = preprocessing.Normalization(axis=None)\n",
    "\n",
    "  # Prepare a Dataset that only yields our feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the statistics of the data.\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237bce3c-9c77-4318-8682-fc36af0a6804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "  # Create a StringLookup layer which will turn strings into integer indices\n",
    "  if dtype == 'string':\n",
    "    index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
    "  else:\n",
    "    index = preprocessing.IntegerLookup(max_tokens=max_tokens)\n",
    "\n",
    "  # Prepare a Dataset that only yields our feature\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the set of possible values and assign them a fixed integer index.\n",
    "  index.adapt(feature_ds)\n",
    "\n",
    "  # Create a Discretization for our integer indices.\n",
    "  encoder = preprocessing.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "\n",
    "  # Apply one-hot encoding to our indices. The lambda function captures the\n",
    "  # layer so we can use them, or include them in the functional model later.\n",
    "  return lambda feature: encoder(index(feature))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c78f01-000a-4d3e-9fbf-6f4660cfb75b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size =32\n",
    "train_ds = multiple_label_df_to_dataset (train, batch_size=batch_size)\n",
    "val_ds = multiple_label_df_to_dataset(val, batch_size=batch_size)\n",
    "test_ds = multiple_label_df_to_dataset(test, batch_size=batch_size)\n",
    "# for element in train_ds.as_numpy_iterator():\n",
    "#     print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a69b6-d9c1-49d9-bc81-ed421d8700cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "encoded_features = []\n",
    "numInputFeatToInitNodeUnit=0\n",
    "# Numeric features.\n",
    "for header in numbericCols:\n",
    "  print(header)  \n",
    "  stat_data=train[header].describe()\n",
    "  print(f\"header: Mean={stat_data['mean']} and Std={stat_data['std']}\") \n",
    "  numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "  normalization_layer = get_normalization_layer(header, train_ds)\n",
    "  encoded_numeric_col = normalization_layer(numeric_col)\n",
    "  all_inputs.append(numeric_col)\n",
    "  encoded_features.append(encoded_numeric_col)\n",
    "  print(\"========================================================================\")\n",
    "    \n",
    "numInputFeatToInitNodeUnit=numInputFeatToInitNodeUnit+len(numbericCols)\n",
    " \n",
    "# Categorical features encoded as string.\n",
    "categorical_cols = cateCols\n",
    "for header in categorical_cols:\n",
    "    \n",
    "  listCateItem=train[header].unique()\n",
    "  noCateItem=len(listCateItem)\n",
    "  numInputFeatToInitNodeUnit=numInputFeatToInitNodeUnit+noCateItem +1  # last 1 is unknow  \n",
    "  print(f\"{header} = {noCateItem} : {listCateItem}\")    \n",
    "    \n",
    "  print(header)  \n",
    "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "  encoding_layer = get_category_encoding_layer(header, train_ds, dtype='string')\n",
    "                                        \n",
    "  encoded_categorical_col = encoding_layer(categorical_col)\n",
    "  all_inputs.append(categorical_col)\n",
    "  encoded_features.append(encoded_categorical_col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f85f8a-4471-44e1-a389-102232c296e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf75dae1-683f-403d-9f2d-aca95c970a21",
   "metadata": {},
   "source": [
    "# Tune HyperParameter By Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dacd5c-0af5-4415-a20e-6e808850de18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unitList.append(numInputFeatToInitNodeUnit)\n",
    "# unitList.append(numInputFeatToInitNodeUnit*2)\n",
    "print(unitList,dropOutList,lrList)\n",
    "nMax_trials=len(unitList)*len(dropOutList)*len(lrList)\n",
    "if nMax_trials<nAtleastMaxTrials:\n",
    " nMax_trials=nAtleastMaxTrials \n",
    "print(nMax_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4267752a-455a-4fde-a7b9-518229a81a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://github.com/technqvi/TimeSeriesML-FinMarket/blob/main/lstm-tune-dev/Tuned-MultiVarToManyOutputLSTM.ipynb\n",
    "#https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "#https://keras.io/guides/keras_tuner/getting_started/\n",
    "import keras_tuner as kt\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "tune_folder=\"IncidentMLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c97a4d39-0145-49cd-9abf-277e76bd03af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model_for_tuning(hp):\n",
    "    \n",
    "    all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(hp.Choice('units', unitList), activation=\"relu\")(all_features)\n",
    "    \n",
    "    x = tf.keras.layers.Dropout(hp.Choice('Dropout_rate',dropOutList))(x)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(nLabel,activation=tf.nn.softmax)(x)\n",
    "    \n",
    "    model = tf.keras.Model(all_inputs, output)\n",
    "    # model.compile(optimizer='adam',loss=tf.keras.losses.CategoricalCrossentropy(),metrics=[main_metric])\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=lrList) \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(),metrics=[main_metric])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b82d3-2cf4-4e1d-8dd3-127bc796ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_Start=time.time()\n",
    "\n",
    "print(f\"Start tund at {datetime.now()}\")\n",
    "print(\"=============================================================\")\n",
    "buildtime = datetime.now().strftime('%d%m%y_%H%M')\n",
    "project_model=f\"{tune_folder}_{buildtime}\"\n",
    "print(project_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a501b2-88c6-4b15-9724-2b05849fde59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://keras.io/api/keras_tuner/tuners/bayesian/\n",
    "# tuner = kt.BayesianOptimization(\n",
    "#     build_model_for_tuning, objective=objective_to_tued_and_monitor,seed=seed,\n",
    "#     max_trials=nMax_trials,executions_per_trial=nExecutions_per_trial,\n",
    "#     directory=f\"tuning/{tune_folder}/\",project_name= project_model)\n",
    "\n",
    "\"\"\"\n",
    "The Hyperband tuning algorithm uses adaptive resource allocation and early-stopping \n",
    "to quickly converge on a high-performing model\n",
    "\"\"\"\n",
    "# https://keras.io/api/keras_tuner/tuners/hyperband/\n",
    "# https://www.tensorflow.org/tutorials/keras/keras_tuner  by Hyperband tuner\n",
    "# https://keras.io/guides/keras_tuner/getting_started/\n",
    "# tuner = keras_tuner.RandomSearch(  #main_objective\n",
    "#https://medium.com/swlh/hyperparameter-tuning-in-keras-tensorflow-2-with-keras-tuner-randomsearch-hyperband-3e212647778f\n",
    "tuner = kt.Hyperband(\n",
    "    build_model_for_tuning, objective=objective_to_tued_and_monitor,\n",
    "    directory=f\"tuning/{tune_folder}/\",project_name= project_model)\n",
    "\n",
    "\n",
    "log_dir = f\"{tsb_path}\" + datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor=objective_to_tued_and_monitor, patience=nEarlyPatience)\n",
    "\n",
    "# tuner.search(train_ds, batch_size=BATCH_SIZE,epochs=EPOCHS,validation_data=val_ds,callbacks=[stop_early])\n",
    "tuner.search(train_ds, batch_size=BATCH_SIZE,epochs=EPOCHS,validation_data=val_ds,callbacks=[stop_early,tensorboard_callback])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c6208-4f0b-4c98-b847-f58b0a5b1308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(best_hps.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b213ad70-70d9-44d4-b70d-453d2f93c431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_End=time.time()\n",
    "t_elapsed=(t_End-t_Start)/60/60\n",
    "print('Total execution : ',round(t_elapsed,2)) \n",
    "print(datetime.now())\n",
    "print(\"=============================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a507e4-8375-4ccd-900f-77afa85f20ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load the TensorBoard notebook extension\n",
    "# # %tensorboard --logdir tsb_logs/tune/\n",
    "\n",
    "# load and comment to use notebook\n",
    "# %tensorboard --logdir tsb_logs/tune/ --host 0.0.0.0\n",
    "\n",
    "from tensorboard import notebook\n",
    "notebook.list() # View open TensorBoard instances\n",
    "# Control TensorBoard display. If no port is provided, \n",
    "# the most recently launched TensorBoard is used\n",
    "notebook.display(port=6006, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e48b135-26b3-4513-8bf0-bc3726258613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8cd0fa5-eb89-4a0d-9575-ccb10b981adc",
   "metadata": {},
   "source": [
    "# Retain by best model to product model to production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544099f0-746c-4e7b-af73-58e6351d90a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Build the model with the optimal hyperparameters and train it on the data to find  N epochs\")\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "# model.summary()\n",
    "history = best_model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958dd9f-5d48-4572-90af-b27754928f3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print(f'Best epoch: {best_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0719611-4770-47de-96fb-343698c0e381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Fid Optimal Best Epoch  by Re-instantiating the hypermodel and train it\")\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "# Retrain the model to get final\n",
    "history_hypermodel=model.fit(train_ds, validation_data=val_ds, epochs=best_epoch)\n",
    "\n",
    "val_loss, val_accuracy = model.evaluate(val_ds)\n",
    "print(f\"Eveluation on Val-Data at {best_epoch} epochs : loss={val_loss} and {main_metric}= {val_accuracy}\")\n",
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "print(f\"Evaluation on Test-Data at {best_epoch} epochs : loss={test_loss} and {main_metric}= {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6ca53e-78d8-400e-a48e-cdcb93d2030f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_metrics(history,metric):\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.title(metric)\n",
    "    plt.plot(history.history[metric], label='train')\n",
    "    plt.plot(history.history[f'val_{metric}'], label='validation')\n",
    "    plt.legend()\n",
    "    plt.show() \n",
    "\n",
    "print(\"4#Explore Result model\")\n",
    "plot_metrics(history_hypermodel,main_metric)\n",
    "plot_metrics(history_hypermodel,\"loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff49fc4-517c-4708-9861-13890bc08ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcf9758-bf3f-46aa-a0a0-1554b78d5baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_tuned_dir)\n",
    "# quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b15af3-b755-4bdb-acbf-ad8409ebc2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.keras.models.load_model(model_tuned_dir)\n",
    "#label_multi_severity\n",
    "sample={\"sla\":\"24x7 4Hrs Response Time\",\n",
    "        \"product_type\":\"Server\",\n",
    "        \"brand\":\"VMWare\",                \n",
    "        \"service_type\":\"Incident\",\n",
    "        \"incident_type\":\"General Incident\",\n",
    "        \"open_to_close_hour\":10,\n",
    "        \"response_to_resolved_hour\":8.000000 \\\n",
    "       }\n",
    "\n",
    "print(sample)\n",
    "              \n",
    "print(\"===============================================================================================================\")    \n",
    "print(\"convert pain data to serdor as input to predict\")    \n",
    "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
    "print(input_dict)\n",
    "\n",
    "predictionList = reloaded_model.predict(input_dict)\n",
    "print(predictionList)\n",
    "prob = tf.nn.sigmoid(predictionList[0])\n",
    "print(f\"{(100 * prob)} %  as Severity\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499f77f8-03ed-4485-b0e6-7f6f5da16223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
