{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c6dd8df7-648e-44c8-87ec-599b76a0934c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pair-code.github.io/lit/\n",
    "## https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "#https://www.tensorflow.org/tensorboard/get_started\n",
    "# https://www.tensorflow.org/tensorboard/dataframe_api\n",
    "# https://www.tensorflow.org/tensorboard/tensorboard_in_notebooks\n",
    "#https://keras.io/guides/keras_tuner/visualize_tuning/\n",
    "\n",
    "#https://github.com/technqvi/TimeSeriesML-FinMarket/blob/main/MultiVarToManyOutputLSTM.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "efd54814-e227-4302-9a81-3d6ddcf0e9a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n",
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "#https://codelabs.developers.google.com/codelabs/fraud-detection-ai-explanations?hl=en#0\n",
    "#he Explainable AI SDK and Copy Model to Deploy\n",
    "\n",
    "#https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive2/introduction_to_tensorflow/solutions/preprocessing_layers.ipynb\n",
    "#https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/custom/custom-tabular-bq-managed-dataset.ipynb\n",
    "#https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/experiments/comparing_local_trained_models.ipynb\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,DenseFeatures\n",
    "\n",
    "from datetime import date, timedelta, datetime # Date Functions\n",
    "import time\n",
    "\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "# from google.cloud import aiplatform as vertex_ai\n",
    "from tensorflow.python.keras.utils import data_utils\n",
    "\n",
    "import tensorboard as tb\n",
    "\n",
    "# from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "print(tf.__version__)\n",
    "print(tb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ebf74622-6c60-4bda-9b9b-9a799bfaed45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# major_ver, minor_ver, _ = version.parse(tb.__version__).release\n",
    "# assert major_ver >= 2 and minor_ver >= 3, \\\n",
    "#     \"This notebook requires TensorBoard 2.3 or later.\"\n",
    "# print(\"TensorBoard version: \", tb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "ab04a472-7abe-4315-b555-0583fdb2a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "# !rm -rf ./logs/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "d28a9538-ca23-45f0-817b-e4b067cf9ab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cateCols=['sla','product_type','brand','service_type','incident_type']\n",
    "numbericCols=['open_to_close_hour','response_to_resolved_hour']\n",
    "unusedCols=['severity_id','severity_name','label_binary_severity']\n",
    "\n",
    "# cateCols=['service_type','product_type','incident_type','sla']\n",
    "# numbericCols=[]\n",
    "# unusedCols=['severity_id','severity_name','label_binary_severity', 'open_to_close_hour','response_to_resolved_hour' ]\n",
    "\n",
    "# cateCols=['sla','product_type','service_type','incident_type']\n",
    "# numbericCols=['open_to_close_hour']\n",
    "# unusedCols=['severity_id','severity_name','label_binary_severity','brand','response_to_resolved_hour']\n",
    "\n",
    "labelCol='label_multi_severity'\n",
    "\n",
    "main_metric='accuracy'\n",
    "main_objective=f'val_{main_metric}'\n",
    "\n",
    "\n",
    "objective_to_tued_and_monitor='val_loss' \n",
    "# objective_to_tued_and_monitor=main_objective\n",
    "\n",
    "seed=1932\n",
    "\n",
    "EPOCHS =100\n",
    "BATCH_SIZE = 32\n",
    "unitList=[32,64,128]\n",
    "dropOutList= [0.1, 0.2]\n",
    "lrList=[0.01,0.001,0.0001]\n",
    "nAtleastMaxTrials=10\n",
    "nExecutions_per_trial=3  # 3,5\n",
    "nEarlyPatience=10\n",
    "\n",
    "\n",
    "# EPOCHS =100\n",
    "# BATCH_SIZE = 32\n",
    "# unitList=[64]\n",
    "# dropOutList= [0.1]\n",
    "# lrList=[0.01,0.001]\n",
    "# nAtleastMaxTrials=5\n",
    "# nExecutions_per_trial=1  # 3,5\n",
    "# nEarlyPatience=10\n",
    "\n",
    "model_tuned_dir='tuned_model_V2'\n",
    "\n",
    "# df['label_multi_severity'] =df['severity_name'].map({'Cosmatic':0,'Minor': 1, \"Major\": 2, \"Critical\": 3}) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0aa901-563f-4d2c-9203-67175431c32f",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "d1db6b46-bd0e-4053-8b4e-42e43f822da7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "projectId='pongthorn'\n",
    "client = bigquery.Client(project=projectId)\n",
    "dataset_id='SMartML'\n",
    "\n",
    "train_name='train_incident'\n",
    "validation_name='validation_incident'\n",
    "test_name='test_incident'\n",
    "\n",
    "train_table_id=f\"{projectId}.{dataset_id}.{train_name}\"\n",
    "val_tabel_id=f\"{projectId}.{dataset_id}.{validation_name}\"\n",
    "test_tabel_id=f\"{projectId}.{dataset_id}.{test_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "5ddea3e1-b732-48c4-95fc-637108802a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2731, 8)\n",
      "(341, 8)\n",
      "(342, 8)\n",
      "[2, 1, 0, 3]\n",
      "No target label : 4\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2731 entries, 0 to 2730\n",
      "Data columns (total 8 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   sla                        2731 non-null   object \n",
      " 1   product_type               2731 non-null   object \n",
      " 2   brand                      2731 non-null   object \n",
      " 3   service_type               2731 non-null   object \n",
      " 4   incident_type              2731 non-null   object \n",
      " 5   open_to_close_hour         2731 non-null   float64\n",
      " 6   response_to_resolved_hour  2731 non-null   float64\n",
      " 7   label_multi_severity       2731 non-null   Int64  \n",
      "dtypes: Int64(1), float64(2), object(5)\n",
      "memory usage: 173.5+ KB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sla</th>\n",
       "      <th>product_type</th>\n",
       "      <th>brand</th>\n",
       "      <th>service_type</th>\n",
       "      <th>incident_type</th>\n",
       "      <th>open_to_close_hour</th>\n",
       "      <th>response_to_resolved_hour</th>\n",
       "      <th>label_multi_severity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2726</th>\n",
       "      <td>24x7 4Hrs Response Time</td>\n",
       "      <td>Security</td>\n",
       "      <td>Trend Micro</td>\n",
       "      <td>Request</td>\n",
       "      <td>General Incident</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2727</th>\n",
       "      <td>24x7 6Hrs Resolution Time</td>\n",
       "      <td>Software</td>\n",
       "      <td>Trend Micro</td>\n",
       "      <td>Request</td>\n",
       "      <td>General Incident</td>\n",
       "      <td>0.883333</td>\n",
       "      <td>0.716667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2728</th>\n",
       "      <td>24x7 4Hrs Response Time</td>\n",
       "      <td>Security</td>\n",
       "      <td>Trend Micro</td>\n",
       "      <td>Request</td>\n",
       "      <td>OS / Firmware</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2729</th>\n",
       "      <td>24x7 4Hrs Response Time</td>\n",
       "      <td>Software</td>\n",
       "      <td>Trend Micro</td>\n",
       "      <td>Request</td>\n",
       "      <td>General Incident</td>\n",
       "      <td>7.400000</td>\n",
       "      <td>7.283333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2730</th>\n",
       "      <td>24x7 6Hrs Resolution Time</td>\n",
       "      <td>Software</td>\n",
       "      <td>Trend Micro</td>\n",
       "      <td>Request</td>\n",
       "      <td>General Incident</td>\n",
       "      <td>5.483333</td>\n",
       "      <td>5.483333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            sla product_type        brand service_type  \\\n",
       "2726    24x7 4Hrs Response Time     Security  Trend Micro      Request   \n",
       "2727  24x7 6Hrs Resolution Time     Software  Trend Micro      Request   \n",
       "2728    24x7 4Hrs Response Time     Security  Trend Micro      Request   \n",
       "2729    24x7 4Hrs Response Time     Software  Trend Micro      Request   \n",
       "2730  24x7 6Hrs Resolution Time     Software  Trend Micro      Request   \n",
       "\n",
       "         incident_type  open_to_close_hour  response_to_resolved_hour  \\\n",
       "2726  General Incident            0.333333                   0.333333   \n",
       "2727  General Incident            0.883333                   0.716667   \n",
       "2728     OS / Firmware           16.666667                  16.666667   \n",
       "2729  General Incident            7.400000                   7.283333   \n",
       "2730  General Incident            5.483333                   5.483333   \n",
       "\n",
       "      label_multi_severity  \n",
       "2726                     0  \n",
       "2727                     0  \n",
       "2728                     0  \n",
       "2729                     0  \n",
       "2730                     0  "
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def load_ml_data(data_path):\n",
    "#  df=pd.read_csv(data_path)\n",
    "#  df =df.drop(columns=unusedCols)\n",
    "#  return df\n",
    "\n",
    "# root_path='../../data'    \n",
    "# train = load_ml_data(f\"{root_path}/train_incident.csv\")\n",
    "# # val=train.copy()\n",
    "# val=load_ml_data(f\"{root_path}/validation_incident.csv\")\n",
    "# # test =val.copy()\n",
    "# test =load_ml_data(f\"{root_path}/test_incident.csv\")\n",
    "\n",
    "def load_data_bq(sql:str):\n",
    " \n",
    " query_result=client.query(sql)\n",
    " df=query_result.to_dataframe()\n",
    " df =df.drop(columns=unusedCols)\n",
    "  \n",
    " return df\n",
    "    \n",
    "\n",
    "train=load_data_bq(f\"SELECT * FROM {train_table_id}\")\n",
    "val=load_data_bq(f\"SELECT * FROM {val_tabel_id}\")\n",
    "test=load_data_bq(f\"SELECT * FROM {test_tabel_id}\")\n",
    "\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)\n",
    "\n",
    "\n",
    "labelList=list(train[labelCol].unique())\n",
    "print(labelList)\n",
    "nLabel=len(labelList)\n",
    "print(f\"No target label : {nLabel}\")\n",
    "\n",
    "# sr_predict=df.iloc[-1,:]\n",
    "# df=df.iloc[0:len(df)-1,:]\n",
    "                 \n",
    "print(train.info())\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "64adbe5e-d737-45a4-a04e-340a2926ad2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Train-No-Severity</th>\n",
       "      <th>Train-%-Severity</th>\n",
       "      <th>Val-No-Severity</th>\n",
       "      <th>Val-%-Severity</th>\n",
       "      <th>Test-No-Severity</th>\n",
       "      <th>Test-%-Severity</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label_multi_severity</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>511</td>\n",
       "      <td>19.0</td>\n",
       "      <td>53</td>\n",
       "      <td>16.0</td>\n",
       "      <td>55</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>778</td>\n",
       "      <td>28.0</td>\n",
       "      <td>97</td>\n",
       "      <td>28.0</td>\n",
       "      <td>92</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1341</td>\n",
       "      <td>49.0</td>\n",
       "      <td>180</td>\n",
       "      <td>53.0</td>\n",
       "      <td>181</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>101</td>\n",
       "      <td>4.0</td>\n",
       "      <td>11</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Train-No-Severity  Train-%-Severity  Val-No-Severity  \\\n",
       "label_multi_severity                                                         \n",
       "0                                   511              19.0               53   \n",
       "1                                   778              28.0               97   \n",
       "2                                  1341              49.0              180   \n",
       "3                                   101               4.0               11   \n",
       "\n",
       "                      Val-%-Severity  Test-No-Severity  Test-%-Severity  \n",
       "label_multi_severity                                                     \n",
       "0                               16.0                55             16.0  \n",
       "1                               28.0                92             27.0  \n",
       "2                               53.0               181             53.0  \n",
       "3                                3.0                14              4.0  "
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def CalPctEachTargetClass(dfx,colSev,colPctSev):\n",
    "    dfClassSummary=dfx.groupby([labelCol]).size().to_frame(colSev)\n",
    "    dfClassSummary[colPctSev]= dfClassSummary[colSev]/dfClassSummary[colSev].sum() *100\n",
    "    dfClassSummary=dfClassSummary.round(0)\n",
    "    return dfClassSummary\n",
    "\n",
    "pctDF1=CalPctEachTargetClass(train,'Train-No-Severity','Train-%-Severity')\n",
    "pctDF2=CalPctEachTargetClass(val,'Val-No-Severity','Val-%-Severity')\n",
    "pdcDF3=CalPctEachTargetClass(test,'Test-No-Severity','Test-%-Severity')\n",
    "pctDF=pd.concat([pctDF1,pctDF2,pdcDF3],axis=1)\n",
    "\n",
    "pctDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa30d3fd-14e9-4c3d-a6aa-45d8114f5362",
   "metadata": {},
   "source": [
    "# Process Data  Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "d9d3a943-09d5-4a43-b8d8-633bc4a90353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multiple_label_df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  features = dataframe.copy()\n",
    "  labels = features.pop(labelCol)\n",
    "  labels  = tf.keras.utils.to_categorical(labels, num_classes=nLabel)\n",
    "    \n",
    "  ds = tf.data.Dataset.from_tensor_slices(( dict(features), labels ))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(features))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "02ad8837-968e-4d68-bdb6-1c7eba97e69c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_normalization_layer(name, dataset):\n",
    "  # Create a Normalization layer for our feature.\n",
    "  normalizer = preprocessing.Normalization(axis=None)\n",
    "\n",
    "  # Prepare a Dataset that only yields our feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the statistics of the data.\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "237bce3c-9c77-4318-8682-fc36af0a6804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "  # Create a StringLookup layer which will turn strings into integer indices\n",
    "  if dtype == 'string':\n",
    "    index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
    "  else:\n",
    "    index = preprocessing.IntegerLookup(max_tokens=max_tokens)\n",
    "\n",
    "  # Prepare a Dataset that only yields our feature\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the set of possible values and assign them a fixed integer index.\n",
    "  index.adapt(feature_ds)\n",
    "\n",
    "  # Create a Discretization for our integer indices.\n",
    "  encoder = preprocessing.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "\n",
    "  # Apply one-hot encoding to our indices. The lambda function captures the\n",
    "  # layer so we can use them, or include them in the functional model later.\n",
    "  return lambda feature: encoder(index(feature))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "a5c78f01-000a-4d3e-9fbf-6f4660cfb75b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size =32\n",
    "train_ds = multiple_label_df_to_dataset (train, batch_size=batch_size)\n",
    "val_ds = multiple_label_df_to_dataset(val, batch_size=batch_size)\n",
    "test_ds = multiple_label_df_to_dataset(test, batch_size=batch_size)\n",
    "# for element in train_ds.as_numpy_iterator():\n",
    "#     print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "723a69b6-d9c1-49d9-bc81-ed421d8700cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "open_to_close_hour\n",
      "header: Mean=120.7417429512999 and Std=339.9325107996055\n",
      "========================================================================\n",
      "response_to_resolved_hour\n",
      "header: Mean=92.4087208592701 and Std=292.40800375934094\n",
      "========================================================================\n",
      "sla = 7 : ['24x7 4Hrs Resolution Time' '24x7 4Hrs Response Time'\n",
      " '24x7 6Hrs Resolution Time' '8x5 4Hrs Response Time'\n",
      " '9x5 NBD 4Hrs Response Time' '24x7 6Hrs Response Time' 'Non MA']\n",
      "sla\n",
      "product_type = 13 : ['Other' 'Printer' 'Storage' 'Software' 'Server' 'Access Point' 'Switch'\n",
      " 'Hardware' 'Tape Library' 'Security' 'Service' 'Notebook' 'Firewall']\n",
      "product_type\n",
      "brand = 28 : ['F5' 'HP' 'EMC' 'HPE' 'IBM' 'YIP' 'DELL' 'QNAP' 'Cisco' 'SAPB1' 'Veeam'\n",
      " 'NetApp' 'Oracle' 'VMWare' 'Zabbix' 'eUnite' 'Nutanix' 'Red Hat'\n",
      " 'Veritas' 'Alfresco' 'Broadcom' 'Fortinet' 'Commvault' 'Microsoft'\n",
      " 'Palo Alto' 'CIMCO-CMMS' 'CheckPoint' 'Trend Micro']\n",
      "brand\n",
      "service_type = 2 : ['Incident' 'Request']\n",
      "service_type\n",
      "incident_type = 21 : ['General Incident' 'Network Adapter Failure' 'Software'\n",
      " 'Maintenance System' 'Hard Disk Drive Failure' 'Power Supply Failure'\n",
      " 'Report' 'System Board Failure' 'Memory Failure' 'Network Card Failure'\n",
      " 'Controller/Node Failure' 'Backup Failure' 'Upgrade Software'\n",
      " 'CPU Failure' 'OS / Firmware' 'Configuration Change'\n",
      " 'Cache Battery Failure' 'Other Failure' 'Network Cable Failure'\n",
      " 'Battery Failure' 'Fan Failure']\n",
      "incident_type\n"
     ]
    }
   ],
   "source": [
    "all_inputs = []\n",
    "encoded_features = []\n",
    "numInputFeatToInitNodeUnit=0\n",
    "# Numeric features.\n",
    "for header in numbericCols:\n",
    "  print(header)  \n",
    "  stat_data=train[header].describe()\n",
    "  print(f\"header: Mean={stat_data['mean']} and Std={stat_data['std']}\") \n",
    "  numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "  normalization_layer = get_normalization_layer(header, train_ds)\n",
    "  encoded_numeric_col = normalization_layer(numeric_col)\n",
    "  all_inputs.append(numeric_col)\n",
    "  encoded_features.append(encoded_numeric_col)\n",
    "  print(\"========================================================================\")\n",
    "    \n",
    "numInputFeatToInitNodeUnit=numInputFeatToInitNodeUnit+len(numbericCols)\n",
    " \n",
    "# Categorical features encoded as string.\n",
    "categorical_cols = cateCols\n",
    "for header in categorical_cols:\n",
    "    \n",
    "  listCateItem=train[header].unique()\n",
    "  noCateItem=len(listCateItem)\n",
    "  numInputFeatToInitNodeUnit=numInputFeatToInitNodeUnit+noCateItem +1  # last 1 is unknow  \n",
    "  print(f\"{header} = {noCateItem} : {listCateItem}\")    \n",
    "    \n",
    "  print(header)  \n",
    "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "  encoding_layer = get_category_encoding_layer(header, train_ds, dtype='string')\n",
    "                                        \n",
    "  encoded_categorical_col = encoding_layer(categorical_col)\n",
    "  all_inputs.append(categorical_col)\n",
    "  encoded_features.append(encoded_categorical_col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f85f8a-4471-44e1-a389-102232c296e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf75dae1-683f-403d-9f2d-aca95c970a21",
   "metadata": {},
   "source": [
    "# Tune HyperParameter By Keras Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "07dacd5c-0af5-4415-a20e-6e808850de18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[32, 64, 128, 78] [0.1, 0.2] [0.01, 0.001, 0.0001]\n",
      "24\n"
     ]
    }
   ],
   "source": [
    "unitList.append(numInputFeatToInitNodeUnit)\n",
    "# unitList.append(numInputFeatToInitNodeUnit*2)\n",
    "print(unitList,dropOutList,lrList)\n",
    "nMax_trials=len(unitList)*len(dropOutList)*len(lrList)\n",
    "if nMax_trials<nAtleastMaxTrials:\n",
    " nMax_trials=nAtleastMaxTrials \n",
    "print(nMax_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "4267752a-455a-4fde-a7b9-518229a81a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#https://github.com/technqvi/TimeSeriesML-FinMarket/blob/main/lstm-tune-dev/Tuned-MultiVarToManyOutputLSTM.ipynb\n",
    "#https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "#https://keras.io/guides/keras_tuner/getting_started/\n",
    "import keras_tuner as kt\n",
    "from keras_tuner.engine.hyperparameters import HyperParameters\n",
    "tune_folder=\"IncidentMLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "e44461a8-a751-40ae-98c4-e6578a1855e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start tund at 2023-11-11 22:49:18.245736\n",
      "=============================================================\n",
      "IncidentMLP_111123_2249\n"
     ]
    }
   ],
   "source": [
    "t_Start=time.time()\n",
    "\n",
    "print(f\"Start tund at {datetime.now()}\")\n",
    "print(\"=============================================================\")\n",
    "buildtime = datetime.now().strftime('%d%m%y_%H%M')\n",
    "project_model=f\"{tune_folder}_{buildtime}\"\n",
    "print(project_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "c97a4d39-0145-49cd-9abf-277e76bd03af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model_for_tuning(hp):\n",
    "    \n",
    "    all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "    \n",
    "    x = tf.keras.layers.Dense(hp.Choice('units', unitList), activation=\"relu\")(all_features)\n",
    "    \n",
    "    x = tf.keras.layers.Dropout(hp.Choice('Dropout_rate',dropOutList))(x)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(nLabel,activation=tf.nn.softmax)(x)\n",
    "    \n",
    "    model = tf.keras.Model(all_inputs, output)\n",
    "    # model.compile(optimizer='adam',loss=tf.keras.losses.CategoricalCrossentropy(),metrics=[main_metric])\n",
    "    \n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=lrList) \n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss=tf.keras.losses.CategoricalCrossentropy(),metrics=[main_metric])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a501b2-88c6-4b15-9724-2b05849fde59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 7 Complete [00h 00m 04s]\n",
      "val_loss: 0.7478495240211487\n",
      "\n",
      "Best val_loss So Far: 0.7266939878463745\n",
      "Total elapsed time: 00h 00m 28s\n",
      "\n",
      "Search: Running Trial #8\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "128               |32                |units\n",
      "0.2               |0.1               |Dropout_rate\n",
      "0.01              |0.01              |learning_rate\n",
      "2                 |2                 |tuner/epochs\n",
      "0                 |0                 |tuner/initial_epoch\n",
      "4                 |4                 |tuner/bracket\n",
      "0                 |0                 |tuner/round\n",
      "\n",
      "Epoch 1/2\n"
     ]
    }
   ],
   "source": [
    "# https://keras.io/api/keras_tuner/tuners/bayesian/\n",
    "# tuner = kt.BayesianOptimization(\n",
    "#     build_model_for_tuning, objective=objective_to_tued_and_monitor,seed=seed,\n",
    "#     max_trials=nMax_trials,executions_per_trial=nExecutions_per_trial,\n",
    "#     directory=f\"tuning/{tune_folder}/\",project_name= project_model)\n",
    "\n",
    "\"\"\"\n",
    "The Hyperband tuning algorithm uses adaptive resource allocation and early-stopping \n",
    "to quickly converge on a high-performing model\n",
    "\"\"\"\n",
    "# https://keras.io/api/keras_tuner/tuners/hyperband/\n",
    "# https://www.tensorflow.org/tutorials/keras/keras_tuner  by Hyperband tuner\n",
    "# https://keras.io/guides/keras_tuner/getting_started/\n",
    "# tuner = keras_tuner.RandomSearch(  #main_objective\n",
    "#https://medium.com/swlh/hyperparameter-tuning-in-keras-tensorflow-2-with-keras-tuner-randomsearch-hyperband-3e212647778f\n",
    "tuner = kt.Hyperband(\n",
    "    build_model_for_tuning, objective=objective_to_tued_and_monitor,\n",
    "    directory=f\"tuning/{tune_folder}/\",project_name= project_model)\n",
    "\n",
    "stop_early = tf.keras.callbacks.EarlyStopping(monitor=objective_to_tued_and_monitor, patience=nEarlyPatience)\n",
    "tuner.search(train_ds, batch_size=BATCH_SIZE,epochs=EPOCHS,validation_data=val_ds,callbacks=[stop_early])\n",
    "\n",
    "# log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# tsb_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "# tuner.search(train_ds, batch_size=BATCH_SIZE,epochs=EPOCHS,validation_data=val_ds,callbacks=[stop_early, tsb_callback])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4c6208-4f0b-4c98-b847-f58b0a5b1308",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(best_hps.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b213ad70-70d9-44d4-b70d-453d2f93c431",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "t_End=time.time()\n",
    "t_elapsed=(t_End-t_Start)/60/60\n",
    "print('Total execution : ',round(t_elapsed,2)) \n",
    "print(datetime.now())\n",
    "print(\"=============================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a507e4-8375-4ccd-900f-77afa85f20ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the TensorBoard notebook extension\n",
    "# %load_ext tensorboard\n",
    "# # %tensorboard --logdir logs/fit \n",
    "# #%tensorboard --logdir logs/fit --host 0.0.0.0\n",
    "\n",
    "# from tensorboard import notebook\n",
    "# notebook.list() # View open TensorBoard instances\n",
    "# # Control TensorBoard display. If no port is provided, \n",
    "# # the most recently launched TensorBoard is used\n",
    "# notebook.display(port=6006, height=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e48b135-26b3-4513-8bf0-bc3726258613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8cd0fa5-eb89-4a0d-9575-ccb10b981adc",
   "metadata": {},
   "source": [
    "# Retain by best model to product model to production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544099f0-746c-4e7b-af73-58e6351d90a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Build the model with the optimal hyperparameters and train it on the data to find  N epochs\")\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "# model.summary()\n",
    "history = best_model.fit(train_ds, validation_data=val_ds, epochs=EPOCHS, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958dd9f-5d48-4572-90af-b27754928f3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_acc_per_epoch = history.history['val_accuracy']\n",
    "best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "print(f'Best epoch: {best_epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0719611-4770-47de-96fb-343698c0e381",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Fid Optimal Best Epoch  by Re-instantiating the hypermodel and train it\")\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "# Retrain the model to get final\n",
    "history_hypermodel=model.fit(train_ds, validation_data=val_ds, epochs=best_epoch)\n",
    "\n",
    "val_loss, val_accuracy = model.evaluate(val_ds)\n",
    "print(f\"Eveluation on Val-Data at {best_epoch} epochs : loss={val_loss} and {main_metric}= {val_accuracy}\")\n",
    "test_loss, test_accuracy = model.evaluate(test_ds)\n",
    "print(f\"Evaluation on Test-Data at {best_epoch} epochs : loss={test_loss} and {main_metric}= {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6ca53e-78d8-400e-a48e-cdcb93d2030f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_metrics(history,metric):\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.title(metric)\n",
    "    plt.plot(history.history[metric], label='train')\n",
    "    plt.plot(history.history[f'val_{metric}'], label='validation')\n",
    "    plt.legend()\n",
    "    plt.show() \n",
    "\n",
    "print(\"4#Explore Result model\")\n",
    "plot_metrics(history_hypermodel,main_metric)\n",
    "plot_metrics(history_hypermodel,\"loss\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff49fc4-517c-4708-9861-13890bc08ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcf9758-bf3f-46aa-a0a0-1554b78d5baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_tuned_dir)\n",
    "# quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b15af3-b755-4bdb-acbf-ad8409ebc2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = tf.keras.models.load_model(model_tuned_dir)\n",
    "#label_multi_severity\n",
    "sample={\"sla\":\"24x7 4Hrs Response Time\",\n",
    "        \"product_type\":\"Server\",\n",
    "        \"brand\":\"VMWare\",                \n",
    "        \"service_type\":\"Incident\",\n",
    "        \"incident_type\":\"General Incident\",\n",
    "        \"open_to_close_hour\":10,\n",
    "        \"response_to_resolved_hour\":8.000000 \\\n",
    "       }\n",
    "\n",
    "print(sample)\n",
    "              \n",
    "print(\"===============================================================================================================\")    \n",
    "print(\"convert pain data to serdor as input to predict\")    \n",
    "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
    "print(input_dict)\n",
    "\n",
    "predictionList = reloaded_model.predict(input_dict)\n",
    "print(predictionList)\n",
    "prob = tf.nn.sigmoid(predictionList[0])\n",
    "print(f\"{(100 * prob)} %  as Severity\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499f77f8-03ed-4485-b0e6-7f6f5da16223",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
