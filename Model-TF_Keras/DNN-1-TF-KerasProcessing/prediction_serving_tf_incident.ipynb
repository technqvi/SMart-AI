{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "50d3f8fc-de52-4c03-b3da-46145fc5adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import json\n",
    "import os\n",
    "from datetime import date,datetime,timedelta\n",
    "\n",
    "import functions_framework\n",
    "\n",
    "from google.cloud.exceptions import NotFound\n",
    "from google.api_core.exceptions import BadRequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b9e4719a-8535-4db4-b01f-806edace1dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @functions_framework.http\n",
    "# def predict_incident_severity_by_tf(request):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "163bee21-3083-46bf-ad1e-f846ad1e5175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data from model\n"
     ]
    }
   ],
   "source": [
    "# load_model_option=os.environ.get('load_model_option', '1')\n",
    "load_model_option='1'\n",
    "\n",
    "# predict_from_date=os.environ.get('predict_from_date', '')\n",
    "predict_from_date='2000-01-01'\n",
    "predict_from_date=''\n",
    "\n",
    "if load_model_option=='1':\n",
    "  PATH_FOLDER_ARTIFACTS=\"model\"  \n",
    "elif load_model_option=='2':\n",
    "  PATH_FOLDER_ARTIFACTS=\"gs://tf1-incident-pongthorn/model\"\n",
    "else:\n",
    "  raise Exception(\"Allow you to set 1 for local and 2 for google storage\")\n",
    "\n",
    "print(f\"Load data from {PATH_FOLDER_ARTIFACTS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fe374606-a5e7-4850-b91b-b14a6dbdcf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_id = \"pongthorn.SMartML.new_incident\"\n",
    "predictResult_table_id=\"pongthorn.SMartML.new_result_prediction_incident\"\n",
    "\n",
    "\n",
    "unUsedColtoPredict=['severity','id','severity_id','severity_name','imported_at']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a87c14ef-f2bc-4bdd-aeae-e333350a89b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Cosmatic': 0, 'Minor': 1, 'Major': 2, 'Critical': 3}\n"
     ]
    }
   ],
   "source": [
    "mapping_file=\"incident_sevirity_to_class.json\"\n",
    "# with open(mapping_file, 'r') as json_file:\n",
    "#      map_sevirity_to_class= json.load(json_file)\n",
    "# print(map_sevirity_to_class)\n",
    "\n",
    "map_sevirity_to_class={'Cosmatic': 0, 'Minor': 1, 'Major': 2, 'Critical': 3}\n",
    "print(map_sevirity_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5c09568e-df8f-4715-adb8-8f94e46802dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get data between 2023-03-29 to 2023-03-30 to predict sevirity level\n"
     ]
    }
   ],
   "source": [
    "# Get today's date\n",
    "prediction_datetime=datetime.now()\n",
    "\n",
    "today = date.today()\n",
    "\n",
    "# Yesterday date\n",
    "if predict_from_date=='':\n",
    " yesterday = today - timedelta(days = 1)\n",
    " str_yesterday=yesterday.strftime('%Y-%m-%d')\n",
    "else:\n",
    " str_yesterday=predict_from_date\n",
    "\n",
    "str_today=today.strftime('%Y-%m-%d')\n",
    "\n",
    "print(f\"Get data between {str_yesterday} to {str_today} to predict sevirity level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4a9808f7-b785-430b-a34c-00686a10cdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_bq(sql:str):\n",
    " client_bq = bigquery.Client()\n",
    " query_result=client_bq.query(sql)\n",
    " df=query_result.to_dataframe()\n",
    " return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a4a03ba4-3ed0-4fa4-b376-3898c271ccb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1 entries, 0 to 0\n",
      "Data columns (total 12 columns):\n",
      " #   Column                     Non-Null Count  Dtype         \n",
      "---  ------                     --------------  -----         \n",
      " 0   id                         1 non-null      Int64         \n",
      " 1   severity_id                1 non-null      Int64         \n",
      " 2   severity                   1 non-null      int64         \n",
      " 3   severity_name              1 non-null      object        \n",
      " 4   sla                        1 non-null      object        \n",
      " 5   product_type               1 non-null      object        \n",
      " 6   brand                      1 non-null      object        \n",
      " 7   service_type               1 non-null      object        \n",
      " 8   incident_type              1 non-null      object        \n",
      " 9   open_to_close_hour         1 non-null      float64       \n",
      " 10  response_to_resolved_hour  1 non-null      float64       \n",
      " 11  imported_at                1 non-null      datetime64[ns]\n",
      "dtypes: Int64(2), datetime64[ns](1), float64(2), int64(1), object(6)\n",
      "memory usage: 106.0+ bytes\n",
      "None\n",
      "     id  severity_id  severity severity_name                      sla  \\\n",
      "0  2570            4         0      Cosmatic  24x7 4Hrs Response Time   \n",
      "\n",
      "  product_type   brand service_type  incident_type  open_to_close_hour  \\\n",
      "0      Storage  NetApp      Request  Other Failure                 0.5   \n",
      "\n",
      "   response_to_resolved_hour                imported_at  \n",
      "0                        0.5 2023-03-29 22:00:06.019094  \n"
     ]
    }
   ],
   "source": [
    "sql=f\"\"\"\n",
    "SELECT *  FROM `{table_id}` \n",
    "WHERE DATE(imported_at) >= '{str_yesterday}' and DATE(imported_at) < '{str_today}' \n",
    "\n",
    "\"\"\"\n",
    "#LIMIT 2\n",
    "dfNewData=load_data_bq(sql)\n",
    "dfNewData=dfNewData.drop_duplicates(subset=['id'],keep='first')\n",
    "\n",
    "dfNewData.insert(2, 'severity', dfNewData['severity_name'].map(map_sevirity_to_class),True)\n",
    "\n",
    "\n",
    "print(dfNewData.info())\n",
    "print(dfNewData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ece7849d-e583-436d-86be-3ea189fe0b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load from model\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sla (InputLayer)               [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " product_type (InputLayer)      [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " brand (InputLayer)             [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " service_type (InputLayer)      [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " incident_type (InputLayer)     [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " open_to_close_hour (InputLayer  [(None, 1)]         0           []                               \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " response_to_resolved_hour (Inp  [(None, 1)]         0           []                               \n",
      " utLayer)                                                                                         \n",
      "                                                                                                  \n",
      " string_lookup_1 (StringLookup)  (None, 1)           0           ['sla[0][0]']                    \n",
      "                                                                                                  \n",
      " string_lookup_2 (StringLookup)  (None, 1)           0           ['product_type[0][0]']           \n",
      "                                                                                                  \n",
      " string_lookup_3 (StringLookup)  (None, 1)           0           ['brand[0][0]']                  \n",
      "                                                                                                  \n",
      " string_lookup_4 (StringLookup)  (None, 1)           0           ['service_type[0][0]']           \n",
      "                                                                                                  \n",
      " string_lookup_5 (StringLookup)  (None, 1)           0           ['incident_type[0][0]']          \n",
      "                                                                                                  \n",
      " normalization_1 (Normalization  (None, 1)           3           ['open_to_close_hour[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " normalization_2 (Normalization  (None, 1)           3           ['response_to_resolved_hour[0][0]\n",
      " )                                                               ']                               \n",
      "                                                                                                  \n",
      " category_encoding_1 (CategoryE  (None, 7)           0           ['string_lookup_1[0][0]']        \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " category_encoding_2 (CategoryE  (None, 10)          0           ['string_lookup_2[0][0]']        \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " category_encoding_3 (CategoryE  (None, 21)          0           ['string_lookup_3[0][0]']        \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " category_encoding_4 (CategoryE  (None, 3)           0           ['string_lookup_4[0][0]']        \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " category_encoding_5 (CategoryE  (None, 22)          0           ['string_lookup_5[0][0]']        \n",
      " ncoding)                                                                                         \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 65)           0           ['normalization_1[0][0]',        \n",
      "                                                                  'normalization_2[0][0]',        \n",
      "                                                                  'category_encoding_1[0][0]',    \n",
      "                                                                  'category_encoding_2[0][0]',    \n",
      "                                                                  'category_encoding_3[0][0]',    \n",
      "                                                                  'category_encoding_4[0][0]',    \n",
      "                                                                  'category_encoding_5[0][0]']    \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           2112        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 32)           0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 4)            132         ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,250\n",
      "Trainable params: 2,244\n",
      "Non-trainable params: 6\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model = tf.keras.models.load_model(PATH_FOLDER_ARTIFACTS)    \n",
    "    print(f\"Load from {PATH_FOLDER_ARTIFACTS}\")\n",
    "    print(model.summary())\n",
    "except Exception as error:\n",
    "  print(str(error))\n",
    "  raise error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "687618c3-7fdb-4294-bfc4-ce0783cc8212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2570 - 0(Cosmatic)\n",
      "1/1 [==============================] - 1s 622ms/step\n",
      "0.30158505,0.6914548,0.0069139185,4.6226673e-05\n",
      "[[25.244595 37.280926 18.80158  18.6729  ]] %   as 1\n",
      "======================================================================================\n"
     ]
    }
   ],
   "source": [
    "pdPrediction=pd.DataFrame(columns=['_id','predict_severity','prob_severity'])\n",
    "\n",
    "for  row_dict in dfNewData.to_dict(orient=\"records\"):\n",
    "      incident_id=row_dict['id']\n",
    "      print(f\"{incident_id} - {row_dict['severity']}({row_dict['severity_name']})\") \n",
    "      for key_removed in unUsedColtoPredict:\n",
    "       row_dict.pop(key_removed)\n",
    "      # print(row_dict)  \n",
    "\n",
    "      input_dict = {name: tf.convert_to_tensor([value]) for name, value in row_dict.items()}\n",
    "\n",
    "\n",
    "      predictionResult = model.predict(input_dict)\n",
    "      result_str=','.join([ str(prob) for prob in predictionResult[0]])  \n",
    "      print(result_str)   \n",
    "\n",
    "      prob = tf.nn.softmax(predictionResult)\n",
    "      prob_pct=(100 * prob)  \n",
    "      _class = tf.argmax(predictionResult,-1).numpy()[0]\n",
    "      \n",
    "      dictPrediction={'_id':incident_id, 'predict_severity':_class,'prob_severity':result_str} \n",
    "      pdPrediction =pd.concat([pdPrediction,pd.DataFrame.from_dict([dictPrediction])] )\n",
    "\n",
    "      print(f\"{prob_pct} %   as {_class}\")     \n",
    "      print(\"======================================================================================\")\n",
    "            \n",
    "dfPredictData=pd.merge(dfNewData,pdPrediction,how='inner',left_on='id',right_on='_id')\n",
    "dfPredictData=dfPredictData.drop(columns=['_id'])\n",
    "dfPredictData['predict_severity']=dfPredictData['predict_severity'].astype('int')\n",
    "dfPredictData=dfPredictData[['id','prob_severity','predict_severity','severity']]\n",
    "dfPredictData['prediction_item_date']= datetime.strptime(str(yesterday), '%Y-%m-%d')\n",
    "dfPredictData['prediction_datetime']=prediction_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3488b1e9-b0df-4c1c-b834-3006c7809aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 1 entries, 0 to 0\n",
      "Data columns (total 6 columns):\n",
      " #   Column                Non-Null Count  Dtype         \n",
      "---  ------                --------------  -----         \n",
      " 0   id                    1 non-null      object        \n",
      " 1   prob_severity         1 non-null      object        \n",
      " 2   predict_severity      1 non-null      int32         \n",
      " 3   severity              1 non-null      int64         \n",
      " 4   prediction_item_date  1 non-null      datetime64[ns]\n",
      " 5   prediction_datetime   1 non-null      datetime64[ns]\n",
      "dtypes: datetime64[ns](2), int32(1), int64(1), object(2)\n",
      "memory usage: 52.0+ bytes\n",
      "None\n",
      "     id                                    prob_severity  predict_severity  \\\n",
      "0  2570  0.30158505,0.6914548,0.0069139185,4.6226673e-05                 1   \n",
      "\n",
      "   severity prediction_item_date        prediction_datetime  \n",
      "0         0           2023-03-29 2023-03-30 22:03:37.861321  \n"
     ]
    }
   ],
   "source": [
    "print(dfPredictData.info())\n",
    "print(dfPredictData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ec2119b1-a353-48fd-91e2-6203367758c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict Result Table pongthorn.SMartML.new_result_prediction_incident already exists.\n"
     ]
    }
   ],
   "source": [
    "#https://cloud.google.com/bigquery/docs/samples/bigquery-create-table#bigquery_create_table-python\n",
    "\n",
    "try:\n",
    "    client = bigquery.Client()\n",
    "    client.get_table(predictResult_table_id)  # Make an API request.\n",
    "    print(\"Predict Result Table {} already exists.\".format(predictResult_table_id))\n",
    "except Exception as ex:\n",
    "    schema = [\n",
    "    bigquery.SchemaField(\"id\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"prob_severity\", \"STRING\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"predict_severity\", \"INTEGER\", mode=\"REQUIRED\"),\n",
    "    bigquery.SchemaField(\"severity\", \"INTEGER\", mode=\"REQUIRED\"),    \n",
    "    bigquery.SchemaField(\"prediction_item_date\", \"DATETIME\", mode=\"REQUIRED\"),    \n",
    "    bigquery.SchemaField(\"prediction_datetime\", \"DATETIME\", mode=\"REQUIRED\") \n",
    "    ]\n",
    "\n",
    "    table = bigquery.Table(predictResult_table_id,schema=schema)\n",
    "    table.time_partitioning = bigquery.TimePartitioning(\n",
    "    type_=bigquery.TimePartitioningType.DAY,field=\"prediction_item_date\")\n",
    "    \n",
    "    table = client.create_table(table)  # Make an API request.\n",
    "    \n",
    "    print(\n",
    "        \"Created table {}.{}.{}\".format(table.project, table.dataset_id, table.table_id)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "43baaa51-ffc8-451f-ba8d-d9d3003256da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Prediction ML  1 Imported igquery successfully\n"
     ]
    }
   ],
   "source": [
    "def loadDataFrameToBQ():\n",
    "    try:\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            write_disposition=\"WRITE_APPEND\",\n",
    "        )\n",
    "\n",
    "        job = client.load_table_from_dataframe(\n",
    "            dfPredictData, predictResult_table_id, job_config=job_config\n",
    "        )\n",
    "        job.result()  # Wait for the job to complete.\n",
    "        print(\"Total Prediction ML \", len(dfPredictData), \"Imported igquery successfully\")\n",
    "\n",
    "    except BadRequest as e:\n",
    "        print(\"Bigquery Error\\n\")\n",
    "        for e in job.errors:\n",
    "            print('ERROR: {}'.format(e['message']))\n",
    "\n",
    "try:\n",
    "    loadDataFrameToBQ()\n",
    "except Exception as ex:\n",
    "    raise ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bbed47-52ee-4a09-bed3-12c70cf03f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# return 'ok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f842295f-2a5a-4ee7-8abb-fa0fdd06a96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == \"__main__\":\n",
    "#  result=predict_incident_severity_by_tf(None)\n",
    "#  print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950857cf-eb31-4973-9925-aa7f0b1434d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c87c03-93b1-4461-af09-9418520b4606",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
