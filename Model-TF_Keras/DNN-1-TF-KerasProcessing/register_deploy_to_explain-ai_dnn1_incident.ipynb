{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efd54814-e227-4302-9a81-3d6ddcf0e9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 09:48:49.851642: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n",
      "1.26.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "from datetime import date, timedelta, datetime # Date Functions\n",
    "import time\n",
    "import os\n",
    "\n",
    "import google.cloud.aiplatform as aip\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "print(tf.__version__)\n",
    "print(aip.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9877dcac-05b5-45d7-a3ae-b4b7ebcb08b1",
   "metadata": {},
   "source": [
    "# Get explanations locally in Vertex AI Workbench user-managed notebooks\n",
    "* https://cloud.google.com/vertex-ai/docs/predictions/overview\n",
    "* https://cloud.google.com/vertex-ai/docs/explainable-ai/configuring-explanations-feature-based\n",
    "* https://cloud.google.com/vertex-ai/docs/explainable-ai/getting-explanations#local-explanations\n",
    "* https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-online-predictions#aiplatform_explain_tabular_sample-python_vertex_ai_sdk\n",
    "* https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-batch-predictions\n",
    "* https://cloud.google.com/vertex-ai/docs/explainable-ai/tensorflow\n",
    "\n",
    "# Use SDK  VertextAi Sample Code\n",
    "* https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/explainable_ai/sdk_custom_tabular_regression_online_explain_get_metadata.ipynb\n",
    "* https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/explainable_ai/sdk_custom_tabular_regression_online_explain.ipynb\n",
    "* https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/explainable_ai/sdk_custom_tabular_regression_batch_explain.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cb7286-7f33-4140-b5ca-129b69dd822b",
   "metadata": {},
   "source": [
    "# Varaible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c943ea3-e29e-42f0-881d-657e9f36bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_multi_severity\n",
    "project_id=\"pongthorn\"\n",
    "region='asia-southeast1'\n",
    "\n",
    "model_dir='gs://demo2-tf-incident-pongthorn/demo_model_tf' # demo\n",
    "# model_with_meta_dir='gs://demo2-tf-incident-pongthorn/demo_model_meta_tf'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac585a9f-71ed-4406-bef1-d7d8ac065804",
   "metadata": {},
   "source": [
    "# Load model from GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a3e401-2b57-4f52-80c7-e705154114be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "#model_with_meta_dir='model_with_meta'\n",
    "local_model= tf.keras.models.load_model(model_dir)\n",
    "print(local_model.tensorflow_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86af40e-2142-4c2f-af6d-13bae8cd463e",
   "metadata": {},
   "source": [
    "# Prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd916787-bacc-4537-b2b6-1c6d62932de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sla': ['24x7 6Hrs Resolution Time'], 'product_type': ['Storage'], 'brand': ['NetApp'], 'service_type': ['Incident'], 'incident_type': ['General Incident'], 'open_to_close_hour': [1268.9333333333334], 'response_to_resolved_hour': [1268.8]}\n",
      "===============================================================================================================\n",
      "convert pain data to serdor as input to predict\n",
      "{'sla': <tf.Tensor: shape=(1, 1), dtype=string, numpy=array([[b'24x7 6Hrs Resolution Time']], dtype=object)>, 'product_type': <tf.Tensor: shape=(1, 1), dtype=string, numpy=array([[b'Storage']], dtype=object)>, 'brand': <tf.Tensor: shape=(1, 1), dtype=string, numpy=array([[b'NetApp']], dtype=object)>, 'service_type': <tf.Tensor: shape=(1, 1), dtype=string, numpy=array([[b'Incident']], dtype=object)>, 'incident_type': <tf.Tensor: shape=(1, 1), dtype=string, numpy=array([[b'General Incident']], dtype=object)>, 'open_to_close_hour': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1268.9333]], dtype=float32)>, 'response_to_resolved_hour': <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1268.8]], dtype=float32)>}\n",
      "1/1 [==============================] - 0s 339ms/step\n",
      "[[0.06162358 0.6317878  0.29896042 0.00762823]]\n",
      "[20.064762 35.485683 25.43947  19.010086] % at 1 as Severity\n"
     ]
    }
   ],
   "source": [
    "sample= {\"sla\": [\"24x7 6Hrs Resolution Time\"], \"product_type\": [\"Storage\"], \"brand\": [\"NetApp\"], \n",
    "         \"service_type\": [\"Incident\"], \"incident_type\": [\"General Incident\"], \n",
    "         \"open_to_close_hour\": [1268.9333333333334], \"response_to_resolved_hour\": [1268.8]}\n",
    "\n",
    "print(sample)\n",
    "              \n",
    "print(\"===============================================================================================================\")    \n",
    "print(\"convert pain data to serdor as input to predict\")    \n",
    "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
    "print(input_dict)\n",
    "\n",
    "predictionList = local_model.predict(input_dict)\n",
    "print(predictionList)\n",
    "\n",
    "prob = tf.nn.softmax(predictionList[0])\n",
    "print(f\"{(100 * prob)} % at {np.argmax(prob, axis=0)} as Severity\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60cd841-c4f7-43fb-a0eb-c7018e32988f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36991efc-88e3-4c00-8c4f-b3dcd6acccac",
   "metadata": {},
   "source": [
    "# Get prepared the serving function signature input/output for explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac5fb999-670a-43fa-b2a9-43510d621469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model input name: ['open_to_close_hour', 'response_to_resolved_hour', 'sla', 'product_type', 'brand', 'service_type', 'incident_type']\n",
      "Model output name: ['dense_1']\n"
     ]
    }
   ],
   "source": [
    "input_name = local_model.input_names\n",
    "print(\"Model input name:\", input_name)\n",
    "output_name = local_model.output_names\n",
    "print(\"Model output name:\", output_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ffeb1db-0b33-40c8-86a6-9cc5a8d12d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = tf.saved_model.load(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7ea0022-acc3-44dd-aa96-1c503f236f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brand', 'incident_type', 'response_to_resolved_hour', 'open_to_close_hour', 'service_type', 'product_type', 'sla']\n",
      "brand\n",
      "Serving function input: brand\n"
     ]
    }
   ],
   "source": [
    "listServingInput= list(\n",
    "    loaded.signatures[\"serving_default\"].structured_input_signature[1].keys())\n",
    "print(listServingInput)\n",
    "serving_input = listServingInput[0]\n",
    "print(serving_input)\n",
    "\n",
    "print(\"Serving function input:\", serving_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "533ef415-e716-4271-9382-89be25fdd393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dense_1']\n",
      "Serving function output: dense_1\n"
     ]
    }
   ],
   "source": [
    "listServingOutput=list(loaded.signatures[\"serving_default\"].structured_outputs.keys())\n",
    "print(listServingOutput)\n",
    "serving_output =listServingOutput[0]\n",
    "print(\"Serving function output:\", serving_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50602507-3d25-4791-95dd-4885e4f2eadb",
   "metadata": {},
   "source": [
    "# Explanation Specification\n",
    "To get explanations when doing a prediction, you must enable the explanation capability and set corresponding settings when you upload your custom model to an Vertex Model resource. These settings are referred to as the explanation metadata, which consists of:\n",
    "\n",
    "* parameters: This is the specification for the explainability algorithm to use for explanations on your model. You can choose between:\n",
    "  * Shapley - Note, not recommended for image data -- can be very long running\n",
    "  * XRAI\n",
    "  * Integrated Gradients\n",
    "* metadata: This is the specification for how the algoithm is applied on your custom model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb322a53-98ee-4c8a-a87e-c4c7961e3eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "XAI = \"shapley\"  # [ shapley, ig, xrai ]\n",
    "path_count=35 # 70% of 50 [1-50]\n",
    "if XAI == \"shapley\":\n",
    "    PARAMETERS = {\"sampled_shapley_attribution\": {\"path_count\": path_count}}\n",
    "elif XAI == \"ig\":\n",
    "    PARAMETERS = {\"integrated_gradients_attribution\": {\"step_count\": path_count}}\n",
    "elif XAI == \"xrai\":\n",
    "    PARAMETERS = {\"xrai_attribution\": {\"step_count\": path_count}}\n",
    "\n",
    "parameters = aip.explain.ExplanationParameters(PARAMETERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3d22cdd-a379-41be-9aec-8244397f5702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs {\n",
      "  key: \"brand\"\n",
      "  value {\n",
      "    input_tensor_name: \"brand\"\n",
      "    modality: \"categorical\"\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  key: \"incident_type\"\n",
      "  value {\n",
      "    input_tensor_name: \"incident_type\"\n",
      "    modality: \"categorical\"\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  key: \"open_to_close_hour\"\n",
      "  value {\n",
      "    input_tensor_name: \"open_to_close_hour\"\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  key: \"product_type\"\n",
      "  value {\n",
      "    input_tensor_name: \"product_type\"\n",
      "    modality: \"categorical\"\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  key: \"response_to_resolved_hour\"\n",
      "  value {\n",
      "    input_tensor_name: \"response_to_resolved_hour\"\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  key: \"service_type\"\n",
      "  value {\n",
      "    input_tensor_name: \"service_type\"\n",
      "    modality: \"categorical\"\n",
      "  }\n",
      "}\n",
      "inputs {\n",
      "  key: \"sla\"\n",
      "  value {\n",
      "    input_tensor_name: \"sla\"\n",
      "    modality: \"categorical\"\n",
      "  }\n",
      "}\n",
      "outputs {\n",
      "  key: \"dense_1\"\n",
      "  value {\n",
      "    output_tensor_name: \"dense_1\"\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from google.cloud.aiplatform.explain.metadata.tf.v2 import saved_model_metadata_builder\n",
    "builder = saved_model_metadata_builder.SavedModelMetadataBuilder(model_dir)\n",
    "metadata = builder.get_metadata_protobuf()\n",
    "print(metadata)\n",
    "\n",
    "# import explainable_ai_sdk\n",
    "# from explainable_ai_sdk.metadata.tf.v2 import SavedModelMetadataBuilder\n",
    "\n",
    "# metadata_and_model_builder = SavedModelMetadataBuilder(model_dir)\n",
    "# metadata_and_model_builder.save_model_with_metadata(model_with_meta_dir)\n",
    "\n",
    "# https://cloud.google.com/vertex-ai/docs/explainable-ai/getting-explanations#local-explanations\n",
    "# error\n",
    "# # Load the model and adjust the configuration for Explainable AI parameters\n",
    "# # https://cloud.google.com/vertex-ai/docs/reference/rest/v1/ExplanationSpec#sampledshapleyattribution\n",
    "# num_paths = 25\n",
    "# model_artifact_with_metadata = explainable_ai_sdk.load_model_from_local_path(\n",
    "#     model_with_meta_dir,explainable_ai_sdk.SampledShapleyConfig(num_paths))\n",
    "\n",
    "# instances = [sample ]\n",
    "# explanations = model_artifact_with_metadata.explain(instances)\n",
    "# explanations[0].visualize_attributions()\n",
    "\n",
    "#AttributeError: module 'explainable_ai_sdk' has no attribute 'SampledShapleyConfig'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08dfe4d3-0750-474e-b9e3-6ca67ab378ec",
   "metadata": {},
   "source": [
    "# Upload model to model registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f58bea48-7deb-4f0f-a9d1-26d006a10881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Create Model backing LRO: projects/780371717407/locations/asia-southeast1/models/2354195132556771328/operations/6596274819749117952\n",
      "Model created. Resource name: projects/780371717407/locations/asia-southeast1/models/2354195132556771328@1\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/780371717407/locations/asia-southeast1/models/2354195132556771328@1')\n"
     ]
    }
   ],
   "source": [
    "# # https://cloud.google.com/ai-platform-unified/docs/predictions/pre-built-containers\n",
    "# #https://cloud.google.com/vertex-ai/docs/samples/aiplatform-upload-model-sample\n",
    "image_uri=\"asia-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-12:latest\"\n",
    "model = aip.Model.upload(\n",
    "    display_name=\"model-incident-tf-explainable\",\n",
    "    artifact_uri=model_dir,\n",
    "    serving_container_image_uri=image_uri,\n",
    "    explanation_parameters=parameters,\n",
    "    explanation_metadata=metadata,\n",
    "    location=region,\n",
    "    sync=False,\n",
    ")\n",
    "\n",
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614f4e95-861a-4554-9ef2-9e447d591522",
   "metadata": {},
   "source": [
    "# Deploy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4ede48c-15b5-47b4-82b5-de57e63df592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/780371717407/locations/asia-southeast1/endpoints/2469916532356939776/operations/7544282541310607360\n",
      "Endpoint created. Resource name: projects/780371717407/locations/asia-southeast1/endpoints/2469916532356939776\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/780371717407/locations/asia-southeast1/endpoints/2469916532356939776')\n",
      "Deploying model to Endpoint : projects/780371717407/locations/asia-southeast1/endpoints/2469916532356939776\n",
      "Deploy Endpoint model backing LRO: projects/780371717407/locations/asia-southeast1/endpoints/2469916532356939776/operations/610990914973728768\n",
      "Endpoint model deployed. Resource name: projects/780371717407/locations/asia-southeast1/endpoints/2469916532356939776\n"
     ]
    }
   ],
   "source": [
    "DEPLOYED_NAME = \"endpoint-incident-tf-explainable\"\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "MACHINE_TYPE = \"n1-standard\"\n",
    "VCP = \"2\"\n",
    "DEPLOY_COMPUTE = MACHINE_TYPE + \"-\" + VCP\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=DEPLOYED_NAME,\n",
    "    traffic_split=TRAFFIC_SPLIT,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    accelerator_count=0,\n",
    "    min_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a88a5a-9115-4473-9483-62f34cc1be59",
   "metadata": {},
   "source": [
    "# Load model from endpoint to make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cc847538-0755-47ec-979c-d45788ddda21",
   "metadata": {},
   "outputs": [],
   "source": [
    "instances=[\n",
    " {\"sla\": [\"24x7 6Hrs Resolution Time\"], \"product_type\": [\"Storage\"], \"brand\": [\"NetApp\"], \"service_type\": [\"Incident\"], \"incident_type\": [\"General Incident\"], \"open_to_close_hour\": [1268.9333333333334], \"response_to_resolved_hour\": [1268.8]},\n",
    " {\"sla\": [\"24x7 4Hrs Resolution Time\"], \"product_type\": [\"Software\"], \"brand\": [\"Veeam\"], \"service_type\": [\"Incident\"], \"incident_type\": [\"Software\"], \"open_to_close_hour\": [16.766666666666666], \"response_to_resolved_hour\": [16.666666666666668]},\n",
    "# {\"sla\": [\"24x7 4Hrs Resolution Time\"], \"product_type\": [\"Server\"], \"brand\": [\"HPE\"], \"service_type\": [\"Incident\"], \"incident_type\": [\"General Incident\"], \"open_to_close_hour\": [1.9], \"response_to_resolved_hour\": [1.8166666666666667]} \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "21a72c85-eb1a-41ff-afb1-4ab2d79ae738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1843916184152440832'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aiplatform.init(project=project_id, location=region)\n",
    "endpoint_id=\"1843916184152440832\"\n",
    "endpoint = aiplatform.Endpoint(endpoint_name=f\"projects/{project_id}/locations/{region}/endpoints/{endpoint_id}\")\n",
    "endpoint.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f435143a-b6e9-4595-8f01-49ac0983e674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0616235808, 0.631787777, 0.298960447, 0.00762823876]\n",
      "[0.00353190163, 0.0667497888, 0.922751129, 0.00696708122]\n"
     ]
    }
   ],
   "source": [
    "response = endpoint.predict(instances=instances)\n",
    "for prediction_ in response.predictions:\n",
    "        print(prediction_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f8c2f4-8999-4837-b504-da94af47907a",
   "metadata": {},
   "source": [
    "# Get explanations\n",
    "* https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-online-predictions#interpret_explanation_results\n",
    "* https://cloud.google.com/vertex-ai/docs/tabular-data/classification-explanations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a330932d-78a0-4151-b677-2a7d9b511721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = endpoint.explain(instances)\n",
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c5e7aa27-80a1-4983-a688-b51ee102cb7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explanation\n",
      "Attribution\n",
      " baseline_output_value: 0.28924238681793213\n",
      " instance_output_value: 0.6317877173423767\n",
      " approximation_error: 0.0007842117239539321\n",
      " feature list:\n",
      "  open_to_close_hour :[0.1434287837147713]\n",
      "  response_to_resolved_hour :[-0.1710035300254822]\n",
      "  sla :[0.06976893305778503]\n",
      "  product_type :[0.07127421796321869]\n",
      "  brand :[0.2291121137142181]\n",
      "  service_type :[-0.04315219312906265]\n",
      "  incident_type :[0.04311700612306595]\n",
      "  The sum of all of the feature importance values(instance-baseline) = 0.34254533141851423\n",
      " output_index: 1\n",
      "================================================================\n",
      "Explanation\n",
      "Attribution\n",
      " baseline_output_value: 0.45522618293762207\n",
      " instance_output_value: 0.9227511882781982\n",
      " approximation_error: 0.0006550258838518107\n",
      " feature list:\n",
      "  open_to_close_hour :[-0.001835397481918335]\n",
      "  response_to_resolved_hour :[0.0003963494300842285]\n",
      "  sla :[0.05224275827407837]\n",
      "  product_type :[0.004122594594955445]\n",
      "  brand :[0.1235325539112091]\n",
      "  service_type :[0.1742244112491608]\n",
      "  incident_type :[0.1148417353630066]\n",
      "  The sum of all of the feature importance values(instance-baseline) = 0.46752500534057617\n",
      " output_index: 2\n",
      "================================================================\n",
      "[0.0616235808, 0.631787777, 0.298960447, 0.00762823876]\n",
      "[0.00353190163, 0.0667497888, 0.922751129, 0.00696708122]\n"
     ]
    }
   ],
   "source": [
    "def explain_model( instanceList):\n",
    "\n",
    "    response = endpoint.explain(instances=instanceList, parameters={})\n",
    "\n",
    "    for explanation in response.explanations:\n",
    "        print(\"Explanation\")\n",
    "        # Feature attributions.\n",
    "        attributions = explanation.attributions\n",
    "        for attribution in attributions:\n",
    "            print(\"Attribution\")\n",
    "            print(\" baseline_output_value:\", attribution.baseline_output_value)\n",
    "            print(\" instance_output_value:\", attribution.instance_output_value)\n",
    "            print(\" approximation_error:\", attribution.approximation_error)\n",
    "            print(\" feature list:\")\n",
    "            sum_feat=0;\n",
    "            for name in input_name:\n",
    "              feat_value= attribution.feature_attributions[name]\n",
    "              sum_feat=sum_feat+feat_value[0]\n",
    "              print(f\"  {name} :{feat_value}\")\n",
    "            print(f\"  The sum of all of the feature importance values(instance-baseline) = {sum_feat}\")\n",
    "            \n",
    "            # print(\" output_display_name:\", attribution.output_display_name)\n",
    "            # print(\"  output_name:\", attribution.output_name)\n",
    "            output_index = attribution.output_index\n",
    "            for output_index in output_index:\n",
    "                print(\" output_index:\", output_index)\n",
    "            print(\"================================================================\")\n",
    "    \n",
    "\n",
    "    for prediction in response.predictions:\n",
    "        print(prediction)\n",
    "explain_model(instances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "851fae0c-b3b7-4b77-8e4e-f13f2d50015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response = endpoint.explain(instances=instances, parameters={})\n",
    "# for explanation in response.explanations:\n",
    "#  print(explanation.attributions)\n",
    "#  print(\"==========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1794b3-f667-4063-97dd-5b77cd437f7b",
   "metadata": {},
   "source": [
    "# Examine feature attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c14ed58e-a89f-4ebd-90c3-e98597fe8da7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline: 0.28924238681793213\n",
      "Sanity Check 1: Passed\n",
      "1  out of  1  sanity checks passed.\n",
      "baseline: 0.45522618293762207\n",
      "Sanity Check 1: Passed\n",
      "1  out of  1  sanity checks passed.\n",
      "baseline: 0.45522618293762207\n",
      "Sanity Check 1: Passed\n",
      "1  out of  1  sanity checks passed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sanity_check_explanations(\n",
    "    explanation, prediction, mean_tgt_value=None, variance_tgt_value=None\n",
    "):\n",
    "    passed_test = 0\n",
    "    total_test = 1\n",
    "    # `attributions` is a dict where keys are the feature names\n",
    "    # and values are the feature attributions for each feature\n",
    "    baseline_score = explanation.attributions[0].baseline_output_value\n",
    "    print(\"baseline:\", baseline_score)\n",
    "\n",
    "    # Sanity check 1\n",
    "    # The prediction at the input is equal to that at the baseline.\n",
    "    #  Please use a different baseline. Some suggestions are: random input, training\n",
    "    #  set mean.\n",
    "    if abs(prediction - baseline_score) <= 0.05:\n",
    "        print(\"Warning: example score and baseline score are too close.\")\n",
    "        print(\"You might not get attributions.\")\n",
    "    else:\n",
    "        passed_test += 1\n",
    "        print(\"Sanity Check 1: Passed\")\n",
    "\n",
    "    print(passed_test, \" out of \", total_test, \" sanity checks passed.\")\n",
    "\n",
    "\n",
    "i = 0\n",
    "for explanation in response.explanations:\n",
    "    try:\n",
    "        prediction = np.max(response.predictions[i][\"scores\"])\n",
    "    except TypeError:\n",
    "        prediction = np.max(response.predictions[i])\n",
    "    sanity_check_explanations(explanation, prediction)\n",
    "    i += 1\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08276282-2b28-4dea-958b-d0f75ef1469a",
   "metadata": {},
   "source": [
    "# Clean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "98f15b8f-582a-45d2-b4d8-33d227c19b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undeploying Endpoint model: projects/780371717407/locations/asia-southeast1/endpoints/1843916184152440832\n",
      "Undeploy Endpoint model backing LRO: projects/780371717407/locations/asia-southeast1/endpoints/1843916184152440832/operations/6508454627015393280\n",
      "Endpoint model undeployed. Resource name: projects/780371717407/locations/asia-southeast1/endpoints/1843916184152440832\n",
      "Deleting Endpoint : projects/780371717407/locations/asia-southeast1/endpoints/1843916184152440832\n",
      "Delete Endpoint  backing LRO: projects/780371717407/locations/asia-southeast1/operations/7895563312245506048\n",
      "Endpoint deleted. . Resource name: projects/780371717407/locations/asia-southeast1/endpoints/1843916184152440832\n"
     ]
    }
   ],
   "source": [
    "endpoint.undeploy_all()\n",
    "delete_bucket = False\n",
    "\n",
    "endpoint.delete()\n",
    "# model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5587682-2265-4eff-985f-ca1f7852ddf4",
   "metadata": {},
   "source": [
    "# Copy Model From Local To GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "eaabd3ac-47ad-43ae-b6e5-fc16fecec238",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://.\\model\\fingerprint.pb [Content-Type=application/octet-stream]...\n",
      "Copying file://.\\model\\saved_model.pb [Content-Type=application/octet-stream]...\n",
      "/ [0/5 files][    0.0 B/523.0 KiB]   0% Done                                    \n",
      "Copying file://.\\model\\keras_metadata.pb [Content-Type=application/octet-stream]...\n",
      "/ [0/5 files][    0.0 B/523.0 KiB]   0% Done                                    \n",
      "Copying file://.\\model\\variables\\variables.index [Content-Type=application/octet-stream]...\n",
      "/ [0/5 files][    0.0 B/523.0 KiB]   0% Done                                    \n",
      "Copying file://.\\model\\variables\\variables.data-00000-of-00001 [Content-Type=application/octet-stream]...\n",
      "/ [0/5 files][    0.0 B/523.0 KiB]   0% Done                                    \n",
      "/ [0/5 files][    0.0 B/523.0 KiB]   0% Done                                    \n",
      "/ [1/5 files][ 38.9 KiB/523.0 KiB]   7% Done                                    \n",
      "/ [2/5 files][523.0 KiB/523.0 KiB]  99% Done                                    \n",
      "/ [3/5 files][523.0 KiB/523.0 KiB]  99% Done                                    \n",
      "/ [4/5 files][523.0 KiB/523.0 KiB]  99% Done                                    \n",
      "/ [5/5 files][523.0 KiB/523.0 KiB] 100% Done                                    \n",
      "\n",
      "Operation completed over 5 objects/523.0 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "# # #https://codelabs.developers.google.com/codelabs/fraud-detection-ai-explanations?hl=en#6\n",
    "# # press_y3=input(f\"Press y=True to save model to Google Cloud storage : \") \n",
    "# # if press_y3.lower()=='y':\n",
    "# MODEL_BUCKET = 'gs://tf1-incident-smart-ml-yip'\n",
    "\n",
    "# # # # !gsutil mb -l $REGION $MODEL_BUCKET\n",
    "# # !gsutil -m cp -r ./$model_dir/* $MODEL_BUCKET/demo_model\n",
    "# !gsutil -m cp -r ./$model_dir/* $MODEL_BUCKET/model\n",
    "# #!gsutil -m cp -r ./$explain_meta_model_dir/* $MODEL_BUCKET/demo_model_explain_meta\n",
    "# # else:\n",
    "# #  quite()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6465851a-f078-41a2-991c-80a71d3e695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0f6c5d-3ea4-4fae-ba78-b8decf134a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff433f46-0e98-48ec-8b32-25b840541253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-12.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-12:m109"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
