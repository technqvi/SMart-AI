{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd54814-e227-4302-9a81-3d6ddcf0e9a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "\n",
    "# processing laryer\n",
    "#https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive2/introduction_to_tensorflow/solutions/preprocessing_layers.ipynb\n",
    "#http://localhost:8888/lab/tree/MyQuantFinProject/SMart-AI/_sample_tf_google/tfcolumn_basic_feat_eng.ipynb\n",
    "\n",
    "# others\n",
    "#https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/custom/custom-tabular-bq-managed-dataset.ipynb\n",
    "#https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive2/introduction_to_tensorflow/solutions/int_logistic_regression.ipynb\n",
    "#http://localhost:8888/lab/tree/MyQuantFinProject/SMart-AI/DemoDataTransform.ipynb\n",
    "\n",
    "# tensorboard get start and all tutorial\n",
    "#https://www.tensorflow.org/tensorboard/get_started\n",
    "# https://www.tensorflow.org/tensorboard/scalars_and_keras\n",
    "#https://keras.io/api/callbacks/tensorboard/ \n",
    "# https://medium.com/mpercept-academy/working-with-tensorboard-on-keras-callbacks-b8d680d451e7\n",
    "\n",
    "#https://pair-code.github.io/lit/\n",
    "\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense,DenseFeatures\n",
    "\n",
    "from datetime import date, timedelta, datetime # Date Functions\n",
    "import time\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "\n",
    "# from google.cloud import aiplatform as vertex_ai\n",
    "from tensorflow.python.keras.utils import data_utils\n",
    "\n",
    "print(tf.__version__)\n",
    "print(aip.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a2aab1-6f61-4737-8da7-a857ba866198",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "#%load_ext tensorboard\n",
    "%reload_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f37a96-65e0-44be-ac95-ecfdc58b6292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cateCols=['sla','product_type','brand','service_type','incident_type']\n",
    "numbericCols=['open_to_close_hour','response_to_resolved_hour']\n",
    "unusedCols=['severity_id','severity_name','label_binary_severity']\n",
    "\n",
    "# cateCols=['sla','product_type','service_type','incident_type']\n",
    "# numbericCols=['open_to_close_hour']\n",
    "# unusedCols=['severity_id','severity_name','label_binary_severity','brand','response_to_resolved_hour']\n",
    "\n",
    "labelCol='label_multi_severity'\n",
    "\n",
    "model_dir='model_v2' # production\n",
    "# model_dir='gs://demo2-tf-incident-pongthorn/demo_model_tf' # demo\n",
    "\n",
    "main_metric='accuracy'\n",
    "main_objective=f'val_{main_metric}'\n",
    "\n",
    "n_epochs= 200 #100\n",
    "n_batch_size=32\n",
    "nEarlyPatience= int(n_epochs/10)+1  # 10\n",
    "\n",
    "tsb_path=\"tsb_logs/fit/\"\n",
    "\n",
    "# df['label_multi_severity'] =df['severity_name'].map({'Cosmatic':0,'Minor': 1, \"Major\": 2, \"Critical\": 3}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89051a07-8d7f-476c-ab6b-1d2216074122",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data from BigQuery\n",
    "#projectId='smart-data-ml'\n",
    "# credentials = service_account.Credentials.from_service_account_file(r'C:\\Windows\\smart-data-ml-91b6f6204773.json')\n",
    "# client = bigquery.Client(credentials=credentials, project=projectId)\n",
    "\n",
    "projectId='pongthorn'\n",
    "client = bigquery.Client(project=projectId)\n",
    "dataset_id='SMartML'\n",
    "\n",
    "train_name='train_incident'\n",
    "validation_name='validation_incident'\n",
    "test_name='test_incident'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0aa901-563f-4d2c-9203-67175431c32f",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09b03b7-3e9d-4f25-b8c8-5d8cd615afce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_ml_data(data_path):\n",
    " df=pd.read_csv(data_path)\n",
    " df =df.drop(columns=unusedCols)\n",
    " \n",
    " return df\n",
    "\n",
    "def load_data_bq(sql:str):\n",
    " \n",
    " query_result=client.query(sql)\n",
    " df=query_result.to_dataframe()\n",
    " df =df.drop(columns=unusedCols)\n",
    "  \n",
    " return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddea3e1-b732-48c4-95fc-637108802a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_table_id=f\"{projectId}.{dataset_id}.{train_name}\"\n",
    "val_tabel_id=f\"{projectId}.{dataset_id}.{validation_name}\"\n",
    "test_tabel_id=f\"{projectId}.{dataset_id}.{test_name}\"\n",
    "\n",
    "train=load_data_bq(f\"SELECT * FROM {train_table_id}\")\n",
    "val=load_data_bq(f\"SELECT * FROM {val_tabel_id}\")\n",
    "test=load_data_bq(f\"SELECT * FROM {test_tabel_id}\")\n",
    "\n",
    "# Load data from CSV File\n",
    "# root_path='../../data'    \n",
    "# train = load_ml_data(f\"{root_path}/{train_name}.csv\")\n",
    "# val=load_ml_data(f\"{root_path}/{validation_name}.csv\")\n",
    "# test =load_ml_data(f\"{root_path}/{test_name}.csv\")\n",
    "\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcdca62-5198-4d5d-8704-b42fa468a47b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "labelList=list(train[labelCol].unique())\n",
    "print(\"List All Label:\" ,sorted(labelList))\n",
    "nLabel=len(labelList)\n",
    "print(f\"No target label : {nLabel}\")\n",
    "\n",
    "print(train.info())\n",
    "train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64adbe5e-d737-45a4-a04e-340a2926ad2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def CalPctEachTargetClass(dfx,colSev,colPctSev):\n",
    "    dfClassSummary=dfx.groupby([labelCol]).size().to_frame(colSev)\n",
    "    dfClassSummary[colPctSev]= dfClassSummary[colSev]/dfClassSummary[colSev].sum() *100\n",
    "    dfClassSummary=dfClassSummary.round(0)\n",
    "    return dfClassSummary\n",
    "\n",
    "pctDF1=CalPctEachTargetClass(train,'Train-No-Severity','Train-%-Severity')\n",
    "pctDF2=CalPctEachTargetClass(val,'Val-No-Severity','Val-%-Severity')\n",
    "pdcDF3=CalPctEachTargetClass(test,'Test-No-Severity','Test-%-Severity')\n",
    "pctDF=pd.concat([pctDF1,pctDF2,pdcDF3],axis=1)\n",
    "\n",
    "pctDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e9916-6668-4c4e-8974-af63c3f840b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_class(df):\n",
    "    fig , ax = plt.subplots(figsize=(8,5))\n",
    "    ax =sns.countplot(x=labelCol, data=df,)\n",
    "    for p in ax.patches:\n",
    "       ax.annotate('{:.1f}'.format(p.get_height()), (p.get_x()+0.25, p.get_height()+0.01))\n",
    "    plt.title(labelCol.title())\n",
    "    plt.show()\n",
    "    \n",
    "plot_class(train)\n",
    "plot_class(val)\n",
    "plot_class(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa30d3fd-14e9-4c3d-a6aa-45d8114f5362",
   "metadata": {},
   "source": [
    "# Process Data  Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d3a943-09d5-4a43-b8d8-633bc4a90353",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def multiple_label_df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  features = dataframe.copy()\n",
    "  labels = features.pop(labelCol)\n",
    "  labels  = tf.keras.utils.to_categorical(labels, num_classes=nLabel)\n",
    "    \n",
    "  ds = tf.data.Dataset.from_tensor_slices(( dict(features), labels ))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(features))\n",
    "  ds = ds.batch(batch_size)\n",
    "  ds = ds.prefetch(batch_size)\n",
    "  return ds\n",
    "     \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274122c2-9481-438b-b637-6e6ad393d809",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Explore Data from Tensor before keras data processing\")\n",
    "batch_size = 1\n",
    "train_ds =multiple_label_df_to_dataset(train, batch_size=batch_size)\n",
    "\n",
    "[(train_features, label_batch)] = train_ds.take(1)\n",
    "print(\"========Features==========\")\n",
    "print('Every feature:', list(train_features.keys()))\n",
    "print('product_type:', train_features['product_type'])  # sample cate feature\n",
    "print('open_to_close_hour:', train_features['open_to_close_hour'])  # sample numberic feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7d4e1c-dbd6-42b4-8958-6aad45777bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"========Labels==========\")\n",
    "print(f'{label_batch }')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd22984c-e079-4b61-b94b-57acc540a487",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ad8837-968e-4d68-bdb6-1c7eba97e69c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_normalization_layer(name, dataset):\n",
    "  # Create a Normalization layer for our feature.\n",
    "  normalizer = preprocessing.Normalization(axis=None)\n",
    "\n",
    "  # Prepare a Dataset that only yields our feature.\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the statistics of the data.\n",
    "  normalizer.adapt(feature_ds)\n",
    "\n",
    "  return normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237bce3c-9c77-4318-8682-fc36af0a6804",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_category_encoding_layer(name, dataset, dtype, max_tokens=None):\n",
    "  # Create a StringLookup layer which will turn strings into integer indices\n",
    "  if dtype == 'string':\n",
    "    index = preprocessing.StringLookup(max_tokens=max_tokens)\n",
    "  else:\n",
    "    index = preprocessing.IntegerLookup(max_tokens=max_tokens)\n",
    "\n",
    "  # Prepare a Dataset that only yields our feature\n",
    "  feature_ds = dataset.map(lambda x, y: x[name])\n",
    "\n",
    "  # Learn the set of possible values and assign them a fixed integer index.\n",
    "  index.adapt(feature_ds)\n",
    "\n",
    "  # Create a Discretization for our integer indices.\n",
    "  encoder = preprocessing.CategoryEncoding(num_tokens=index.vocabulary_size())\n",
    "\n",
    "  # Apply one-hot encoding to our indices. The lambda function captures the\n",
    "  # layer so we can use them, or include them in the functional model later.\n",
    "  return lambda feature: encoder(index(feature))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab6c0ac-ee9b-42f3-b140-a102e8f2f121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hour_col = train_features['open_to_close_hour']\n",
    "layer = get_normalization_layer('open_to_close_hour', train_ds)\n",
    "layer(hour_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c0de50-a1fb-44b9-b91f-555ac779ec40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# total no type+unkonw\n",
    "print(train['product_type'].unique())\n",
    "type_col = train_features['product_type']\n",
    "layer = get_category_encoding_layer('product_type', train_ds, 'string')\n",
    "layer(type_col)\n",
    "\n",
    "# onehot-encoding = 10 category+unknown=11 columnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c78f01-000a-4d3e-9fbf-6f4660cfb75b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size =32\n",
    "train_ds = multiple_label_df_to_dataset (train, batch_size=batch_size)\n",
    "val_ds = multiple_label_df_to_dataset(val, batch_size=batch_size)\n",
    "test_ds = multiple_label_df_to_dataset(test, batch_size=batch_size)\n",
    "# for element in train_ds.as_numpy_iterator():\n",
    "#     print(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5430f2c-4130-4b3a-b0dd-095ec81926ba",
   "metadata": {},
   "source": [
    "# Data Transformation by Keras PreProcessing Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a69b6-d9c1-49d9-bc81-ed421d8700cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_inputs = []\n",
    "encoded_features = []\n",
    "numInputFeatToInitNodeUnit=0\n",
    "\n",
    "# Numeric features.\n",
    "for header in numbericCols:\n",
    "  stat_data=train[header].describe()\n",
    "  print(f\"header: Mean={stat_data['mean']} and Std={stat_data['std']}\")  \n",
    "  numeric_col = tf.keras.Input(shape=(1,), name=header)\n",
    "  normalization_layer = get_normalization_layer(header, train_ds)\n",
    "  encoded_numeric_col = normalization_layer(numeric_col)\n",
    "  all_inputs.append(numeric_col)\n",
    "  encoded_features.append(encoded_numeric_col)\n",
    "    \n",
    "numInputFeatToInitNodeUnit=numInputFeatToInitNodeUnit+len(numbericCols)\n",
    "    \n",
    "    # Categorical features encoded as string.\n",
    "categorical_cols = cateCols\n",
    "for header in categorical_cols:\n",
    "  listCateItem=train[header].unique()\n",
    "  noCateItem=len(listCateItem)\n",
    "  numInputFeatToInitNodeUnit=numInputFeatToInitNodeUnit+noCateItem +1  # last 1 is unknow  \n",
    "  print(f\"{header} = {noCateItem} : {listCateItem}\")\n",
    "\n",
    "  print(header)  \n",
    "  categorical_col = tf.keras.Input(shape=(1,), name=header, dtype='string')\n",
    "  encoding_layer = get_category_encoding_layer(header, train_ds, dtype='string')\n",
    "                                        \n",
    "  encoded_categorical_col = encoding_layer(categorical_col)\n",
    "  all_inputs.append(categorical_col)\n",
    "  encoded_features.append(encoded_categorical_col)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f85f8a-4471-44e1-a389-102232c296e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Total column to initialize first node input: {numInputFeatToInitNodeUnit}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa733595-628e-4a07-a6b8-719995fa535b",
   "metadata": {},
   "source": [
    "# Build and Train and Eveluate and Plot Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee6ca53e-78d8-400e-a48e-cdcb93d2030f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_metrics(history,metric):\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.title(metric)\n",
    "    plt.plot(history.history[metric], label='train')\n",
    "    plt.plot(history.history[f'val_{metric}'], label='validation')\n",
    "    plt.legend()\n",
    "    plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a23e758-73c0-4343-86f4-cf04daac639d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# logdir = f\"{tsb_path}/scalars\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "# file_writer = tf.summary.create_file_writer(logdir + \"/metrics\")\n",
    "# file_writer.set_as_default()\n",
    "\n",
    "\n",
    "# def lr_schedule(epoch):\n",
    "#   \"\"\"\n",
    "#   Returns a custom learning rate that decreases as epochs progress.\n",
    "#   \"\"\"\n",
    "#   learning_rate = 0.01 # default\n",
    "#   if epoch >= 50:\n",
    "#     learning_rate = 0.001   # default\n",
    "#   if epoch >= 100:\n",
    "#     learning_rate = 0.0001\n",
    "#   if epoch > 150:\n",
    "#     learning_rate = 0.00001\n",
    "\n",
    "#   tf.summary.scalar('learning rate', data=learning_rate, step=epoch)\n",
    "#   return learning_rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e27d3b-9b17-4311-a7a2-d4c4007430b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# def build_model(num_units_1,num_units_layer2, dropout_rate):\n",
    "def build_model(num_units_layer1, dropout_rate,learningRate,no_addtional_hidden_layer=1):\n",
    "    \n",
    "    all_features = tf.keras.layers.concatenate(encoded_features)\n",
    "   \n",
    "    x = tf.keras.layers.Dense(num_units_layer1, activation=\"relu\")(all_features)# layer1\n",
    "    \n",
    "    if dropout_rate>0:\n",
    "     print(f\"Dropout: { dropout_rate}\")\n",
    "     x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
    "    \n",
    "    # if no_addtional_hidden_layer>1:\n",
    "    # for x  in range(0,1):  \n",
    "    # x = tf.keras.layers.Dense(num_units_layer1, activation=\"relu\")(x)  # layer2\n",
    "\n",
    "\n",
    "    output = tf.keras.layers.Dense(nLabel,activation=tf.nn.softmax)(x)\n",
    "\n",
    "    model = tf.keras.Model(all_inputs, output)\n",
    "    \n",
    "    if  learningRate==0:\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss=tf.keras.losses.CategoricalCrossentropy(),metrics=[main_metric])\n",
    "    else:\n",
    "        print(f\"LR: {learningRate}\")\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learningRate),\n",
    "                      loss=tf.keras.losses.CategoricalCrossentropy(),metrics=[main_metric])\n",
    "    return model\n",
    "\n",
    "def train_model(model,x_epochs,x_batch_size):  \n",
    "    # model.summary()\n",
    "\n",
    "    \n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=nEarlyPatience, verbose=1)  \n",
    "    # history =model.fit(train_ds, validation_data=val_ds,epochs=x_epochs,batch_size=x_batch_size,callbacks=[early_stopping])\n",
    "    \n",
    "    log_dir = f\"{tsb_path}\" + datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "    \n",
    "    history =model.fit(train_ds, validation_data=val_ds,epochs=x_epochs,batch_size=x_batch_size,\n",
    "                       callbacks=[early_stopping,tensorboard_callback])\n",
    "    \n",
    "     # lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "    # history =model.fit(train_ds, validation_data=val_ds,epochs=x_epochs,batch_size=x_batch_size,\n",
    "    #                    callbacks=[early_stopping,tensorboard_callback, lr_callback ])\n",
    "    return history\n",
    "\n",
    "\n",
    "# def build_wide_deep_model()\n",
    "# METRICS = [\n",
    "#       keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "#       keras.metrics.Precision(name='precision'),\n",
    "#       keras.metrics.Recall(name='recall'),\n",
    "#       keras.metrics.AUC(name='auc'),\n",
    "# ]\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3816a15-6688-4c87-9d20-e5fc45d48d11",
   "metadata": {},
   "source": [
    "# Test Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25688a22-05f5-4cac-b662-d261b5dcfb78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"1#Build model\")\n",
    "\n",
    "# Smaller RL , longer , By doing this, you learn gradually rather than jumping around \n",
    "#A learning rate of 0.001 is the default (Adam)\n",
    "\n",
    "#model=build_model(32,0.1,0.0001)\n",
    "model=build_model(64,0.1,0.0001)\n",
    "#model=build_model(numInputFeatToInitNodeUnit,0,0)\n",
    "\n",
    "# print(model.summary())\n",
    "#model=build_model(128,0,0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655da7c1-a716-437c-b1c0-fde9745a1a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"2#Train model\")\n",
    "history=train_model(model,n_epochs,n_batch_size)\n",
    "print(\"=============================================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19961e63-4585-4662-98ee-25402fe7e735",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"3#Evaluate model\")\n",
    "\n",
    "loss_val, accuracy_val = model.evaluate(val_ds)\n",
    "print(f\"Average Loss - Accuracy on Eveluation {loss_val} -{accuracy_val}\")\n",
    "\n",
    "loss_test, accuracy_test = model.evaluate(test_ds)\n",
    "print(f\"Average Loss - Accuracy on Test {loss_test} -{accuracy_test}\")\n",
    "\n",
    "print(\"=============================================================================\")\n",
    "\n",
    "\n",
    "# production data set to 12MAr23\n",
    "# 7/7 [==============================] - 0s 2ms/step - loss: 0.6655 - accuracy: 0.7689\n",
    "# Average Loss - Accuracy on Eveluation 0.6654592752456665 -0.7688679099082947\n",
    "# 7/7 [==============================] - 0s 2ms/step - loss: 0.6977 - accuracy: 0.7358\n",
    "# Average Loss - Accuracy on Test 0.6977102160453796 -0.7358490824699402\n",
    "\n",
    "\n",
    "# demo set to 04 April23\n",
    "# 8/8 [==============================] - 0s 2ms/step - loss: 0.7334 - accuracy: 0.7621\n",
    "# Average Loss - Accuracy on Eveluation 0.7333917617797852 -0.7621145248413086\n",
    "# 8/8 [==============================] - 0s 3ms/step - loss: 0.7452 - accuracy: 0.6740\n",
    "# Average Loss - Accuracy on Test 0.745168149471283 -0.6740087866783142\n",
    "# ============================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8dbb57-b0df-4e35-88fe-3ad1061ee047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f43e5ca-f8a5-4f75-b768-33a296ee2fb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Log result with Tensorboad\n",
    "# !kill 18800\n",
    "# load and use jupyter notebok\n",
    "# %tensorboard --logdir tsb_logs/fit/  --host 0.0.0.0\n",
    "\n",
    "#tensorboard --logdir=ilogs/ --port=6006\n",
    "#%tensorboard --logdir tsb_logs/fit/\n",
    "# ! del /f ./logs/\n",
    "\n",
    "from tensorboard import notebook\n",
    "notebook.list() # View open TensorBoard instances\n",
    "\n",
    "notebook.display(port=6006, height=1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f3a39c-d489-4970-a709-cdf9c38805f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"4#Explore Result model\")\n",
    "plot_metrics(history,main_metric)\n",
    "plot_metrics(history,\"loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05e9e8b-804d-4c37-9865-9dfcf2343411",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8f677c-dfa2-48b5-b980-6cc10f2e0033",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86af40e-2142-4c2f-af6d-13bae8cd463e",
   "metadata": {},
   "source": [
    "# Try Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd916787-bacc-4537-b2b6-1c6d62932de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#label_multi_severity\n",
    "\n",
    "local_model= tf.keras.models.load_model(model_dir)\n",
    "\n",
    "sample={\"sla\":\"24x7 4Hrs Response Time\",\n",
    "        \"product_type\":\"Server\",\n",
    "        \"brand\":\"VMWare\",                \n",
    "        \"service_type\":\"Incident\",\n",
    "        \"incident_type\":\"General Incident\",\n",
    "        \"open_to_close_hour\":12,\n",
    "        \"response_to_resolved_hour\":8.500000 \\\n",
    "       }\n",
    "\n",
    "print(sample)\n",
    "              \n",
    "print(\"===============================================================================================================\")    \n",
    "print(\"convert pain data to serdor as input to predict\")    \n",
    "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
    "print(input_dict)\n",
    "\n",
    "predictionList = local_model.predict(input_dict)\n",
    "print(predictionList)\n",
    "\n",
    "prob = tf.nn.softmax(predictionList[0])\n",
    "print(f\"{(100 * prob)} % at {np.argmax(prob, axis=0)} as Severity\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5587682-2265-4eff-985f-ca1f7852ddf4",
   "metadata": {},
   "source": [
    "# Copy Model From Local To GCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaabd3ac-47ad-43ae-b6e5-fc16fecec238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #https://codelabs.developers.google.com/codelabs/fraud-detection-ai-explanations?hl=en#6\n",
    "# # press_y3=input(f\"Press y=True to save model to Google Cloud storage : \") \n",
    "# # if press_y3.lower()=='y':\n",
    "# MODEL_BUCKET = 'gs://tf1-incident-smart-ml-yip'\n",
    "\n",
    "# # # # !gsutil mb -l $REGION $MODEL_BUCKET\n",
    "# # !gsutil -m cp -r ./$model_dir/* $MODEL_BUCKET/demo_model\n",
    "# !gsutil -m cp -r ./$model_dir/* $MODEL_BUCKET/model\n",
    "# #!gsutil -m cp -r ./$explain_meta_model_dir/* $MODEL_BUCKET/demo_model_explain_meta\n",
    "# # else:\n",
    "# #  quite()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6465851a-f078-41a2-991c-80a71d3e695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c0f6c5d-3ea4-4fae-ba78-b8decf134a92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff433f46-0e98-48ec-8b32-25b840541253",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-12.m109",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-12:m109"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
