{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "import psycopg2.extras as extras\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime,timezone\n",
    "from dateutil import tz\n",
    "\n",
    "import os\n",
    "import sys \n",
    "\n",
    "from configupdater import ConfigUpdater\n",
    "# pip install ConfigUpdater\n",
    "\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "from google.api_core.exceptions import BadRequest\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "# [metadata]\n",
    "# pmr_pm_plan = 2019-01-01 00:00:00\n",
    "# pmr_pm_item = 2019-01-01 00:00:00\n",
    "# pmr_project = 2019-01-01 00:00:00\n",
    "# pmr_inventory  = 2019-01-01 00:00:00\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imported Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View name to load to BQ :pmr_inventory\n"
     ]
    }
   ],
   "source": [
    "is_py=False\n",
    "view_name = \"pmr_inventory\"\n",
    "isFirstLoad=False\n",
    "if is_py:\n",
    "    press_Y=''\n",
    "    ok=False\n",
    "\n",
    "    if len(sys.argv) > 1:\n",
    "        view_name=sys.argv[1]\n",
    "    else:\n",
    "        print(\"Enter the following input: \")\n",
    "        view_name = input(\"View Table Name : \")\n",
    "print(f\"View name to load to BQ :{view_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imported date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTC: 2023-12-31 08:12:24 For This Import\n"
     ]
    }
   ],
   "source": [
    "dt_imported=datetime.now(timezone.utc) # utc\n",
    "dt_imported=datetime.strptime(dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\"),\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"UTC: {dt_imported} For This Import\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set view data and log table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14  -  inventory_id  -  merge_inventory\n"
     ]
    }
   ],
   "source": [
    "log = \"models_logging_change\"\n",
    "def get_contentID_keyName(view_name):\n",
    "\n",
    "    if view_name == \"pmr_pm_plan\":\n",
    "        tableContentID = 36\n",
    "        key_name = \"pm_id\"\n",
    "        sp=\"merge_pm_plan\"\n",
    "        changed_field_mapping=['planned_date','ended_pm_date',\n",
    "                               'project_id','remark','team_lead_id']\n",
    "\n",
    "    elif view_name == \"pmr_pm_item\":\n",
    "        tableContentID = 37\n",
    "        key_name = \"pm_item_id\"\n",
    "        sp=\"merge_pm_item\"\n",
    "\n",
    "    elif view_name == \"pmr_project\":\n",
    "        tableContentID = 7\n",
    "        key_name = \"project_id\"\n",
    "        sp=\"merge_project\"\n",
    "\n",
    "    elif view_name == \"pmr_inventory\":\n",
    "        tableContentID = 14\n",
    "        key_name = \"inventory_id\"\n",
    "        sp=\"merge_inventory\"\n",
    "        hanged_field_mapping=[\n",
    "        ]\n",
    "\n",
    "    else:\n",
    "        raise Exception(\"No specified content type id\")\n",
    "        \n",
    "    return tableContentID, key_name,sp\n",
    "\n",
    "\n",
    "content_id , view_name_id,sp_name=get_contentID_keyName(view_name)\n",
    "print(content_id,\" - \",view_name_id,\" - \",sp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set data and cofig path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".env\n",
      "cfg_last_import\n"
     ]
    }
   ],
   "source": [
    "# Test config,env file and key to be used ,all of used key  are existing.\n",
    "cfg_path=\"cfg_last_import\"\n",
    "env_path='.env'\n",
    "\n",
    "updater = ConfigUpdater()\n",
    "updater.read(os.path.join(cfg_path,f\"{view_name}.cfg\"))\n",
    "\n",
    "config = dotenv_values(dotenv_path=env_path)\n",
    "\n",
    "print(env_path)\n",
    "print(cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pongthorn.SMartData_Temp.temp_inventory\n",
      "pongthorn.SMartDataAnalytics.inventory\n"
     ]
    }
   ],
   "source": [
    "# Test exsitng project dataset and table anme\n",
    "\n",
    "projectId=config['PROJECT_ID']  # smart-data-ml  or kku-intern-dataai or ponthorn\n",
    "credential_file=config['PROJECT_CREDENTIAL_FILE']\n",
    "# C:\\Windows\\smart-data-ml-91b6f6204773.json\n",
    "# C:\\Windows\\kku-intern-dataai-a5449aee8483.json\n",
    "# C:\\Windows\\pongthorn-5decdc5124f5.json\n",
    "\n",
    "\n",
    "dataset_id='SMartData_Temp'  # 'SMartData_Temp'  'PMReport_Temp'\n",
    "main_dataset_id='SMartDataAnalytics'  # ='SMartDataAnalytics'  'PMReport_Main'\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(credential_file)\n",
    "\n",
    "table_name=view_name.replace(\"pmr_\",\"temp_\") #can change in (\"name\") to temp table\n",
    "table_id = f\"{projectId}.{dataset_id}.{table_name}\"\n",
    "print(table_id)\n",
    "\n",
    "\n",
    "main_table_name=view_name.replace(\"pmr_\",\"\")\n",
    "main_table_id = f\"{projectId}.{main_dataset_id}.{main_table_name}\"\n",
    "print(main_table_id)\n",
    "\n",
    "# https://cloud.google.com/bigquery/docs/reference/rest/v2/Job\n",
    "to_bq_mode=\"WRITE_EMPTY\"\n",
    "\n",
    "\n",
    "client = bigquery.Client(credentials= credentials,project=projectId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Configuration File and Initialize BQ Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTC:2019-01-01 00:00:00  Of Last Import\n"
     ]
    }
   ],
   "source": [
    "last_imported=datetime.strptime(updater[\"metadata\"][view_name].value,\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"UTC:{last_imported}  Of Last Import\")\n",
    "\n",
    "# local_zone = tz.tzlocal()\n",
    "# last_imported = last_imported.astimezone(local_zone)\n",
    "# print(f\"Local Asia/Bangkok:{last_imported}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postgres &BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_postgres_conn():\n",
    " try:\n",
    "  conn = psycopg2.connect(\n",
    "        database=config['DATABASES_NAME'], user=config['DATABASES_USER'],\n",
    "      password=config['DATABASES_PASSWORD'], host=config['DATABASES_HOST']\n",
    "     )\n",
    "  return conn\n",
    "\n",
    " except Exception as error:\n",
    "  print(error)      \n",
    "  raise error\n",
    "def list_data(sql,params,connection):\n",
    " df=None   \n",
    " with connection.cursor() as cursor:\n",
    "    \n",
    "    if params is None:\n",
    "       cursor.execute(sql)\n",
    "    else:\n",
    "       cursor.execute(sql,params)\n",
    "    \n",
    "    columns = [col[0] for col in cursor.description]\n",
    "    dataList = [dict(zip(columns, row)) for row in cursor.fetchall()]\n",
    "    df = pd.DataFrame(data=dataList) \n",
    " return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bq_table():\n",
    " try:\n",
    "    table=client.get_table(table_id)  # Make an API request.\n",
    "    print(\"Table {} already exists.\".format(table_id))\n",
    "    print(table.schema)\n",
    "    return True\n",
    " except NotFound:\n",
    "    raise Exception(\"Table {} is not found.\".format(table_id))\n",
    "    \n",
    "\n",
    "def insertDataFrameToBQ(df_trasns):\n",
    "    try:\n",
    "        job_config = bigquery.LoadJobConfig(write_disposition=to_bq_mode,)\n",
    "        job = client.load_table_from_dataframe(df_trasns, table_id, job_config=job_config)\n",
    "        try:\n",
    "         job.result()  # Wait for the job to complete.\n",
    "        except ClientError as e:\n",
    "         print(job.errors)\n",
    "\n",
    "        print(\"Total \", len(df_trasns), f\"Imported data to {table_id} on bigquery successfully\")\n",
    "\n",
    "    except BadRequest as e:\n",
    "        print(\"Bigquery Error\\n\")\n",
    "        print(e) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check whether it is the first loading?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def checkFirstLoad():\n",
    "    print(\"If the main table is empty , so the action of each row  must be 'added' on temp table\")\n",
    "    rows_iter   = client.list_rows(main_table_id, max_results=1) \n",
    "    no_main=len(list(rows_iter))\n",
    "    if no_main==0:\n",
    "     isFirstLoad=True\n",
    "     print(f\"This is the first loaing , so there is No DATA in {main_table_id}, we load all rows from {view_name} to import into {table_id} action will be 'added' \")\n",
    "    else:\n",
    "     isFirstLoad=False   \n",
    "    return isFirstLoad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If the main table is empty , so the action of each row  must be 'added' on temp table\n",
      "This is the first loaing , so there is No DATA in pongthorn.SMartDataAnalytics.inventory, we load all rows from pmr_inventory to import into pongthorn.SMartData_Temp.temp_inventory action will be 'added' \n",
      "IsFirstLoad=True\n"
     ]
    }
   ],
   "source": [
    "isFirstLoad=checkFirstLoad()\n",
    "print(f\"IsFirstLoad={isFirstLoad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For The next Load\n",
    "* get data from model log based on condition last_imported and table\n",
    "* Get all actions from log table by selecting unique object_id and setting by doing something as logic\n",
    "* Create  id and action dataframe form filtered rows from log table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_model_log(x_last_imported,x_content_id):\n",
    "    sql_log = f\"\"\"\n",
    "    SELECT object_id, action,TO_CHAR(date_created,'YYYY-MM-DD HH24:MI:SS') as date_created ,changed_data\n",
    "    FROM {log}\n",
    "    WHERE date_created  AT time zone 'utc' >= '{x_last_imported}' AND content_type_id = {x_content_id} \n",
    "    ORDER BY object_id, date_created\n",
    "    \"\"\"\n",
    "    print(sql_log)\n",
    "\n",
    "\n",
    "    # Asia/Bangkok \n",
    "    lf = list_data(sql_log, None, get_postgres_conn())\n",
    "    print(f\"Retrieve all rows after {last_imported}\")\n",
    "    print(lf.info())\n",
    "    return lf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_any_changes_to_collumns_view(dfAction,x_view_name,_x_key_name):\n",
    "    \"\"\"\n",
    "    Check dataframe from log model that contain only changed action to select changed fields on view.\n",
    "    \"\"\"\n",
    "\n",
    "    listACtion=dfAction[\"action\"].unique().tolist()\n",
    "    if len(listACtion)==1 and listACtion[0]=='changed':\n",
    "        print(\"###########################################################\")\n",
    "        print(\"Process dataframe containing only all changed action\")\n",
    "        print(dfAction)\n",
    "        print(\"###########################################################\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def select_actual_action(lf):\n",
    "    listIDs=lf[\"object_id\"].unique().tolist()\n",
    "    listUpdateData=[]\n",
    "    for id in listIDs:\n",
    "        lfTemp=lf.query(\"object_id==@id\")\n",
    "        print(lfTemp)\n",
    "        print(\"----------------------------------------------------------------\")\n",
    "        \n",
    "        \n",
    "        # check_any_changes_to_collumns_view(lfTemp,content_id,view_name_id)\n",
    "\n",
    "\n",
    "        first_row = lfTemp.iloc[0]\n",
    "        last_row = lfTemp.iloc[-1]\n",
    "        # print(first_row)\n",
    "        # print(last_row)\n",
    "\n",
    "        if len(lfTemp)==1:\n",
    "            listUpdateData.append([id,first_row[\"action\"]])\n",
    "        else:\n",
    "            if first_row[\"action\"] == \"added\" and last_row[\"action\"] == \"deleted\":\n",
    "                continue\n",
    "            elif first_row[\"action\"] == \"added\" and last_row[\"action\"] != \"deleted\":\n",
    "                listUpdateData.append([id,\"added\"])\n",
    "            else : listUpdateData.append([id,last_row[\"action\"]])\n",
    "\n",
    "    print(\"Convert listUpdate to dataframe\")\n",
    "    dfUpdateData = pd.DataFrame(listUpdateData, columns= ['id', 'action'])\n",
    "    dfUpdateData['id'] = dfUpdateData['id'].astype('int64')\n",
    "    dfUpdateData=dfUpdateData.sort_values(by=\"id\")\n",
    "    dfUpdateData=dfUpdateData.reset_index(drop=True)\n",
    "\n",
    "    return dfUpdateData\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if isFirstLoad==False:\n",
    "    listModelLogObjectIDs=[]\n",
    "    dfModelLog=list_model_log(last_imported,content_id)\n",
    "    if dfModelLog.empty==True:\n",
    "        print(\"No row to be imported.\")\n",
    "        exit()\n",
    "    else:\n",
    "       print(\"Get row imported from model log to set action\") \n",
    "       dfModelLog=select_actual_action( dfModelLog)\n",
    "       listModelLogObjectIDs=dfModelLog['id'].tolist()\n",
    "       print(dfModelLog.info())\n",
    "       print(dfModelLog)       \n",
    "       print(listModelLogObjectIDs) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load view and transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select *  from pmr_inventory  where  updated_at AT time zone 'utc' >= '2019-01-01 00:00:00'\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16399 entries, 0 to 16398\n",
      "Data columns (total 9 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   inventory_id             16399 non-null  int64 \n",
      " 1   serial_number            16399 non-null  object\n",
      " 2   customer_warranty_start  16399 non-null  object\n",
      " 3   customer_warranty_end    16399 non-null  object\n",
      " 4   brand                    16399 non-null  object\n",
      " 5   model                    16399 non-null  object\n",
      " 6   product_type             16399 non-null  object\n",
      " 7   project_id               16399 non-null  int64 \n",
      " 8   action                   16399 non-null  object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 1.1+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def retrive_next_data_from_view(x_view,x_id,x_listModelLogObjectIDs):\n",
    "    if len(x_listModelLogObjectIDs)>1:\n",
    "     sql_view=f\"select *  from {x_view}  where {x_id} in {tuple(x_listModelLogObjectIDs)}\"\n",
    "    else:\n",
    "     sql_view=f\"select *  from {x_view}  where {x_id} ={x_listModelLogObjectIDs[0]}\"\n",
    "    \n",
    "    print(sql_view)\n",
    "    df=list_data(sql_view,None,get_postgres_conn())\n",
    "\n",
    "    if df.empty==True:\n",
    "     return df\n",
    "    df=df.drop(columns='updated_at')\n",
    "    return df \n",
    "\n",
    "\n",
    "def retrive_first_data_from_view(x_view,x_last_imported):\n",
    "     sql_view=f\"select *  from {x_view}  where  updated_at AT time zone 'utc' >= '{x_last_imported}'\"\n",
    "     print(sql_view)\n",
    "     df=list_data(sql_view,None,get_postgres_conn())\n",
    "     if df.empty==True:\n",
    "            return df\n",
    "     df=df.drop(columns='updated_at')\n",
    "     df['action']='added'\n",
    "     return df   \n",
    "def retrive_one_row_from_view_to_gen_df_schema(x_view):\n",
    "    sql_view=f\"select *  from {x_view}  limit 1\"\n",
    "    print(sql_view)\n",
    "    df=list_data(sql_view,None,get_postgres_conn())\n",
    "    df=df.drop(columns='updated_at')\n",
    "    return df\n",
    "\n",
    "\n",
    "if isFirstLoad:\n",
    " df=retrive_first_data_from_view(view_name,last_imported)\n",
    " if df.empty==True:\n",
    "    print(\"No row to be imported.\")\n",
    "    exit()\n",
    " else:\n",
    "    print(df.info())\n",
    "else:\n",
    " df=retrive_next_data_from_view(view_name,view_name_id,listModelLogObjectIDs)  \n",
    " if df.empty==True:\n",
    "    print(\"Due to having deleted items, we will Get schema from {} to create empty dataframe with schema.\")\n",
    "    df=retrive_one_row_from_view_to_gen_df_schema(view_name)\n",
    "    # this id has been included in listModelLogObjectIDs which contain deleted action , so we can use it as schema generation\n",
    "    print(df)\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transaformation\n",
    "* IF The first load then add actio='Added'\n",
    "* IF The nextload then Merge LogDF and ViewDF and add deleted row \n",
    "  * Get Deleted Items  to Create deleted dataframe by using listDeleted\n",
    "  * If there is one deletd row then  we will merge it to master dataframe\n",
    "* IF the next load has only deleted action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_acutal_action_to_df_at_next(df,dfUpdateData,x_view,x_id):\n",
    "    # merget model log(id and action) to data view\n",
    "    # if  dfUpdateData  contain only deleted action\n",
    "    # we will merge to get datafdame shcema, it can perform inner without have actual data fram view\n",
    "    merged_df = pd.merge(df, dfUpdateData, left_on=view_name_id, right_on='id', how='inner')\n",
    "    merged_df = merged_df.drop(columns=['id'])\n",
    "\n",
    "    listAllAction=dfUpdateData['id'].tolist()\n",
    "    print(f\"List {listAllAction} all action\")\n",
    "    \n",
    "    listSeleted = merged_df[view_name_id].tolist()\n",
    "    print(f\"List  {x_view}   {listSeleted} from {x_view} exluding deleted action\")\n",
    "    \n",
    "    allActionSet = set(listAllAction)\n",
    "    anotherSet = set(listSeleted)\n",
    "    \n",
    "    listDeleted = list(allActionSet.symmetric_difference(anotherSet))\n",
    "    print(f\"List deleted {listDeleted}\")\n",
    "    \n",
    "    # Test List  select by view + List deeleted = List All Action\n",
    "\n",
    "    if len(listDeleted)>0:\n",
    "        print(\"There are some deleted rows\")\n",
    "        dfDeleted=pd.DataFrame(data=listDeleted,columns=[view_name_id])\n",
    "        dfDeleted['action']='deleted'\n",
    "        print(dfDeleted)\n",
    "        merged_df=pd.concat([merged_df,dfDeleted],axis=0)\n",
    "\n",
    "    else:\n",
    "        print(\"No row deleted\")\n",
    "\n",
    "    return merged_df    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       inventory_id  serial_number customer_warranty_start  \\\n",
      "0                47   451951000035              2020-01-12   \n",
      "1                46   451915000214              2019-04-26   \n",
      "2                18   451741000112              2017-10-25   \n",
      "3                45   451915000213              2019-04-26   \n",
      "4                44   451902000028              2019-01-23   \n",
      "...             ...            ...                     ...   \n",
      "16394         19205    FDO23011B4G              2020-07-04   \n",
      "16395         19206    FDO23011AVZ              2020-07-04   \n",
      "16396         19211           np01              2023-12-01   \n",
      "16397         19213  win-server-01              2023-12-01   \n",
      "16398         19214              -              2023-12-01   \n",
      "\n",
      "      customer_warranty_end      brand                              model  \\\n",
      "0                2025-01-12     NetApp                           AFF A300   \n",
      "1                2024-04-29     NetApp                           AFF A300   \n",
      "2                2022-10-31     NetApp                           AFF A300   \n",
      "3                2024-04-29     NetApp                           AFF A300   \n",
      "4                2024-01-22     NetApp                            FAS8200   \n",
      "...                     ...        ...                                ...   \n",
      "16394            2024-07-03      Cisco                         Nexus 9300   \n",
      "16395            2024-07-03      Cisco                         Nexus 9300   \n",
      "16396            2024-12-31     NetApp                        16Pt 10Gb-C   \n",
      "16397            2024-12-31  Microsoft  Microsoft Windows Server Standard   \n",
      "16398            2024-12-31        YIP         Incident System Management   \n",
      "\n",
      "      product_type  project_id action  \n",
      "0          Storage          22  added  \n",
      "1          Storage          20  added  \n",
      "2          Storage          11  added  \n",
      "3          Storage          20  added  \n",
      "4          Storage          19  added  \n",
      "...            ...         ...    ...  \n",
      "16394       Switch         721  added  \n",
      "16395       Switch         721  added  \n",
      "16396      Storage         765  added  \n",
      "16397       Server         765  added  \n",
      "16398     Software         765  added  \n",
      "\n",
      "[16399 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "if isFirstLoad==False:\n",
    " df=add_acutal_action_to_df_at_next(df,dfModelLog,view_name,view_name_id)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Last Step :Check duplicate ID & reset index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no duplicate inventory_id ID\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16399 entries, 0 to 16398\n",
      "Data columns (total 9 columns):\n",
      " #   Column                   Non-Null Count  Dtype \n",
      "---  ------                   --------------  ----- \n",
      " 0   inventory_id             16399 non-null  int64 \n",
      " 1   serial_number            16399 non-null  object\n",
      " 2   customer_warranty_start  16399 non-null  object\n",
      " 3   customer_warranty_end    16399 non-null  object\n",
      " 4   brand                    16399 non-null  object\n",
      " 5   model                    16399 non-null  object\n",
      " 6   product_type             16399 non-null  object\n",
      " 7   project_id               16399 non-null  int64 \n",
      " 8   action                   16399 non-null  object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "       inventory_id  serial_number customer_warranty_start  \\\n",
      "0                47   451951000035              2020-01-12   \n",
      "1                46   451915000214              2019-04-26   \n",
      "2                18   451741000112              2017-10-25   \n",
      "3                45   451915000213              2019-04-26   \n",
      "4                44   451902000028              2019-01-23   \n",
      "...             ...            ...                     ...   \n",
      "16394         19205    FDO23011B4G              2020-07-04   \n",
      "16395         19206    FDO23011AVZ              2020-07-04   \n",
      "16396         19211           np01              2023-12-01   \n",
      "16397         19213  win-server-01              2023-12-01   \n",
      "16398         19214              -              2023-12-01   \n",
      "\n",
      "      customer_warranty_end      brand                              model  \\\n",
      "0                2025-01-12     NetApp                           AFF A300   \n",
      "1                2024-04-29     NetApp                           AFF A300   \n",
      "2                2022-10-31     NetApp                           AFF A300   \n",
      "3                2024-04-29     NetApp                           AFF A300   \n",
      "4                2024-01-22     NetApp                            FAS8200   \n",
      "...                     ...        ...                                ...   \n",
      "16394            2024-07-03      Cisco                         Nexus 9300   \n",
      "16395            2024-07-03      Cisco                         Nexus 9300   \n",
      "16396            2024-12-31     NetApp                        16Pt 10Gb-C   \n",
      "16397            2024-12-31  Microsoft  Microsoft Windows Server Standard   \n",
      "16398            2024-12-31        YIP         Incident System Management   \n",
      "\n",
      "      product_type  project_id action  \n",
      "0          Storage          22  added  \n",
      "1          Storage          20  added  \n",
      "2          Storage          11  added  \n",
      "3          Storage          20  added  \n",
      "4          Storage          19  added  \n",
      "...            ...         ...    ...  \n",
      "16394       Switch         721  added  \n",
      "16395       Switch         721  added  \n",
      "16396      Storage         765  added  \n",
      "16397       Server         765  added  \n",
      "16398     Software         765  added  \n",
      "\n",
      "[16399 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "hasDplicateIDs = df[view_name_id].duplicated().any()\n",
    "if  hasDplicateIDs:\n",
    " raise Exception(\"There are some duplicate id on dfUpdateData\")\n",
    "else:\n",
    " print(f\"There is no duplicate {view_name_id} ID\")  \n",
    "\n",
    "\n",
    "# merged_df['imported_at']=dt_imported\n",
    "df=df.reset_index(drop=True  )\n",
    "print(df.info())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inventory_id</th>\n",
       "      <th>serial_number</th>\n",
       "      <th>customer_warranty_start</th>\n",
       "      <th>customer_warranty_end</th>\n",
       "      <th>brand</th>\n",
       "      <th>model</th>\n",
       "      <th>product_type</th>\n",
       "      <th>project_id</th>\n",
       "      <th>action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>47</td>\n",
       "      <td>451951000035</td>\n",
       "      <td>2020-01-12</td>\n",
       "      <td>2025-01-12</td>\n",
       "      <td>NetApp</td>\n",
       "      <td>AFF A300</td>\n",
       "      <td>Storage</td>\n",
       "      <td>22</td>\n",
       "      <td>added</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>46</td>\n",
       "      <td>451915000214</td>\n",
       "      <td>2019-04-26</td>\n",
       "      <td>2024-04-29</td>\n",
       "      <td>NetApp</td>\n",
       "      <td>AFF A300</td>\n",
       "      <td>Storage</td>\n",
       "      <td>20</td>\n",
       "      <td>added</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18</td>\n",
       "      <td>451741000112</td>\n",
       "      <td>2017-10-25</td>\n",
       "      <td>2022-10-31</td>\n",
       "      <td>NetApp</td>\n",
       "      <td>AFF A300</td>\n",
       "      <td>Storage</td>\n",
       "      <td>11</td>\n",
       "      <td>added</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45</td>\n",
       "      <td>451915000213</td>\n",
       "      <td>2019-04-26</td>\n",
       "      <td>2024-04-29</td>\n",
       "      <td>NetApp</td>\n",
       "      <td>AFF A300</td>\n",
       "      <td>Storage</td>\n",
       "      <td>20</td>\n",
       "      <td>added</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>44</td>\n",
       "      <td>451902000028</td>\n",
       "      <td>2019-01-23</td>\n",
       "      <td>2024-01-22</td>\n",
       "      <td>NetApp</td>\n",
       "      <td>FAS8200</td>\n",
       "      <td>Storage</td>\n",
       "      <td>19</td>\n",
       "      <td>added</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16394</th>\n",
       "      <td>19205</td>\n",
       "      <td>FDO23011B4G</td>\n",
       "      <td>2020-07-04</td>\n",
       "      <td>2024-07-03</td>\n",
       "      <td>Cisco</td>\n",
       "      <td>Nexus 9300</td>\n",
       "      <td>Switch</td>\n",
       "      <td>721</td>\n",
       "      <td>added</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16395</th>\n",
       "      <td>19206</td>\n",
       "      <td>FDO23011AVZ</td>\n",
       "      <td>2020-07-04</td>\n",
       "      <td>2024-07-03</td>\n",
       "      <td>Cisco</td>\n",
       "      <td>Nexus 9300</td>\n",
       "      <td>Switch</td>\n",
       "      <td>721</td>\n",
       "      <td>added</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16396</th>\n",
       "      <td>19211</td>\n",
       "      <td>np01</td>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>NetApp</td>\n",
       "      <td>16Pt 10Gb-C</td>\n",
       "      <td>Storage</td>\n",
       "      <td>765</td>\n",
       "      <td>added</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16397</th>\n",
       "      <td>19213</td>\n",
       "      <td>win-server-01</td>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>Microsoft Windows Server Standard</td>\n",
       "      <td>Server</td>\n",
       "      <td>765</td>\n",
       "      <td>added</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16398</th>\n",
       "      <td>19214</td>\n",
       "      <td>-</td>\n",
       "      <td>2023-12-01</td>\n",
       "      <td>2024-12-31</td>\n",
       "      <td>YIP</td>\n",
       "      <td>Incident System Management</td>\n",
       "      <td>Software</td>\n",
       "      <td>765</td>\n",
       "      <td>added</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16399 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       inventory_id  serial_number customer_warranty_start  \\\n",
       "0                47   451951000035              2020-01-12   \n",
       "1                46   451915000214              2019-04-26   \n",
       "2                18   451741000112              2017-10-25   \n",
       "3                45   451915000213              2019-04-26   \n",
       "4                44   451902000028              2019-01-23   \n",
       "...             ...            ...                     ...   \n",
       "16394         19205    FDO23011B4G              2020-07-04   \n",
       "16395         19206    FDO23011AVZ              2020-07-04   \n",
       "16396         19211           np01              2023-12-01   \n",
       "16397         19213  win-server-01              2023-12-01   \n",
       "16398         19214              -              2023-12-01   \n",
       "\n",
       "      customer_warranty_end      brand                              model  \\\n",
       "0                2025-01-12     NetApp                           AFF A300   \n",
       "1                2024-04-29     NetApp                           AFF A300   \n",
       "2                2022-10-31     NetApp                           AFF A300   \n",
       "3                2024-04-29     NetApp                           AFF A300   \n",
       "4                2024-01-22     NetApp                            FAS8200   \n",
       "...                     ...        ...                                ...   \n",
       "16394            2024-07-03      Cisco                         Nexus 9300   \n",
       "16395            2024-07-03      Cisco                         Nexus 9300   \n",
       "16396            2024-12-31     NetApp                        16Pt 10Gb-C   \n",
       "16397            2024-12-31  Microsoft  Microsoft Windows Server Standard   \n",
       "16398            2024-12-31        YIP         Incident System Management   \n",
       "\n",
       "      product_type  project_id action  \n",
       "0          Storage          22  added  \n",
       "1          Storage          20  added  \n",
       "2          Storage          11  added  \n",
       "3          Storage          20  added  \n",
       "4          Storage          19  added  \n",
       "...            ...         ...    ...  \n",
       "16394       Switch         721  added  \n",
       "16395       Switch         721  added  \n",
       "16396      Storage         765  added  \n",
       "16397       Server         765  added  \n",
       "16398     Software         765  added  \n",
       "\n",
       "[16399 rows x 9 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert data to BQ data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table pongthorn.SMartData_Temp.temp_inventory already exists.\n",
      "[SchemaField('inventory_id', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('serial_number', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('customer_warranty_start', 'DATE', 'NULLABLE', None, None, (), None), SchemaField('customer_warranty_end', 'DATE', 'NULLABLE', None, None, (), None), SchemaField('brand', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('model', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('product_type', 'STRING', 'NULLABLE', None, None, (), None), SchemaField('project_id', 'INTEGER', 'NULLABLE', None, None, (), None), SchemaField('action', 'STRING', 'NULLABLE', None, None, (), None)]\n",
      "Total  16399 Imported data to pongthorn.SMartData_Temp.temp_inventory on bigquery successfully\n"
     ]
    }
   ],
   "source": [
    "if get_bq_table():\n",
    "    try:\n",
    "        insertDataFrameToBQ(df)\n",
    "    except Exception as ex:\n",
    "        raise ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run StoreProcedure To Merge Temp&Main and Truncate Transaction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Run StoreProcedure To Merge Temp&Main and Truncate Transaction.\n",
      " CALL `pongthorn.SMartDataAnalytics.merge_inventory`() \n"
     ]
    }
   ],
   "source": [
    "print(\"# Run StoreProcedure To Merge Temp&Main and Truncate Transaction.\")\n",
    "# https://cloud.google.com/bigquery/docs/transactions\n",
    "sp_id_to_invoke=f\"\"\" CALL `{projectId}.{main_dataset_id}.{sp_name}`() \"\"\"\n",
    "print(sp_id_to_invoke)\n",
    "\n",
    "sp_job = client.query(sp_id_to_invoke)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConfigUpdater [\n",
       "    <Section: 'metadata' [\n",
       "        <Option: pmr_inventory = '2023-12-31 08:12:24'>\n",
       "    ]>\n",
       "]>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updater[\"metadata\"][view_name].value=dt_imported.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "updater.update_file() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-31 08:12:32.364800+00:00\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now(timezone.utc) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
